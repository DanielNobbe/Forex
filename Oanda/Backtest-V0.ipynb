{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from oandapyV20 import API\n",
    "from oandapyV20.exceptions import V20Error\n",
    "import oandapyV20.endpoints.instruments as instruments\n",
    "from oandapyV20.definitions.instruments import CandlestickGranularity\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting\n",
    "In this notebook, we set up the basic structure of testing the algorithm. \\\n",
    "The algorithm basically works as follows: \\\n",
    "<ins>Inputs:</ins> \n",
    "- A history of an instrument, consisting of:\n",
    "    - Historical values, and\n",
    "    - their timestamps\n",
    "- The current value of the same instrument,\n",
    "    - Including timestamp\n",
    "- boolean value for current ownership of the quote currency\n",
    "\n",
    "<ins>Outputs:</ins>\n",
    "- Prediction whether value will go up/down\n",
    "- and/or prediction whether quote currency should be bought/sold/kept with base currency (the algorithm should do the trading end-to-end, so use both) (kept means no buying nor selling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a default set of historical values for now (from History.ipynb)\n",
    "\n",
    "# Helper function to convert datetime string to datetime object\n",
    "def to_datetime(t):\n",
    "    return datetime.datetime.strptime(t[0:-4], '%Y-%m-%dT%H:%M:%S.%f') \n",
    "# granularities = CandlestickGranularity().definitions.keys()\n",
    "# Define parameters\n",
    "granularity = 'M2'\n",
    "From = \"2017-01-01\"\n",
    "To = \"2017-12-31\"\n",
    "i = 'EUR_USD' # Instrument - first is base currency, second quote currency\n",
    "                                # I think base should always be EUR for this acct\n",
    "params = {\n",
    "    'from': From,\n",
    "    'to': To,\n",
    "    'granularity': 'H2',\n",
    "}\n",
    "# Compile request\n",
    "r = instruments.InstrumentsCandles(instrument=i, params=params)\n",
    "\n",
    "# Define API with our access token (for demo acct)\n",
    "api = API(access_token='378d83764609aa3a4eb262663b7c02ef-482ed5696d2a3cede7fca4aa7ded1c76')\n",
    "\n",
    "# Request candles\n",
    "rv = api.request(r)\n",
    "\n",
    "history_val0 = [float(candle['mid']['c']) for candle in rv['candles']] # center? values\n",
    "timestamps0 = [to_datetime(candle['time']).timestamp() for candle in rv['candles']] # time values\n",
    "# dates = [to_datetime(candle['time']).date() for candle in rv['candles']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a default set of historical values for now (from History.ipynb)\n",
    "\n",
    "# Helper function to convert datetime string to datetime object\n",
    "def to_datetime(t):\n",
    "    return datetime.datetime.strptime(t[0:-4], '%Y-%m-%dT%H:%M:%S.%f') \n",
    "# granularities = CandlestickGranularity().definitions.keys()\n",
    "# Define parameters\n",
    "granularity = 'M2'\n",
    "From = \"2018-01-01\"\n",
    "To = \"2018-12-31\"\n",
    "i = 'EUR_USD' # Instrument - first is base currency, second quote currency\n",
    "                                # I think base should always be EUR for this acct\n",
    "params = {\n",
    "    'from': From,\n",
    "    'to': To,\n",
    "    'granularity': 'H2',\n",
    "}\n",
    "# Compile request\n",
    "r = instruments.InstrumentsCandles(instrument=i, params=params)\n",
    "\n",
    "# Define API with our access token (for demo acct)\n",
    "api = API(access_token='378d83764609aa3a4eb262663b7c02ef-482ed5696d2a3cede7fca4aa7ded1c76')\n",
    "\n",
    "# Request candles\n",
    "rv = api.request(r)\n",
    "\n",
    "history_val1 = [float(candle['mid']['c']) for candle in rv['candles']] # center? values\n",
    "timestamps1 = [to_datetime(candle['time']).timestamp() for candle in rv['candles']] # time values\n",
    "# dates = [to_datetime(candle['time']).date() for candle in rv['candles']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a default set of historical values for now (from History.ipynb)\n",
    "\n",
    "# Helper function to convert datetime string to datetime object\n",
    "def to_datetime(t):\n",
    "    return datetime.datetime.strptime(t[0:-4], '%Y-%m-%dT%H:%M:%S.%f') \n",
    "# granularities = CandlestickGranularity().definitions.keys()\n",
    "# Define parameters\n",
    "granularity = 'M2'\n",
    "From = \"2019-01-01\"\n",
    "To = \"2020-01-01\"\n",
    "i = 'EUR_USD' # Instrument - first is base currency, second quote currency\n",
    "                                # I think base should always be EUR for this acct\n",
    "params = {\n",
    "    'from': From,\n",
    "    'to': To,\n",
    "    'granularity': 'H2',\n",
    "}\n",
    "# Compile request\n",
    "r = instruments.InstrumentsCandles(instrument=i, params=params)\n",
    "\n",
    "# Define API with our access token (for demo acct)\n",
    "api = API(access_token='378d83764609aa3a4eb262663b7c02ef-482ed5696d2a3cede7fca4aa7ded1c76')\n",
    "\n",
    "# Request candles\n",
    "rv = api.request(r)\n",
    "\n",
    "history_val2 = [float(candle['mid']['c']) for candle in rv['candles']] # center? values\n",
    "timestamps2 = [to_datetime(candle['time']).timestamp() for candle in rv['candles']] # time values\n",
    "# dates = [to_datetime(candle['time']).date() for candle in rv['candles']]\n",
    "\n",
    "history_val = [*history_val0, *history_val1, *history_val2]\n",
    "timestamps = [*timestamps0, *timestamps1, *timestamps2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History():\n",
    "    def __init__(self, values, timestamps):\n",
    "        self.values = values\n",
    "        self.timestamps = timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9306\n"
     ]
    }
   ],
   "source": [
    "print(len(history_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History(history_val[0:9000], timestamps[0:9000]) # train set\n",
    "history_test = History(history_val[9001:], timestamps[9001:]) # test set\n",
    "current = History([current_value], [current_timestamp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov kernel: Probability of increase (over minimum), decrease (over minimum), or neither (within minimum)\n",
    "\n",
    "minimum = 0.0002\n",
    "\n",
    "min_pos = minimum # Minimum positive increase\n",
    "min_neg = minimum # Minimum negative increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovKernel(nn.Module):\n",
    "    # Simple neural network for markov kernel\n",
    "    def __init__(self, hidden_size):\n",
    "        super(MarkovKernel, self).__init__()\n",
    "        layer_list = [\n",
    "            nn.Linear(1, hidden_size[0]), \n",
    "            nn.Tanh(),]\n",
    "        for hidden_layer_index in np.arange(1, len( hidden_size ) ):\n",
    "          layer_list.append( nn.Linear( hidden_size[hidden_layer_index-1], \\\n",
    "            hidden_size[hidden_layer_index]  ) ) # take in_features out_features into account\n",
    "          layer_list.append( nn.Tanh() ) \n",
    "        \n",
    "        layer_list.append( nn.Linear(hidden_size[-1],  3 ) )\n",
    "        layer_list.append(nn.Softmax())\n",
    "        \n",
    "        self.model = nn.Sequential( *layer_list ) #Unpacks list for sequential\n",
    "        \n",
    "    def forward(self, value):\n",
    "        classification = self.model(value)\n",
    "        return classification\n",
    "    \n",
    "    def classify(self, history):\n",
    "        classification = self.model(history[-1])\n",
    "        return classification\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MarkovKernel([16])\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(markov_kernel.parameters(), lr=1e-3)\n",
    "no_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 - Loss: -0.4793016314506531 - prediction 0.4793016314506531 - improvement 2\n",
      "Step 10 - Loss: -0.47566935420036316 - prediction 0.47566935420036316 - improvement 2\n",
      "Step 20 - Loss: -0.32767489552497864 - prediction 0.32767489552497864 - improvement 0\n",
      "Step 30 - Loss: -0.3275674879550934 - prediction 0.3275674879550934 - improvement 0\n",
      "Step 40 - Loss: -0.327438622713089 - prediction 0.327438622713089 - improvement 0\n",
      "Step 50 - Loss: -0.47639063000679016 - prediction 0.47639063000679016 - improvement 2\n",
      "Step 60 - Loss: -0.327457457780838 - prediction 0.327457457780838 - improvement 0\n",
      "Step 70 - Loss: -0.47651129961013794 - prediction 0.47651129961013794 - improvement 2\n",
      "Step 80 - Loss: -0.3275533616542816 - prediction 0.3275533616542816 - improvement 0\n",
      "Step 90 - Loss: -0.32734161615371704 - prediction 0.32734161615371704 - improvement 0\n",
      "Step 100 - Loss: -0.32738178968429565 - prediction 0.32738178968429565 - improvement 0\n",
      "Step 110 - Loss: -0.1957419067621231 - prediction 0.1957419067621231 - improvement 1\n",
      "Step 120 - Loss: -0.32740893959999084 - prediction 0.32740893959999084 - improvement 0\n",
      "Step 130 - Loss: -0.3272450864315033 - prediction 0.3272450864315033 - improvement 0\n",
      "Step 140 - Loss: -0.3272962272167206 - prediction 0.3272962272167206 - improvement 0\n",
      "Step 150 - Loss: -0.47713154554367065 - prediction 0.47713154554367065 - improvement 2\n",
      "Step 160 - Loss: -0.47720423340797424 - prediction 0.47720423340797424 - improvement 2\n",
      "Step 170 - Loss: -0.32716965675354004 - prediction 0.32716965675354004 - improvement 0\n",
      "Step 180 - Loss: -0.477771133184433 - prediction 0.477771133184433 - improvement 2\n",
      "Step 190 - Loss: -0.47772476077079773 - prediction 0.47772476077079773 - improvement 2\n",
      "Step 200 - Loss: -0.4776524007320404 - prediction 0.4776524007320404 - improvement 2\n",
      "Step 210 - Loss: -0.4775063395500183 - prediction 0.4775063395500183 - improvement 2\n",
      "Step 220 - Loss: -0.3272911012172699 - prediction 0.3272911012172699 - improvement 0\n",
      "Step 230 - Loss: -0.47760289907455444 - prediction 0.47760289907455444 - improvement 2\n",
      "Step 240 - Loss: -0.4773476719856262 - prediction 0.4773476719856262 - improvement 2\n",
      "Step 250 - Loss: -0.3271040618419647 - prediction 0.3271040618419647 - improvement 0\n",
      "Step 260 - Loss: -0.47774946689605713 - prediction 0.47774946689605713 - improvement 2\n",
      "Step 270 - Loss: -0.19486138224601746 - prediction 0.19486138224601746 - improvement 1\n",
      "Step 280 - Loss: -0.4777328073978424 - prediction 0.4777328073978424 - improvement 2\n",
      "Step 290 - Loss: -0.4778674840927124 - prediction 0.4778674840927124 - improvement 2\n",
      "Step 300 - Loss: -0.4776870608329773 - prediction 0.4776870608329773 - improvement 2\n",
      "Step 310 - Loss: -0.47732529044151306 - prediction 0.47732529044151306 - improvement 2\n",
      "Step 320 - Loss: -0.32729342579841614 - prediction 0.32729342579841614 - improvement 0\n",
      "Step 330 - Loss: -0.4773891568183899 - prediction 0.4773891568183899 - improvement 2\n",
      "Step 340 - Loss: -0.47708237171173096 - prediction 0.47708237171173096 - improvement 2\n",
      "Step 350 - Loss: -0.3273753225803375 - prediction 0.3273753225803375 - improvement 0\n",
      "Step 360 - Loss: -0.19585265219211578 - prediction 0.19585265219211578 - improvement 1\n",
      "Step 370 - Loss: -0.32746773958206177 - prediction 0.32746773958206177 - improvement 0\n",
      "Step 380 - Loss: -0.3275187313556671 - prediction 0.3275187313556671 - improvement 0\n",
      "Step 390 - Loss: -0.4769996702671051 - prediction 0.4769996702671051 - improvement 2\n",
      "Step 400 - Loss: -0.4771973788738251 - prediction 0.4771973788738251 - improvement 2\n",
      "Step 410 - Loss: -0.32739439606666565 - prediction 0.32739439606666565 - improvement 0\n",
      "Step 420 - Loss: -0.47683900594711304 - prediction 0.47683900594711304 - improvement 2\n",
      "Step 430 - Loss: -0.4764031767845154 - prediction 0.4764031767845154 - improvement 2\n",
      "Step 440 - Loss: -0.32755136489868164 - prediction 0.32755136489868164 - improvement 0\n",
      "Step 450 - Loss: -0.3274982273578644 - prediction 0.3274982273578644 - improvement 0\n",
      "Step 460 - Loss: -0.3274417519569397 - prediction 0.3274417519569397 - improvement 0\n",
      "Step 470 - Loss: -0.4765763282775879 - prediction 0.4765763282775879 - improvement 2\n",
      "Step 480 - Loss: -0.47666874527931213 - prediction 0.47666874527931213 - improvement 2\n",
      "Step 490 - Loss: -0.4768003523349762 - prediction 0.4768003523349762 - improvement 2\n",
      "Step 500 - Loss: -0.3275332450866699 - prediction 0.3275332450866699 - improvement 0\n",
      "Step 510 - Loss: -0.4762917160987854 - prediction 0.4762917160987854 - improvement 2\n",
      "Step 520 - Loss: -0.1962141990661621 - prediction 0.1962141990661621 - improvement 1\n",
      "Step 530 - Loss: -0.4768003523349762 - prediction 0.4768003523349762 - improvement 2\n",
      "Step 540 - Loss: -0.19592051208019257 - prediction 0.19592051208019257 - improvement 1\n",
      "Step 550 - Loss: -0.4765969216823578 - prediction 0.4765969216823578 - improvement 2\n",
      "Step 560 - Loss: -0.4764644205570221 - prediction 0.4764644205570221 - improvement 2\n",
      "Step 570 - Loss: -0.32747843861579895 - prediction 0.32747843861579895 - improvement 0\n",
      "Step 580 - Loss: -0.32741329073905945 - prediction 0.32741329073905945 - improvement 0\n",
      "Step 590 - Loss: -0.32727327942848206 - prediction 0.32727327942848206 - improvement 0\n",
      "Step 600 - Loss: -0.3273220360279083 - prediction 0.3273220360279083 - improvement 0\n",
      "Step 610 - Loss: -0.47698286175727844 - prediction 0.47698286175727844 - improvement 2\n",
      "Step 620 - Loss: -0.32737645506858826 - prediction 0.32737645506858826 - improvement 0\n",
      "Step 630 - Loss: -0.3272280991077423 - prediction 0.3272280991077423 - improvement 0\n",
      "Step 640 - Loss: -0.19505774974822998 - prediction 0.19505774974822998 - improvement 1\n",
      "Step 650 - Loss: -0.3271598219871521 - prediction 0.3271598219871521 - improvement 0\n",
      "Step 660 - Loss: -0.327178031206131 - prediction 0.327178031206131 - improvement 0\n",
      "Step 670 - Loss: -0.47808709740638733 - prediction 0.47808709740638733 - improvement 2\n",
      "Step 680 - Loss: -0.3270716667175293 - prediction 0.3270716667175293 - improvement 0\n",
      "Step 690 - Loss: -0.3271191418170929 - prediction 0.3271191418170929 - improvement 0\n",
      "Step 700 - Loss: -0.3271365463733673 - prediction 0.3271365463733673 - improvement 0\n",
      "Step 710 - Loss: -0.47827550768852234 - prediction 0.47827550768852234 - improvement 2\n",
      "Step 720 - Loss: -0.478380024433136 - prediction 0.478380024433136 - improvement 2\n",
      "Step 730 - Loss: -0.4781986176967621 - prediction 0.4781986176967621 - improvement 2\n",
      "Step 740 - Loss: -0.4777686297893524 - prediction 0.4777686297893524 - improvement 2\n",
      "Step 750 - Loss: -0.47758060693740845 - prediction 0.47758060693740845 - improvement 2\n",
      "Step 760 - Loss: -0.3272852301597595 - prediction 0.3272852301597595 - improvement 0\n",
      "Step 770 - Loss: -0.3272891044616699 - prediction 0.3272891044616699 - improvement 0\n",
      "Step 780 - Loss: -0.19551846385002136 - prediction 0.19551846385002136 - improvement 1\n",
      "Step 790 - Loss: -0.3273092210292816 - prediction 0.3273092210292816 - improvement 0\n",
      "Step 800 - Loss: -0.4771650731563568 - prediction 0.4771650731563568 - improvement 2\n",
      "Step 810 - Loss: -0.32731354236602783 - prediction 0.32731354236602783 - improvement 0\n",
      "Step 820 - Loss: -0.47705069184303284 - prediction 0.47705069184303284 - improvement 2\n",
      "Step 830 - Loss: -0.3274478614330292 - prediction 0.3274478614330292 - improvement 0\n",
      "Step 840 - Loss: -0.4767255187034607 - prediction 0.4767255187034607 - improvement 2\n",
      "Step 850 - Loss: -0.47681400179862976 - prediction 0.47681400179862976 - improvement 2\n",
      "Step 860 - Loss: -0.32740944623947144 - prediction 0.32740944623947144 - improvement 0\n",
      "Step 870 - Loss: -0.4770171046257019 - prediction 0.4770171046257019 - improvement 2\n",
      "Step 880 - Loss: -0.3273841142654419 - prediction 0.3273841142654419 - improvement 0\n",
      "Step 890 - Loss: -0.19574281573295593 - prediction 0.19574281573295593 - improvement 1\n",
      "Step 900 - Loss: -0.1956387311220169 - prediction 0.1956387311220169 - improvement 1\n",
      "Step 910 - Loss: -0.3272307813167572 - prediction 0.3272307813167572 - improvement 0\n",
      "Step 920 - Loss: -0.47750014066696167 - prediction 0.47750014066696167 - improvement 2\n",
      "Step 930 - Loss: -0.4778069853782654 - prediction 0.4778069853782654 - improvement 2\n",
      "Step 940 - Loss: -0.32721641659736633 - prediction 0.32721641659736633 - improvement 0\n",
      "Step 950 - Loss: -0.3269919157028198 - prediction 0.3269919157028198 - improvement 0\n",
      "Step 960 - Loss: -0.4784082770347595 - prediction 0.4784082770347595 - improvement 2\n",
      "Step 970 - Loss: -0.4789097309112549 - prediction 0.4789097309112549 - improvement 2\n",
      "Step 980 - Loss: -0.4785286486148834 - prediction 0.4785286486148834 - improvement 2\n",
      "Step 990 - Loss: -0.47865501046180725 - prediction 0.47865501046180725 - improvement 2\n",
      "Step 1000 - Loss: -0.3269682824611664 - prediction 0.3269682824611664 - improvement 0\n",
      "Step 1010 - Loss: -0.4786059558391571 - prediction 0.4786059558391571 - improvement 2\n",
      "Step 1020 - Loss: -0.32690998911857605 - prediction 0.32690998911857605 - improvement 0\n",
      "Step 1030 - Loss: -0.3268977105617523 - prediction 0.3268977105617523 - improvement 0\n",
      "Step 1040 - Loss: -0.3268830180168152 - prediction 0.3268830180168152 - improvement 0\n",
      "Step 1050 - Loss: -0.3268769383430481 - prediction 0.3268769383430481 - improvement 0\n",
      "Step 1060 - Loss: -0.47911927103996277 - prediction 0.47911927103996277 - improvement 2\n",
      "Step 1070 - Loss: -0.3267950415611267 - prediction 0.3267950415611267 - improvement 0\n",
      "Step 1080 - Loss: -0.194377601146698 - prediction 0.194377601146698 - improvement 1\n",
      "Step 1090 - Loss: -0.4785832464694977 - prediction 0.4785832464694977 - improvement 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1100 - Loss: -0.4784850776195526 - prediction 0.4784850776195526 - improvement 2\n",
      "Step 1110 - Loss: -0.47847646474838257 - prediction 0.47847646474838257 - improvement 2\n",
      "Step 1120 - Loss: -0.47843098640441895 - prediction 0.47843098640441895 - improvement 2\n",
      "Step 1130 - Loss: -0.3268667757511139 - prediction 0.3268667757511139 - improvement 0\n",
      "Step 1140 - Loss: -0.32678133249282837 - prediction 0.32678133249282837 - improvement 0\n",
      "Step 1150 - Loss: -0.3266105055809021 - prediction 0.3266105055809021 - improvement 0\n",
      "Step 1160 - Loss: -0.3265290856361389 - prediction 0.3265290856361389 - improvement 0\n",
      "Step 1170 - Loss: -0.4799922704696655 - prediction 0.4799922704696655 - improvement 2\n",
      "Step 1180 - Loss: -0.32654857635498047 - prediction 0.32654857635498047 - improvement 0\n",
      "Step 1190 - Loss: -0.32642054557800293 - prediction 0.32642054557800293 - improvement 0\n",
      "Step 1200 - Loss: -0.326333612203598 - prediction 0.326333612203598 - improvement 0\n",
      "Step 1210 - Loss: -0.4803856611251831 - prediction 0.4803856611251831 - improvement 2\n",
      "Step 1220 - Loss: -0.48041945695877075 - prediction 0.48041945695877075 - improvement 2\n",
      "Step 1230 - Loss: -0.48068687319755554 - prediction 0.48068687319755554 - improvement 2\n",
      "Step 1240 - Loss: -0.3263974189758301 - prediction 0.3263974189758301 - improvement 0\n",
      "Step 1250 - Loss: -0.4802655279636383 - prediction 0.4802655279636383 - improvement 2\n",
      "Step 1260 - Loss: -0.4802094101905823 - prediction 0.4802094101905823 - improvement 2\n",
      "Step 1270 - Loss: -0.3264416456222534 - prediction 0.3264416456222534 - improvement 0\n",
      "Step 1280 - Loss: -0.32635942101478577 - prediction 0.32635942101478577 - improvement 0\n",
      "Step 1290 - Loss: -0.4805905520915985 - prediction 0.4805905520915985 - improvement 2\n",
      "Step 1300 - Loss: -0.19308596849441528 - prediction 0.19308596849441528 - improvement 1\n",
      "Step 1310 - Loss: -0.4808701276779175 - prediction 0.4808701276779175 - improvement 2\n",
      "Step 1320 - Loss: -0.32630419731140137 - prediction 0.32630419731140137 - improvement 0\n",
      "Step 1330 - Loss: -0.32627731561660767 - prediction 0.32627731561660767 - improvement 0\n",
      "Step 1340 - Loss: -0.326345831155777 - prediction 0.326345831155777 - improvement 0\n",
      "Step 1350 - Loss: -0.4807289242744446 - prediction 0.4807289242744446 - improvement 2\n",
      "Step 1360 - Loss: -0.4804568290710449 - prediction 0.4804568290710449 - improvement 2\n",
      "Step 1370 - Loss: -0.19314798712730408 - prediction 0.19314798712730408 - improvement 1\n",
      "Step 1380 - Loss: -0.4804544448852539 - prediction 0.4804544448852539 - improvement 2\n",
      "Step 1390 - Loss: -0.32639461755752563 - prediction 0.32639461755752563 - improvement 0\n",
      "Step 1400 - Loss: -0.32628941535949707 - prediction 0.32628941535949707 - improvement 0\n",
      "Step 1410 - Loss: -0.48031085729599 - prediction 0.48031085729599 - improvement 2\n",
      "Step 1420 - Loss: -0.32649335265159607 - prediction 0.32649335265159607 - improvement 0\n",
      "Step 1430 - Loss: -0.32641369104385376 - prediction 0.32641369104385376 - improvement 0\n",
      "Step 1440 - Loss: -0.4801193177700043 - prediction 0.4801193177700043 - improvement 2\n",
      "Step 1450 - Loss: -0.32652682065963745 - prediction 0.32652682065963745 - improvement 0\n",
      "Step 1460 - Loss: -0.19339531660079956 - prediction 0.19339531660079956 - improvement 1\n",
      "Step 1470 - Loss: -0.48024019598960876 - prediction 0.48024019598960876 - improvement 2\n",
      "Step 1480 - Loss: -0.32646337151527405 - prediction 0.32646337151527405 - improvement 0\n",
      "Step 1490 - Loss: -0.48041218519210815 - prediction 0.48041218519210815 - improvement 2\n",
      "Step 1500 - Loss: -0.3264291286468506 - prediction 0.3264291286468506 - improvement 0\n",
      "Step 1510 - Loss: -0.3262052536010742 - prediction 0.3262052536010742 - improvement 0\n",
      "Step 1520 - Loss: -0.32617121934890747 - prediction 0.32617121934890747 - improvement 0\n",
      "Step 1530 - Loss: -0.48178422451019287 - prediction 0.48178422451019287 - improvement 2\n",
      "Step 1540 - Loss: -0.4818722903728485 - prediction 0.4818722903728485 - improvement 2\n",
      "Step 1550 - Loss: -0.4817348122596741 - prediction 0.4817348122596741 - improvement 2\n",
      "Step 1560 - Loss: -0.3261150121688843 - prediction 0.3261150121688843 - improvement 0\n",
      "Step 1570 - Loss: -0.4813404083251953 - prediction 0.4813404083251953 - improvement 2\n",
      "Step 1580 - Loss: -0.3261900544166565 - prediction 0.3261900544166565 - improvement 0\n",
      "Step 1590 - Loss: -0.32614096999168396 - prediction 0.32614096999168396 - improvement 0\n",
      "Step 1600 - Loss: -0.32602646946907043 - prediction 0.32602646946907043 - improvement 0\n",
      "Step 1610 - Loss: -0.32605162262916565 - prediction 0.32605162262916565 - improvement 0\n",
      "Step 1620 - Loss: -0.4816281497478485 - prediction 0.4816281497478485 - improvement 2\n",
      "Step 1630 - Loss: -0.3259839117527008 - prediction 0.3259839117527008 - improvement 0\n",
      "Step 1640 - Loss: -0.4820338785648346 - prediction 0.4820338785648346 - improvement 2\n",
      "Step 1650 - Loss: -0.3260462284088135 - prediction 0.3260462284088135 - improvement 0\n",
      "Step 1660 - Loss: -0.3260512351989746 - prediction 0.3260512351989746 - improvement 0\n",
      "Step 1670 - Loss: -0.4820178747177124 - prediction 0.4820178747177124 - improvement 2\n",
      "Step 1680 - Loss: -0.32591676712036133 - prediction 0.32591676712036133 - improvement 0\n",
      "Step 1690 - Loss: -0.48259881138801575 - prediction 0.48259881138801575 - improvement 2\n",
      "Step 1700 - Loss: -0.4823782444000244 - prediction 0.4823782444000244 - improvement 2\n",
      "Step 1710 - Loss: -0.32587021589279175 - prediction 0.32587021589279175 - improvement 0\n",
      "Step 1720 - Loss: -0.32565319538116455 - prediction 0.32565319538116455 - improvement 0\n",
      "Step 1730 - Loss: -0.19118602573871613 - prediction 0.19118602573871613 - improvement 1\n",
      "Step 1740 - Loss: -0.483064204454422 - prediction 0.483064204454422 - improvement 2\n",
      "Step 1750 - Loss: -0.48313236236572266 - prediction 0.48313236236572266 - improvement 2\n",
      "Step 1760 - Loss: -0.48305830359458923 - prediction 0.48305830359458923 - improvement 2\n",
      "Step 1770 - Loss: -0.48357197642326355 - prediction 0.48357197642326355 - improvement 2\n",
      "Step 1780 - Loss: -0.32555481791496277 - prediction 0.32555481791496277 - improvement 0\n",
      "Step 1790 - Loss: -0.48365500569343567 - prediction 0.48365500569343567 - improvement 2\n",
      "Step 1800 - Loss: -0.48423531651496887 - prediction 0.48423531651496887 - improvement 2\n",
      "Step 1810 - Loss: -0.48401492834091187 - prediction 0.48401492834091187 - improvement 2\n",
      "Step 1820 - Loss: -0.32530826330184937 - prediction 0.32530826330184937 - improvement 0\n",
      "Step 1830 - Loss: -0.3252747654914856 - prediction 0.3252747654914856 - improvement 0\n",
      "Step 1840 - Loss: -0.3252108097076416 - prediction 0.3252108097076416 - improvement 0\n",
      "Step 1850 - Loss: -0.32537615299224854 - prediction 0.32537615299224854 - improvement 0\n",
      "Step 1860 - Loss: -0.3253609538078308 - prediction 0.3253609538078308 - improvement 0\n",
      "Step 1870 - Loss: -0.1908523589372635 - prediction 0.1908523589372635 - improvement 1\n",
      "Step 1880 - Loss: -0.19088295102119446 - prediction 0.19088295102119446 - improvement 1\n",
      "Step 1890 - Loss: -0.32548800110816956 - prediction 0.32548800110816956 - improvement 0\n",
      "Step 1900 - Loss: -0.48384061455726624 - prediction 0.48384061455726624 - improvement 2\n",
      "Step 1910 - Loss: -0.19058218598365784 - prediction 0.19058218598365784 - improvement 1\n",
      "Step 1920 - Loss: -0.19073878228664398 - prediction 0.19073878228664398 - improvement 1\n",
      "Step 1930 - Loss: -0.4836217164993286 - prediction 0.4836217164993286 - improvement 2\n",
      "Step 1940 - Loss: -0.19105423986911774 - prediction 0.19105423986911774 - improvement 1\n",
      "Step 1950 - Loss: -0.4835970997810364 - prediction 0.4835970997810364 - improvement 2\n",
      "Step 1960 - Loss: -0.32546260952949524 - prediction 0.32546260952949524 - improvement 0\n",
      "Step 1970 - Loss: -0.19085603952407837 - prediction 0.19085603952407837 - improvement 1\n",
      "Step 1980 - Loss: -0.19060000777244568 - prediction 0.19060000777244568 - improvement 1\n",
      "Step 1990 - Loss: -0.48377883434295654 - prediction 0.48377883434295654 - improvement 2\n",
      "Step 2000 - Loss: -0.32536131143569946 - prediction 0.32536131143569946 - improvement 0\n",
      "Step 2010 - Loss: -0.3253747224807739 - prediction 0.3253747224807739 - improvement 0\n",
      "Step 2020 - Loss: -0.19066691398620605 - prediction 0.19066691398620605 - improvement 1\n",
      "Step 2030 - Loss: -0.48474302887916565 - prediction 0.48474302887916565 - improvement 2\n",
      "Step 2040 - Loss: -0.48502352833747864 - prediction 0.48502352833747864 - improvement 2\n",
      "Step 2050 - Loss: -0.4852377772331238 - prediction 0.4852377772331238 - improvement 2\n",
      "Step 2060 - Loss: -0.4846748113632202 - prediction 0.4846748113632202 - improvement 2\n",
      "Step 2070 - Loss: -0.4845485985279083 - prediction 0.4845485985279083 - improvement 2\n",
      "Step 2080 - Loss: -0.4845532476902008 - prediction 0.4845532476902008 - improvement 2\n",
      "Step 2090 - Loss: -0.19033601880073547 - prediction 0.19033601880073547 - improvement 1\n",
      "Step 2100 - Loss: -0.32518187165260315 - prediction 0.32518187165260315 - improvement 0\n",
      "Step 2110 - Loss: -0.3251689076423645 - prediction 0.3251689076423645 - improvement 0\n",
      "Step 2120 - Loss: -0.4847545623779297 - prediction 0.4847545623779297 - improvement 2\n",
      "Step 2130 - Loss: -0.3250700533390045 - prediction 0.3250700533390045 - improvement 0\n",
      "Step 2140 - Loss: -0.4855465888977051 - prediction 0.4855465888977051 - improvement 2\n",
      "Step 2150 - Loss: -0.48521700501441956 - prediction 0.48521700501441956 - improvement 2\n",
      "Step 2160 - Loss: -0.4848804771900177 - prediction 0.4848804771900177 - improvement 2\n",
      "Step 2170 - Loss: -0.32506293058395386 - prediction 0.32506293058395386 - improvement 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2180 - Loss: -0.48499587178230286 - prediction 0.48499587178230286 - improvement 2\n",
      "Step 2190 - Loss: -0.48459550738334656 - prediction 0.48459550738334656 - improvement 2\n",
      "Step 2200 - Loss: -0.32515254616737366 - prediction 0.32515254616737366 - improvement 0\n",
      "Step 2210 - Loss: -0.4848250448703766 - prediction 0.4848250448703766 - improvement 2\n",
      "Step 2220 - Loss: -0.3250778615474701 - prediction 0.3250778615474701 - improvement 0\n",
      "Step 2230 - Loss: -0.32504338026046753 - prediction 0.32504338026046753 - improvement 0\n",
      "Step 2240 - Loss: -0.32501089572906494 - prediction 0.32501089572906494 - improvement 0\n",
      "Step 2250 - Loss: -0.3251507878303528 - prediction 0.3251507878303528 - improvement 0\n",
      "Step 2260 - Loss: -0.3250679075717926 - prediction 0.3250679075717926 - improvement 0\n",
      "Step 2270 - Loss: -0.3251248300075531 - prediction 0.3251248300075531 - improvement 0\n",
      "Step 2280 - Loss: -0.32526713609695435 - prediction 0.32526713609695435 - improvement 0\n",
      "Step 2290 - Loss: -0.3253832161426544 - prediction 0.3253832161426544 - improvement 0\n",
      "Step 2300 - Loss: -0.3254932463169098 - prediction 0.3254932463169098 - improvement 0\n",
      "Step 2310 - Loss: -0.19079573452472687 - prediction 0.19079573452472687 - improvement 1\n",
      "Step 2320 - Loss: -0.3253888487815857 - prediction 0.3253888487815857 - improvement 0\n",
      "Step 2330 - Loss: -0.4839280843734741 - prediction 0.4839280843734741 - improvement 2\n",
      "Step 2340 - Loss: -0.483593612909317 - prediction 0.483593612909317 - improvement 2\n",
      "Step 2350 - Loss: -0.4837157428264618 - prediction 0.4837157428264618 - improvement 2\n",
      "Step 2360 - Loss: -0.4838255047798157 - prediction 0.4838255047798157 - improvement 2\n",
      "Step 2370 - Loss: -0.4838435649871826 - prediction 0.4838435649871826 - improvement 2\n",
      "Step 2380 - Loss: -0.483402818441391 - prediction 0.483402818441391 - improvement 2\n",
      "Step 2390 - Loss: -0.3254597783088684 - prediction 0.3254597783088684 - improvement 0\n",
      "Step 2400 - Loss: -0.4836947023868561 - prediction 0.4836947023868561 - improvement 2\n",
      "Step 2410 - Loss: -0.48409000039100647 - prediction 0.48409000039100647 - improvement 2\n",
      "Step 2420 - Loss: -0.3253117799758911 - prediction 0.3253117799758911 - improvement 0\n",
      "Step 2430 - Loss: -0.48437759280204773 - prediction 0.48437759280204773 - improvement 2\n",
      "Step 2440 - Loss: -0.4842725694179535 - prediction 0.4842725694179535 - improvement 2\n",
      "Step 2450 - Loss: -0.19063079357147217 - prediction 0.19063079357147217 - improvement 1\n",
      "Step 2460 - Loss: -0.19069164991378784 - prediction 0.19069164991378784 - improvement 1\n",
      "Step 2470 - Loss: -0.3254390060901642 - prediction 0.3254390060901642 - improvement 0\n",
      "Step 2480 - Loss: -0.325436532497406 - prediction 0.325436532497406 - improvement 0\n",
      "Step 2490 - Loss: -0.32531461119651794 - prediction 0.32531461119651794 - improvement 0\n",
      "Step 2500 - Loss: -0.32533547282218933 - prediction 0.32533547282218933 - improvement 0\n",
      "Step 2510 - Loss: -0.48387154936790466 - prediction 0.48387154936790466 - improvement 2\n",
      "Step 2520 - Loss: -0.3254500925540924 - prediction 0.3254500925540924 - improvement 0\n",
      "Step 2530 - Loss: -0.32542043924331665 - prediction 0.32542043924331665 - improvement 0\n",
      "Step 2540 - Loss: -0.3254091441631317 - prediction 0.3254091441631317 - improvement 0\n",
      "Step 2550 - Loss: -0.484140008687973 - prediction 0.484140008687973 - improvement 2\n",
      "Step 2560 - Loss: -0.3256503939628601 - prediction 0.3256503939628601 - improvement 0\n",
      "Step 2570 - Loss: -0.19144582748413086 - prediction 0.19144582748413086 - improvement 1\n",
      "Step 2580 - Loss: -0.32563531398773193 - prediction 0.32563531398773193 - improvement 0\n",
      "Step 2590 - Loss: -0.32564055919647217 - prediction 0.32564055919647217 - improvement 0\n",
      "Step 2600 - Loss: -0.48305362462997437 - prediction 0.48305362462997437 - improvement 2\n",
      "Step 2610 - Loss: -0.4831582009792328 - prediction 0.4831582009792328 - improvement 2\n",
      "Step 2620 - Loss: -0.3256029486656189 - prediction 0.3256029486656189 - improvement 0\n",
      "Step 2630 - Loss: -0.19143712520599365 - prediction 0.19143712520599365 - improvement 1\n",
      "Step 2640 - Loss: -0.4828871190547943 - prediction 0.4828871190547943 - improvement 2\n",
      "Step 2650 - Loss: -0.32576948404312134 - prediction 0.32576948404312134 - improvement 0\n",
      "Step 2660 - Loss: -0.19151124358177185 - prediction 0.19151124358177185 - improvement 1\n",
      "Step 2670 - Loss: -0.19146567583084106 - prediction 0.19146567583084106 - improvement 1\n",
      "Step 2680 - Loss: -0.19128891825675964 - prediction 0.19128891825675964 - improvement 1\n",
      "Step 2690 - Loss: -0.4831993281841278 - prediction 0.4831993281841278 - improvement 2\n",
      "Step 2700 - Loss: -0.3255939185619354 - prediction 0.3255939185619354 - improvement 0\n",
      "Step 2710 - Loss: -0.3254372477531433 - prediction 0.3254372477531433 - improvement 0\n",
      "Step 2720 - Loss: -0.3252820670604706 - prediction 0.3252820670604706 - improvement 0\n",
      "Step 2730 - Loss: -0.48386862874031067 - prediction 0.48386862874031067 - improvement 2\n",
      "Step 2740 - Loss: -0.4840649962425232 - prediction 0.4840649962425232 - improvement 2\n",
      "Step 2750 - Loss: -0.4839363098144531 - prediction 0.4839363098144531 - improvement 2\n",
      "Step 2760 - Loss: -0.48367953300476074 - prediction 0.48367953300476074 - improvement 2\n",
      "Step 2770 - Loss: -0.3254893720149994 - prediction 0.3254893720149994 - improvement 0\n",
      "Step 2780 - Loss: -0.4837367832660675 - prediction 0.4837367832660675 - improvement 2\n",
      "Step 2790 - Loss: -0.32530006766319275 - prediction 0.32530006766319275 - improvement 0\n",
      "Step 2800 - Loss: -0.3252660930156708 - prediction 0.3252660930156708 - improvement 0\n",
      "Step 2810 - Loss: -0.4847574830055237 - prediction 0.4847574830055237 - improvement 2\n",
      "Step 2820 - Loss: -0.48458218574523926 - prediction 0.48458218574523926 - improvement 2\n",
      "Step 2830 - Loss: -0.48453354835510254 - prediction 0.48453354835510254 - improvement 2\n",
      "Step 2840 - Loss: -0.48437702655792236 - prediction 0.48437702655792236 - improvement 2\n",
      "Step 2850 - Loss: -0.4843636453151703 - prediction 0.4843636453151703 - improvement 2\n",
      "Step 2860 - Loss: -0.3251582384109497 - prediction 0.3251582384109497 - improvement 0\n",
      "Step 2870 - Loss: -0.4845544099807739 - prediction 0.4845544099807739 - improvement 2\n",
      "Step 2880 - Loss: -0.3252749443054199 - prediction 0.3252749443054199 - improvement 0\n",
      "Step 2890 - Loss: -0.4841923415660858 - prediction 0.4841923415660858 - improvement 2\n",
      "Step 2900 - Loss: -0.4841248691082001 - prediction 0.4841248691082001 - improvement 2\n",
      "Step 2910 - Loss: -0.48394155502319336 - prediction 0.48394155502319336 - improvement 2\n",
      "Step 2920 - Loss: -0.19080713391304016 - prediction 0.19080713391304016 - improvement 1\n",
      "Step 2930 - Loss: -0.3254013955593109 - prediction 0.3254013955593109 - improvement 0\n",
      "Step 2940 - Loss: -0.4839397668838501 - prediction 0.4839397668838501 - improvement 2\n",
      "Step 2950 - Loss: -0.4837000072002411 - prediction 0.4837000072002411 - improvement 2\n",
      "Step 2960 - Loss: -0.3254421055316925 - prediction 0.3254421055316925 - improvement 0\n",
      "Step 2970 - Loss: -0.4841446578502655 - prediction 0.4841446578502655 - improvement 2\n",
      "Step 2980 - Loss: -0.19072090089321136 - prediction 0.19072090089321136 - improvement 1\n",
      "Step 2990 - Loss: -0.32543984055519104 - prediction 0.32543984055519104 - improvement 0\n",
      "Step 3000 - Loss: -0.4839327335357666 - prediction 0.4839327335357666 - improvement 2\n",
      "Step 3010 - Loss: -0.32530468702316284 - prediction 0.32530468702316284 - improvement 0\n",
      "Step 3020 - Loss: -0.3252851963043213 - prediction 0.3252851963043213 - improvement 0\n",
      "Step 3030 - Loss: -0.3252313435077667 - prediction 0.3252313435077667 - improvement 0\n",
      "Step 3040 - Loss: -0.32527318596839905 - prediction 0.32527318596839905 - improvement 0\n",
      "Step 3050 - Loss: -0.3252428472042084 - prediction 0.3252428472042084 - improvement 0\n",
      "Step 3060 - Loss: -0.48439788818359375 - prediction 0.48439788818359375 - improvement 2\n",
      "Step 3070 - Loss: -0.48457637429237366 - prediction 0.48457637429237366 - improvement 2\n",
      "Step 3080 - Loss: -0.3251180648803711 - prediction 0.3251180648803711 - improvement 0\n",
      "Step 3090 - Loss: -0.325088232755661 - prediction 0.325088232755661 - improvement 0\n",
      "Step 3100 - Loss: -0.32496917247772217 - prediction 0.32496917247772217 - improvement 0\n",
      "Step 3110 - Loss: -0.4854801595211029 - prediction 0.4854801595211029 - improvement 2\n",
      "Step 3120 - Loss: -0.4853020906448364 - prediction 0.4853020906448364 - improvement 2\n",
      "Step 3130 - Loss: -0.32488635182380676 - prediction 0.32488635182380676 - improvement 0\n",
      "Step 3140 - Loss: -0.32491496205329895 - prediction 0.32491496205329895 - improvement 0\n",
      "Step 3150 - Loss: -0.4852607250213623 - prediction 0.4852607250213623 - improvement 2\n",
      "Step 3160 - Loss: -0.48495835065841675 - prediction 0.48495835065841675 - improvement 2\n",
      "Step 3170 - Loss: -0.3251093924045563 - prediction 0.3251093924045563 - improvement 0\n",
      "Step 3180 - Loss: -0.4849197268486023 - prediction 0.4849197268486023 - improvement 2\n",
      "Step 3190 - Loss: -0.324940949678421 - prediction 0.324940949678421 - improvement 0\n",
      "Step 3200 - Loss: -0.48586544394493103 - prediction 0.48586544394493103 - improvement 2\n",
      "Step 3210 - Loss: -0.3245979845523834 - prediction 0.3245979845523834 - improvement 0\n",
      "Step 3220 - Loss: -0.4866633415222168 - prediction 0.4866633415222168 - improvement 2\n",
      "Step 3230 - Loss: -0.3245297372341156 - prediction 0.3245297372341156 - improvement 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3240 - Loss: -0.4865867793560028 - prediction 0.4865867793560028 - improvement 2\n",
      "Step 3250 - Loss: -0.48656922578811646 - prediction 0.48656922578811646 - improvement 2\n",
      "Step 3260 - Loss: -0.48675447702407837 - prediction 0.48675447702407837 - improvement 2\n",
      "Step 3270 - Loss: -0.3245815932750702 - prediction 0.3245815932750702 - improvement 0\n",
      "Step 3280 - Loss: -0.4866684079170227 - prediction 0.4866684079170227 - improvement 2\n",
      "Step 3290 - Loss: -0.3244607150554657 - prediction 0.3244607150554657 - improvement 0\n",
      "Step 3300 - Loss: -0.3242935240268707 - prediction 0.3242935240268707 - improvement 0\n",
      "Step 3310 - Loss: -0.48796749114990234 - prediction 0.48796749114990234 - improvement 2\n",
      "Step 3320 - Loss: -0.4877346158027649 - prediction 0.4877346158027649 - improvement 2\n",
      "Step 3330 - Loss: -0.4875023663043976 - prediction 0.4875023663043976 - improvement 2\n",
      "Step 3340 - Loss: -0.4873381555080414 - prediction 0.4873381555080414 - improvement 2\n",
      "Step 3350 - Loss: -0.3242737650871277 - prediction 0.3242737650871277 - improvement 0\n",
      "Step 3360 - Loss: -0.18824222683906555 - prediction 0.18824222683906555 - improvement 1\n",
      "Step 3370 - Loss: -0.324188232421875 - prediction 0.324188232421875 - improvement 0\n",
      "Step 3380 - Loss: -0.4879087209701538 - prediction 0.4879087209701538 - improvement 2\n",
      "Step 3390 - Loss: -0.18806946277618408 - prediction 0.18806946277618408 - improvement 1\n",
      "Step 3400 - Loss: -0.18839320540428162 - prediction 0.18839320540428162 - improvement 1\n",
      "Step 3410 - Loss: -0.3243197798728943 - prediction 0.3243197798728943 - improvement 0\n",
      "Step 3420 - Loss: -0.48668885231018066 - prediction 0.48668885231018066 - improvement 2\n",
      "Step 3430 - Loss: -0.48671939969062805 - prediction 0.48671939969062805 - improvement 2\n",
      "Step 3440 - Loss: -0.32452234625816345 - prediction 0.32452234625816345 - improvement 0\n",
      "Step 3450 - Loss: -0.48676133155822754 - prediction 0.48676133155822754 - improvement 2\n",
      "Step 3460 - Loss: -0.32447007298469543 - prediction 0.32447007298469543 - improvement 0\n",
      "Step 3470 - Loss: -0.3243648111820221 - prediction 0.3243648111820221 - improvement 0\n",
      "Step 3480 - Loss: -0.32424551248550415 - prediction 0.32424551248550415 - improvement 0\n",
      "Step 3490 - Loss: -0.18801987171173096 - prediction 0.18801987171173096 - improvement 1\n",
      "Step 3500 - Loss: -0.4881366491317749 - prediction 0.4881366491317749 - improvement 2\n",
      "Step 3510 - Loss: -0.18826352059841156 - prediction 0.18826352059841156 - improvement 1\n",
      "Step 3520 - Loss: -0.4873915910720825 - prediction 0.4873915910720825 - improvement 2\n",
      "Step 3530 - Loss: -0.18853949010372162 - prediction 0.18853949010372162 - improvement 1\n",
      "Step 3540 - Loss: -0.48701125383377075 - prediction 0.48701125383377075 - improvement 2\n",
      "Step 3550 - Loss: -0.3244350850582123 - prediction 0.3244350850582123 - improvement 0\n",
      "Step 3560 - Loss: -0.48690274357795715 - prediction 0.48690274357795715 - improvement 2\n",
      "Step 3570 - Loss: -0.32440778613090515 - prediction 0.32440778613090515 - improvement 0\n",
      "Step 3580 - Loss: -0.4870631694793701 - prediction 0.4870631694793701 - improvement 2\n",
      "Step 3590 - Loss: -0.4864806830883026 - prediction 0.4864806830883026 - improvement 2\n",
      "Step 3600 - Loss: -0.4863414466381073 - prediction 0.4863414466381073 - improvement 2\n",
      "Step 3610 - Loss: -0.32466158270835876 - prediction 0.32466158270835876 - improvement 0\n",
      "Step 3620 - Loss: -0.3245023488998413 - prediction 0.3245023488998413 - improvement 0\n",
      "Step 3630 - Loss: -0.3244244456291199 - prediction 0.3244244456291199 - improvement 0\n",
      "Step 3640 - Loss: -0.48717427253723145 - prediction 0.48717427253723145 - improvement 2\n",
      "Step 3650 - Loss: -0.32426905632019043 - prediction 0.32426905632019043 - improvement 0\n",
      "Step 3660 - Loss: -0.32427743077278137 - prediction 0.32427743077278137 - improvement 0\n",
      "Step 3670 - Loss: -0.4874359965324402 - prediction 0.4874359965324402 - improvement 2\n",
      "Step 3680 - Loss: -0.4868699610233307 - prediction 0.4868699610233307 - improvement 2\n",
      "Step 3690 - Loss: -0.32441338896751404 - prediction 0.32441338896751404 - improvement 0\n",
      "Step 3700 - Loss: -0.48708686232566833 - prediction 0.48708686232566833 - improvement 2\n",
      "Step 3710 - Loss: -0.3242955207824707 - prediction 0.3242955207824707 - improvement 0\n",
      "Step 3720 - Loss: -0.1884230375289917 - prediction 0.1884230375289917 - improvement 1\n",
      "Step 3730 - Loss: -0.48707497119903564 - prediction 0.48707497119903564 - improvement 2\n",
      "Step 3740 - Loss: -0.48696210980415344 - prediction 0.48696210980415344 - improvement 2\n",
      "Step 3750 - Loss: -0.32451874017715454 - prediction 0.32451874017715454 - improvement 0\n",
      "Step 3760 - Loss: -0.18852493166923523 - prediction 0.18852493166923523 - improvement 1\n",
      "Step 3770 - Loss: -0.3245648443698883 - prediction 0.3245648443698883 - improvement 0\n",
      "Step 3780 - Loss: -0.3245275914669037 - prediction 0.3245275914669037 - improvement 0\n",
      "Step 3790 - Loss: -0.4869344234466553 - prediction 0.4869344234466553 - improvement 2\n",
      "Step 3800 - Loss: -0.18854331970214844 - prediction 0.18854331970214844 - improvement 1\n",
      "Step 3810 - Loss: -0.3243264853954315 - prediction 0.3243264853954315 - improvement 0\n",
      "Step 3820 - Loss: -0.18811152875423431 - prediction 0.18811152875423431 - improvement 1\n",
      "Step 3830 - Loss: -0.32427307963371277 - prediction 0.32427307963371277 - improvement 0\n",
      "Step 3840 - Loss: -0.48711729049682617 - prediction 0.48711729049682617 - improvement 2\n",
      "Step 3850 - Loss: -0.4869649410247803 - prediction 0.4869649410247803 - improvement 2\n",
      "Step 3860 - Loss: -0.18857286870479584 - prediction 0.18857286870479584 - improvement 1\n",
      "Step 3870 - Loss: -0.32442858815193176 - prediction 0.32442858815193176 - improvement 0\n",
      "Step 3880 - Loss: -0.3244531452655792 - prediction 0.3244531452655792 - improvement 0\n",
      "Step 3890 - Loss: -0.32451188564300537 - prediction 0.32451188564300537 - improvement 0\n",
      "Step 3900 - Loss: -0.48682472109794617 - prediction 0.48682472109794617 - improvement 2\n",
      "Step 3910 - Loss: -0.48657655715942383 - prediction 0.48657655715942383 - improvement 2\n",
      "Step 3920 - Loss: -0.3245750963687897 - prediction 0.3245750963687897 - improvement 0\n",
      "Step 3930 - Loss: -0.3244992792606354 - prediction 0.3244992792606354 - improvement 0\n",
      "Step 3940 - Loss: -0.48700326681137085 - prediction 0.48700326681137085 - improvement 2\n",
      "Step 3950 - Loss: -0.3243577480316162 - prediction 0.3243577480316162 - improvement 0\n",
      "Step 3960 - Loss: -0.48727959394454956 - prediction 0.48727959394454956 - improvement 2\n",
      "Step 3970 - Loss: -0.32443001866340637 - prediction 0.32443001866340637 - improvement 0\n",
      "Step 3980 - Loss: -0.487055242061615 - prediction 0.487055242061615 - improvement 2\n",
      "Step 3990 - Loss: -0.32440274953842163 - prediction 0.32440274953842163 - improvement 0\n",
      "Step 4000 - Loss: -0.18836189806461334 - prediction 0.18836189806461334 - improvement 1\n",
      "Step 4010 - Loss: -0.18841461837291718 - prediction 0.18841461837291718 - improvement 1\n",
      "Step 4020 - Loss: -0.4873652160167694 - prediction 0.4873652160167694 - improvement 2\n",
      "Step 4030 - Loss: -0.48730722069740295 - prediction 0.48730722069740295 - improvement 2\n",
      "Step 4040 - Loss: -0.4869411885738373 - prediction 0.4869411885738373 - improvement 2\n",
      "Step 4050 - Loss: -0.48669958114624023 - prediction 0.48669958114624023 - improvement 2\n",
      "Step 4060 - Loss: -0.32462602853775024 - prediction 0.32462602853775024 - improvement 0\n",
      "Step 4070 - Loss: -0.4864789843559265 - prediction 0.4864789843559265 - improvement 2\n",
      "Step 4080 - Loss: -0.4862060248851776 - prediction 0.4862060248851776 - improvement 2\n",
      "Step 4090 - Loss: -0.48629137873649597 - prediction 0.48629137873649597 - improvement 2\n",
      "Step 4100 - Loss: -0.3248694837093353 - prediction 0.3248694837093353 - improvement 0\n",
      "Step 4110 - Loss: -0.485887736082077 - prediction 0.485887736082077 - improvement 2\n",
      "Step 4120 - Loss: -0.485603928565979 - prediction 0.485603928565979 - improvement 2\n",
      "Step 4130 - Loss: -0.18988798558712006 - prediction 0.18988798558712006 - improvement 1\n",
      "Step 4140 - Loss: -0.48496127128601074 - prediction 0.48496127128601074 - improvement 2\n",
      "Step 4150 - Loss: -0.32503408193588257 - prediction 0.32503408193588257 - improvement 0\n",
      "Step 4160 - Loss: -0.3250630795955658 - prediction 0.3250630795955658 - improvement 0\n",
      "Step 4170 - Loss: -0.4849162697792053 - prediction 0.4849162697792053 - improvement 2\n",
      "Step 4180 - Loss: -0.4847777187824249 - prediction 0.4847777187824249 - improvement 2\n",
      "Step 4190 - Loss: -0.4843631088733673 - prediction 0.4843631088733673 - improvement 2\n",
      "Step 4200 - Loss: -0.4843085706233978 - prediction 0.4843085706233978 - improvement 2\n",
      "Step 4210 - Loss: -0.4847256541252136 - prediction 0.4847256541252136 - improvement 2\n",
      "Step 4220 - Loss: -0.3251369297504425 - prediction 0.3251369297504425 - improvement 0\n",
      "Step 4230 - Loss: -0.32507431507110596 - prediction 0.32507431507110596 - improvement 0\n",
      "Step 4240 - Loss: -0.48478230834007263 - prediction 0.48478230834007263 - improvement 2\n",
      "Step 4250 - Loss: -0.48420631885528564 - prediction 0.48420631885528564 - improvement 2\n",
      "Step 4260 - Loss: -0.4840620756149292 - prediction 0.4840620756149292 - improvement 2\n",
      "Step 4270 - Loss: -0.19063688814640045 - prediction 0.19063688814640045 - improvement 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4280 - Loss: -0.48402541875839233 - prediction 0.48402541875839233 - improvement 2\n",
      "Step 4290 - Loss: -0.4836982190608978 - prediction 0.4836982190608978 - improvement 2\n",
      "Step 4300 - Loss: -0.4839368462562561 - prediction 0.4839368462562561 - improvement 2\n",
      "Step 4310 - Loss: -0.32539117336273193 - prediction 0.32539117336273193 - improvement 0\n",
      "Step 4320 - Loss: -0.32554006576538086 - prediction 0.32554006576538086 - improvement 0\n",
      "Step 4330 - Loss: -0.32548198103904724 - prediction 0.32548198103904724 - improvement 0\n",
      "Step 4340 - Loss: -0.4835643768310547 - prediction 0.4835643768310547 - improvement 2\n",
      "Step 4350 - Loss: -0.4835497736930847 - prediction 0.4835497736930847 - improvement 2\n",
      "Step 4360 - Loss: -0.19136685132980347 - prediction 0.19136685132980347 - improvement 1\n",
      "Step 4370 - Loss: -0.4824545979499817 - prediction 0.4824545979499817 - improvement 2\n",
      "Step 4380 - Loss: -0.1912156045436859 - prediction 0.1912156045436859 - improvement 1\n",
      "Step 4390 - Loss: -0.19118022918701172 - prediction 0.19118022918701172 - improvement 1\n",
      "Step 4400 - Loss: -0.4832978844642639 - prediction 0.4832978844642639 - improvement 2\n",
      "Step 4410 - Loss: -0.3255431056022644 - prediction 0.3255431056022644 - improvement 0\n",
      "Step 4420 - Loss: -0.48336705565452576 - prediction 0.48336705565452576 - improvement 2\n",
      "Step 4430 - Loss: -0.32549604773521423 - prediction 0.32549604773521423 - improvement 0\n",
      "Step 4440 - Loss: -0.48385581374168396 - prediction 0.48385581374168396 - improvement 2\n",
      "Step 4450 - Loss: -0.48418423533439636 - prediction 0.48418423533439636 - improvement 2\n",
      "Step 4460 - Loss: -0.48377999663352966 - prediction 0.48377999663352966 - improvement 2\n",
      "Step 4470 - Loss: -0.4839828610420227 - prediction 0.4839828610420227 - improvement 2\n",
      "Step 4480 - Loss: -0.3254352807998657 - prediction 0.3254352807998657 - improvement 0\n",
      "Step 4490 - Loss: -0.32545167207717896 - prediction 0.32545167207717896 - improvement 0\n",
      "Step 4500 - Loss: -0.325368732213974 - prediction 0.325368732213974 - improvement 0\n",
      "Step 4510 - Loss: -0.4833389222621918 - prediction 0.4833389222621918 - improvement 2\n",
      "Step 4520 - Loss: -0.32571539282798767 - prediction 0.32571539282798767 - improvement 0\n",
      "Step 4530 - Loss: -0.1915162205696106 - prediction 0.1915162205696106 - improvement 1\n",
      "Step 4540 - Loss: -0.4830189347267151 - prediction 0.4830189347267151 - improvement 2\n",
      "Step 4550 - Loss: -0.4827675223350525 - prediction 0.4827675223350525 - improvement 2\n",
      "Step 4560 - Loss: -0.4827539622783661 - prediction 0.4827539622783661 - improvement 2\n",
      "Step 4570 - Loss: -0.3257226049900055 - prediction 0.3257226049900055 - improvement 0\n",
      "Step 4580 - Loss: -0.4832228124141693 - prediction 0.4832228124141693 - improvement 2\n",
      "Step 4590 - Loss: -0.32561245560646057 - prediction 0.32561245560646057 - improvement 0\n",
      "Step 4600 - Loss: -0.3255210816860199 - prediction 0.3255210816860199 - improvement 0\n",
      "Step 4610 - Loss: -0.3256224989891052 - prediction 0.3256224989891052 - improvement 0\n",
      "Step 4620 - Loss: -0.4827156364917755 - prediction 0.4827156364917755 - improvement 2\n",
      "Step 4630 - Loss: -0.32576560974121094 - prediction 0.32576560974121094 - improvement 0\n",
      "Step 4640 - Loss: -0.48310592770576477 - prediction 0.48310592770576477 - improvement 2\n",
      "Step 4650 - Loss: -0.48311060667037964 - prediction 0.48311060667037964 - improvement 2\n",
      "Step 4660 - Loss: -0.48302069306373596 - prediction 0.48302069306373596 - improvement 2\n",
      "Step 4670 - Loss: -0.32560059428215027 - prediction 0.32560059428215027 - improvement 0\n",
      "Step 4680 - Loss: -0.48320984840393066 - prediction 0.48320984840393066 - improvement 2\n",
      "Step 4690 - Loss: -0.48348772525787354 - prediction 0.48348772525787354 - improvement 2\n",
      "Step 4700 - Loss: -0.3255045413970947 - prediction 0.3255045413970947 - improvement 0\n",
      "Step 4710 - Loss: -0.32541975378990173 - prediction 0.32541975378990173 - improvement 0\n",
      "Step 4720 - Loss: -0.48375311493873596 - prediction 0.48375311493873596 - improvement 2\n",
      "Step 4730 - Loss: -0.4836643636226654 - prediction 0.4836643636226654 - improvement 2\n",
      "Step 4740 - Loss: -0.48340165615081787 - prediction 0.48340165615081787 - improvement 2\n",
      "Step 4750 - Loss: -0.3255784213542938 - prediction 0.3255784213542938 - improvement 0\n",
      "Step 4760 - Loss: -0.32565706968307495 - prediction 0.32565706968307495 - improvement 0\n",
      "Step 4770 - Loss: -0.32555311918258667 - prediction 0.32555311918258667 - improvement 0\n",
      "Step 4780 - Loss: -0.32551583647727966 - prediction 0.32551583647727966 - improvement 0\n",
      "Step 4790 - Loss: -0.48318052291870117 - prediction 0.48318052291870117 - improvement 2\n",
      "Step 4800 - Loss: -0.4831288754940033 - prediction 0.4831288754940033 - improvement 2\n",
      "Step 4810 - Loss: -0.3257278800010681 - prediction 0.3257278800010681 - improvement 0\n",
      "Step 4820 - Loss: -0.32563215494155884 - prediction 0.32563215494155884 - improvement 0\n",
      "Step 4830 - Loss: -0.48360180854797363 - prediction 0.48360180854797363 - improvement 2\n",
      "Step 4840 - Loss: -0.48336297273635864 - prediction 0.48336297273635864 - improvement 2\n",
      "Step 4850 - Loss: -0.32555821537971497 - prediction 0.32555821537971497 - improvement 0\n",
      "Step 4860 - Loss: -0.32556629180908203 - prediction 0.32556629180908203 - improvement 0\n",
      "Step 4870 - Loss: -0.4833706021308899 - prediction 0.4833706021308899 - improvement 2\n",
      "Step 4880 - Loss: -0.32565808296203613 - prediction 0.32565808296203613 - improvement 0\n",
      "Step 4890 - Loss: -0.32560721039772034 - prediction 0.32560721039772034 - improvement 0\n",
      "Step 4900 - Loss: -0.3255119025707245 - prediction 0.3255119025707245 - improvement 0\n",
      "Step 4910 - Loss: -0.1911046952009201 - prediction 0.1911046952009201 - improvement 1\n",
      "Step 4920 - Loss: -0.4832134246826172 - prediction 0.4832134246826172 - improvement 2\n",
      "Step 4930 - Loss: -0.48297247290611267 - prediction 0.48297247290611267 - improvement 2\n",
      "Step 4940 - Loss: -0.3257404565811157 - prediction 0.3257404565811157 - improvement 0\n",
      "Step 4950 - Loss: -0.19165655970573425 - prediction 0.19165655970573425 - improvement 1\n",
      "Step 4960 - Loss: -0.4825863838195801 - prediction 0.4825863838195801 - improvement 2\n",
      "Step 4970 - Loss: -0.32570716738700867 - prediction 0.32570716738700867 - improvement 0\n",
      "Step 4980 - Loss: -0.32571282982826233 - prediction 0.32571282982826233 - improvement 0\n",
      "Step 4990 - Loss: -0.48281586170196533 - prediction 0.48281586170196533 - improvement 2\n",
      "Step 5000 - Loss: -0.4819721281528473 - prediction 0.4819721281528473 - improvement 2\n",
      "Step 5010 - Loss: -0.32610946893692017 - prediction 0.32610946893692017 - improvement 0\n",
      "Step 5020 - Loss: -0.19230212271213531 - prediction 0.19230212271213531 - improvement 1\n",
      "Step 5030 - Loss: -0.48129498958587646 - prediction 0.48129498958587646 - improvement 2\n",
      "Step 5040 - Loss: -0.3261502981185913 - prediction 0.3261502981185913 - improvement 0\n",
      "Step 5050 - Loss: -0.192464679479599 - prediction 0.192464679479599 - improvement 1\n",
      "Step 5060 - Loss: -0.32605940103530884 - prediction 0.32605940103530884 - improvement 0\n",
      "Step 5070 - Loss: -0.4817889630794525 - prediction 0.4817889630794525 - improvement 2\n",
      "Step 5080 - Loss: -0.32584038376808167 - prediction 0.32584038376808167 - improvement 0\n",
      "Step 5090 - Loss: -0.3257553279399872 - prediction 0.3257553279399872 - improvement 0\n",
      "Step 5100 - Loss: -0.32572102546691895 - prediction 0.32572102546691895 - improvement 0\n",
      "Step 5110 - Loss: -0.4826660752296448 - prediction 0.4826660752296448 - improvement 2\n",
      "Step 5120 - Loss: -0.32576844096183777 - prediction 0.32576844096183777 - improvement 0\n",
      "Step 5130 - Loss: -0.4829253852367401 - prediction 0.4829253852367401 - improvement 2\n",
      "Step 5140 - Loss: -0.4833552837371826 - prediction 0.4833552837371826 - improvement 2\n",
      "Step 5150 - Loss: -0.48337411880493164 - prediction 0.48337411880493164 - improvement 2\n",
      "Step 5160 - Loss: -0.3255193531513214 - prediction 0.3255193531513214 - improvement 0\n",
      "Step 5170 - Loss: -0.48327264189720154 - prediction 0.48327264189720154 - improvement 2\n",
      "Step 5180 - Loss: -0.48322397470474243 - prediction 0.48322397470474243 - improvement 2\n",
      "Step 5190 - Loss: -0.3256935477256775 - prediction 0.3256935477256775 - improvement 0\n",
      "Step 5200 - Loss: -0.19146443903446198 - prediction 0.19146443903446198 - improvement 1\n",
      "Step 5210 - Loss: -0.32573381066322327 - prediction 0.32573381066322327 - improvement 0\n",
      "Step 5220 - Loss: -0.3256666958332062 - prediction 0.3256666958332062 - improvement 0\n",
      "Step 5230 - Loss: -0.48306187987327576 - prediction 0.48306187987327576 - improvement 2\n",
      "Step 5240 - Loss: -0.4830736219882965 - prediction 0.4830736219882965 - improvement 2\n",
      "Step 5250 - Loss: -0.32579606771469116 - prediction 0.32579606771469116 - improvement 0\n",
      "Step 5260 - Loss: -0.3257232904434204 - prediction 0.3257232904434204 - improvement 0\n",
      "Step 5270 - Loss: -0.48284292221069336 - prediction 0.48284292221069336 - improvement 2\n",
      "Step 5280 - Loss: -0.48298949003219604 - prediction 0.48298949003219604 - improvement 2\n",
      "Step 5290 - Loss: -0.19115352630615234 - prediction 0.19115352630615234 - improvement 1\n",
      "Step 5300 - Loss: -0.4834408760070801 - prediction 0.4834408760070801 - improvement 2\n",
      "Step 5310 - Loss: -0.32563984394073486 - prediction 0.32563984394073486 - improvement 0\n",
      "Step 5320 - Loss: -0.3255738615989685 - prediction 0.3255738615989685 - improvement 0\n",
      "Step 5330 - Loss: -0.19119997322559357 - prediction 0.19119997322559357 - improvement 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5340 - Loss: -0.4833049476146698 - prediction 0.4833049476146698 - improvement 2\n",
      "Step 5350 - Loss: -0.4838138222694397 - prediction 0.4838138222694397 - improvement 2\n",
      "Step 5360 - Loss: -0.48394790291786194 - prediction 0.48394790291786194 - improvement 2\n",
      "Step 5370 - Loss: -0.3254519999027252 - prediction 0.3254519999027252 - improvement 0\n",
      "Step 5380 - Loss: -0.4837087094783783 - prediction 0.4837087094783783 - improvement 2\n",
      "Step 5390 - Loss: -0.4837939739227295 - prediction 0.4837939739227295 - improvement 2\n",
      "Step 5400 - Loss: -0.4837069809436798 - prediction 0.4837069809436798 - improvement 2\n",
      "Step 5410 - Loss: -0.19116955995559692 - prediction 0.19116955995559692 - improvement 1\n",
      "Step 5420 - Loss: -0.48290130496025085 - prediction 0.48290130496025085 - improvement 2\n",
      "Step 5430 - Loss: -0.32571175694465637 - prediction 0.32571175694465637 - improvement 0\n",
      "Step 5440 - Loss: -0.4826737344264984 - prediction 0.4826737344264984 - improvement 2\n",
      "Step 5450 - Loss: -0.48251786828041077 - prediction 0.48251786828041077 - improvement 2\n",
      "Step 5460 - Loss: -0.4823113679885864 - prediction 0.4823113679885864 - improvement 2\n",
      "Step 5470 - Loss: -0.4822770357131958 - prediction 0.4822770357131958 - improvement 2\n",
      "Step 5480 - Loss: -0.3258921802043915 - prediction 0.3258921802043915 - improvement 0\n",
      "Step 5490 - Loss: -0.48229360580444336 - prediction 0.48229360580444336 - improvement 2\n",
      "Step 5500 - Loss: -0.3258973956108093 - prediction 0.3258973956108093 - improvement 0\n",
      "Step 5510 - Loss: -0.32589271664619446 - prediction 0.32589271664619446 - improvement 0\n",
      "Step 5520 - Loss: -0.48245692253112793 - prediction 0.48245692253112793 - improvement 2\n",
      "Step 5530 - Loss: -0.19158633053302765 - prediction 0.19158633053302765 - improvement 1\n",
      "Step 5540 - Loss: -0.48273923993110657 - prediction 0.48273923993110657 - improvement 2\n",
      "Step 5550 - Loss: -0.3257925510406494 - prediction 0.3257925510406494 - improvement 0\n",
      "Step 5560 - Loss: -0.4827580749988556 - prediction 0.4827580749988556 - improvement 2\n",
      "Step 5570 - Loss: -0.3257483243942261 - prediction 0.3257483243942261 - improvement 0\n",
      "Step 5580 - Loss: -0.4823906719684601 - prediction 0.4823906719684601 - improvement 2\n",
      "Step 5590 - Loss: -0.4822331666946411 - prediction 0.4822331666946411 - improvement 2\n",
      "Step 5600 - Loss: -0.3259682357311249 - prediction 0.3259682357311249 - improvement 0\n",
      "Step 5610 - Loss: -0.32585957646369934 - prediction 0.32585957646369934 - improvement 0\n",
      "Step 5620 - Loss: -0.4819946587085724 - prediction 0.4819946587085724 - improvement 2\n",
      "Step 5630 - Loss: -0.48206058144569397 - prediction 0.48206058144569397 - improvement 2\n",
      "Step 5640 - Loss: -0.48159298300743103 - prediction 0.48159298300743103 - improvement 2\n",
      "Step 5650 - Loss: -0.48181039094924927 - prediction 0.48181039094924927 - improvement 2\n",
      "Step 5660 - Loss: -0.4815088212490082 - prediction 0.4815088212490082 - improvement 2\n",
      "Step 5670 - Loss: -0.48158523440361023 - prediction 0.48158523440361023 - improvement 2\n",
      "Step 5680 - Loss: -0.32609283924102783 - prediction 0.32609283924102783 - improvement 0\n",
      "Step 5690 - Loss: -0.4813045561313629 - prediction 0.4813045561313629 - improvement 2\n",
      "Step 5700 - Loss: -0.1926824152469635 - prediction 0.1926824152469635 - improvement 1\n",
      "Step 5710 - Loss: -0.326074481010437 - prediction 0.326074481010437 - improvement 0\n",
      "Step 5720 - Loss: -0.4818127155303955 - prediction 0.4818127155303955 - improvement 2\n",
      "Step 5730 - Loss: -0.48157450556755066 - prediction 0.48157450556755066 - improvement 2\n",
      "Step 5740 - Loss: -0.19228731095790863 - prediction 0.19228731095790863 - improvement 1\n",
      "Step 5750 - Loss: -0.48179134726524353 - prediction 0.48179134726524353 - improvement 2\n",
      "Step 5760 - Loss: -0.48201248049736023 - prediction 0.48201248049736023 - improvement 2\n",
      "Step 5770 - Loss: -0.48178958892822266 - prediction 0.48178958892822266 - improvement 2\n",
      "Step 5780 - Loss: -0.3261604905128479 - prediction 0.3261604905128479 - improvement 0\n",
      "Step 5790 - Loss: -0.4810938239097595 - prediction 0.4810938239097595 - improvement 2\n",
      "Step 5800 - Loss: -0.3263359069824219 - prediction 0.3263359069824219 - improvement 0\n",
      "Step 5810 - Loss: -0.326241672039032 - prediction 0.326241672039032 - improvement 0\n",
      "Step 5820 - Loss: -0.32623669505119324 - prediction 0.32623669505119324 - improvement 0\n",
      "Step 5830 - Loss: -0.32624271512031555 - prediction 0.32624271512031555 - improvement 0\n",
      "Step 5840 - Loss: -0.326156884431839 - prediction 0.326156884431839 - improvement 0\n",
      "Step 5850 - Loss: -0.32605764269828796 - prediction 0.32605764269828796 - improvement 0\n",
      "Step 5860 - Loss: -0.19211748242378235 - prediction 0.19211748242378235 - improvement 1\n",
      "Step 5870 - Loss: -0.19244049489498138 - prediction 0.19244049489498138 - improvement 1\n",
      "Step 5880 - Loss: -0.48160967230796814 - prediction 0.48160967230796814 - improvement 2\n",
      "Step 5890 - Loss: -0.32604676485061646 - prediction 0.32604676485061646 - improvement 0\n",
      "Step 5900 - Loss: -0.4814796447753906 - prediction 0.4814796447753906 - improvement 2\n",
      "Step 5910 - Loss: -0.32614272832870483 - prediction 0.32614272832870483 - improvement 0\n",
      "Step 5920 - Loss: -0.3261744976043701 - prediction 0.3261744976043701 - improvement 0\n",
      "Step 5930 - Loss: -0.48099374771118164 - prediction 0.48099374771118164 - improvement 2\n",
      "Step 5940 - Loss: -0.3261491060256958 - prediction 0.3261491060256958 - improvement 0\n",
      "Step 5950 - Loss: -0.19243501126766205 - prediction 0.19243501126766205 - improvement 1\n",
      "Step 5960 - Loss: -0.4814593195915222 - prediction 0.4814593195915222 - improvement 2\n",
      "Step 5970 - Loss: -0.19245705008506775 - prediction 0.19245705008506775 - improvement 1\n",
      "Step 5980 - Loss: -0.32611122727394104 - prediction 0.32611122727394104 - improvement 0\n",
      "Step 5990 - Loss: -0.4812932312488556 - prediction 0.4812932312488556 - improvement 2\n",
      "Step 6000 - Loss: -0.4813344478607178 - prediction 0.4813344478607178 - improvement 2\n",
      "Step 6010 - Loss: -0.3261219263076782 - prediction 0.3261219263076782 - improvement 0\n",
      "Step 6020 - Loss: -0.3261108696460724 - prediction 0.3261108696460724 - improvement 0\n",
      "Step 6030 - Loss: -0.481845498085022 - prediction 0.481845498085022 - improvement 2\n",
      "Step 6040 - Loss: -0.3261178135871887 - prediction 0.3261178135871887 - improvement 0\n",
      "Step 6050 - Loss: -0.32619452476501465 - prediction 0.32619452476501465 - improvement 0\n",
      "Step 6060 - Loss: -0.48149213194847107 - prediction 0.48149213194847107 - improvement 2\n",
      "Step 6070 - Loss: -0.3261478841304779 - prediction 0.3261478841304779 - improvement 0\n",
      "Step 6080 - Loss: -0.48099076747894287 - prediction 0.48099076747894287 - improvement 2\n",
      "Step 6090 - Loss: -0.32621148228645325 - prediction 0.32621148228645325 - improvement 0\n",
      "Step 6100 - Loss: -0.48138701915740967 - prediction 0.48138701915740967 - improvement 2\n",
      "Step 6110 - Loss: -0.32611677050590515 - prediction 0.32611677050590515 - improvement 0\n",
      "Step 6120 - Loss: -0.48178184032440186 - prediction 0.48178184032440186 - improvement 2\n",
      "Step 6130 - Loss: -0.32597315311431885 - prediction 0.32597315311431885 - improvement 0\n",
      "Step 6140 - Loss: -0.4817996621131897 - prediction 0.4817996621131897 - improvement 2\n",
      "Step 6150 - Loss: -0.32606858015060425 - prediction 0.32606858015060425 - improvement 0\n",
      "Step 6160 - Loss: -0.48169252276420593 - prediction 0.48169252276420593 - improvement 2\n",
      "Step 6170 - Loss: -0.32613545656204224 - prediction 0.32613545656204224 - improvement 0\n",
      "Step 6180 - Loss: -0.4818466901779175 - prediction 0.4818466901779175 - improvement 2\n",
      "Step 6190 - Loss: -0.4819946587085724 - prediction 0.4819946587085724 - improvement 2\n",
      "Step 6200 - Loss: -0.3259618282318115 - prediction 0.3259618282318115 - improvement 0\n",
      "Step 6210 - Loss: -0.3261484205722809 - prediction 0.3261484205722809 - improvement 0\n",
      "Step 6220 - Loss: -0.4816245436668396 - prediction 0.4816245436668396 - improvement 2\n",
      "Step 6230 - Loss: -0.3261018991470337 - prediction 0.3261018991470337 - improvement 0\n",
      "Step 6240 - Loss: -0.32598111033439636 - prediction 0.32598111033439636 - improvement 0\n",
      "Step 6250 - Loss: -0.32597658038139343 - prediction 0.32597658038139343 - improvement 0\n",
      "Step 6260 - Loss: -0.3259526193141937 - prediction 0.3259526193141937 - improvement 0\n",
      "Step 6270 - Loss: -0.3258044421672821 - prediction 0.3258044421672821 - improvement 0\n",
      "Step 6280 - Loss: -0.48226165771484375 - prediction 0.48226165771484375 - improvement 2\n",
      "Step 6290 - Loss: -0.4823835790157318 - prediction 0.4823835790157318 - improvement 2\n",
      "Step 6300 - Loss: -0.48200303316116333 - prediction 0.48200303316116333 - improvement 2\n",
      "Step 6310 - Loss: -0.4821276366710663 - prediction 0.4821276366710663 - improvement 2\n",
      "Step 6320 - Loss: -0.32605576515197754 - prediction 0.32605576515197754 - improvement 0\n",
      "Step 6330 - Loss: -0.3260675370693207 - prediction 0.3260675370693207 - improvement 0\n",
      "Step 6340 - Loss: -0.3260762095451355 - prediction 0.3260762095451355 - improvement 0\n",
      "Step 6350 - Loss: -0.4816180169582367 - prediction 0.4816180169582367 - improvement 2\n",
      "Step 6360 - Loss: -0.4814610779285431 - prediction 0.4814610779285431 - improvement 2\n",
      "Step 6370 - Loss: -0.3261258900165558 - prediction 0.3261258900165558 - improvement 0\n",
      "Step 6380 - Loss: -0.3261122405529022 - prediction 0.3261122405529022 - improvement 0\n",
      "Step 6390 - Loss: -0.3260860741138458 - prediction 0.3260860741138458 - improvement 0\n",
      "Step 6400 - Loss: -0.3262268602848053 - prediction 0.3262268602848053 - improvement 0\n",
      "Step 6410 - Loss: -0.32612645626068115 - prediction 0.32612645626068115 - improvement 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6420 - Loss: -0.32603806257247925 - prediction 0.32603806257247925 - improvement 0\n",
      "Step 6430 - Loss: -0.32599517703056335 - prediction 0.32599517703056335 - improvement 0\n",
      "Step 6440 - Loss: -0.3259839117527008 - prediction 0.3259839117527008 - improvement 0\n",
      "Step 6450 - Loss: -0.32591360807418823 - prediction 0.32591360807418823 - improvement 0\n",
      "Step 6460 - Loss: -0.4819287657737732 - prediction 0.4819287657737732 - improvement 2\n",
      "Step 6470 - Loss: -0.4820707142353058 - prediction 0.4820707142353058 - improvement 2\n",
      "Step 6480 - Loss: -0.325973778963089 - prediction 0.325973778963089 - improvement 0\n",
      "Step 6490 - Loss: -0.4818359613418579 - prediction 0.4818359613418579 - improvement 2\n",
      "Step 6500 - Loss: -0.48163947463035583 - prediction 0.48163947463035583 - improvement 2\n",
      "Step 6510 - Loss: -0.19247739017009735 - prediction 0.19247739017009735 - improvement 1\n",
      "Step 6520 - Loss: -0.48132845759391785 - prediction 0.48132845759391785 - improvement 2\n",
      "Step 6530 - Loss: -0.4813153147697449 - prediction 0.4813153147697449 - improvement 2\n",
      "Step 6540 - Loss: -0.3262212872505188 - prediction 0.3262212872505188 - improvement 0\n",
      "Step 6550 - Loss: -0.480910986661911 - prediction 0.480910986661911 - improvement 2\n",
      "Step 6560 - Loss: -0.3261648118495941 - prediction 0.3261648118495941 - improvement 0\n",
      "Step 6570 - Loss: -0.32629334926605225 - prediction 0.32629334926605225 - improvement 0\n",
      "Step 6580 - Loss: -0.1927541047334671 - prediction 0.1927541047334671 - improvement 1\n",
      "Step 6590 - Loss: -0.3263069689273834 - prediction 0.3263069689273834 - improvement 0\n",
      "Step 6600 - Loss: -0.32619887590408325 - prediction 0.32619887590408325 - improvement 0\n",
      "Step 6610 - Loss: -0.3262263238430023 - prediction 0.3262263238430023 - improvement 0\n",
      "Step 6620 - Loss: -0.4813607335090637 - prediction 0.4813607335090637 - improvement 2\n",
      "Step 6630 - Loss: -0.48125550150871277 - prediction 0.48125550150871277 - improvement 2\n",
      "Step 6640 - Loss: -0.3261728286743164 - prediction 0.3261728286743164 - improvement 0\n",
      "Step 6650 - Loss: -0.3261629641056061 - prediction 0.3261629641056061 - improvement 0\n",
      "Step 6660 - Loss: -0.32614028453826904 - prediction 0.32614028453826904 - improvement 0\n",
      "Step 6670 - Loss: -0.32612335681915283 - prediction 0.32612335681915283 - improvement 0\n",
      "Step 6680 - Loss: -0.4815458655357361 - prediction 0.4815458655357361 - improvement 2\n",
      "Step 6690 - Loss: -0.3261055052280426 - prediction 0.3261055052280426 - improvement 0\n",
      "Step 6700 - Loss: -0.48152321577072144 - prediction 0.48152321577072144 - improvement 2\n",
      "Step 6710 - Loss: -0.48152440786361694 - prediction 0.48152440786361694 - improvement 2\n",
      "Step 6720 - Loss: -0.1925576627254486 - prediction 0.1925576627254486 - improvement 1\n",
      "Step 6730 - Loss: -0.192616805434227 - prediction 0.192616805434227 - improvement 1\n",
      "Step 6740 - Loss: -0.4810507297515869 - prediction 0.4810507297515869 - improvement 2\n",
      "Step 6750 - Loss: -0.19271229207515717 - prediction 0.19271229207515717 - improvement 1\n",
      "Step 6760 - Loss: -0.4805857539176941 - prediction 0.4805857539176941 - improvement 2\n",
      "Step 6770 - Loss: -0.32635390758514404 - prediction 0.32635390758514404 - improvement 0\n",
      "Step 6780 - Loss: -0.4807121157646179 - prediction 0.4807121157646179 - improvement 2\n",
      "Step 6790 - Loss: -0.3263090252876282 - prediction 0.3263090252876282 - improvement 0\n",
      "Step 6800 - Loss: -0.4809625446796417 - prediction 0.4809625446796417 - improvement 2\n",
      "Step 6810 - Loss: -0.19262191653251648 - prediction 0.19262191653251648 - improvement 1\n",
      "Step 6820 - Loss: -0.326226145029068 - prediction 0.326226145029068 - improvement 0\n",
      "Step 6830 - Loss: -0.32621923089027405 - prediction 0.32621923089027405 - improvement 0\n",
      "Step 6840 - Loss: -0.19254957139492035 - prediction 0.19254957139492035 - improvement 1\n",
      "Step 6850 - Loss: -0.3261461853981018 - prediction 0.3261461853981018 - improvement 0\n",
      "Step 6860 - Loss: -0.19253897666931152 - prediction 0.19253897666931152 - improvement 1\n",
      "Step 6870 - Loss: -0.326030433177948 - prediction 0.326030433177948 - improvement 0\n",
      "Step 6880 - Loss: -0.3261352777481079 - prediction 0.3261352777481079 - improvement 0\n",
      "Step 6890 - Loss: -0.4810686707496643 - prediction 0.4810686707496643 - improvement 2\n",
      "Step 6900 - Loss: -0.32621076703071594 - prediction 0.32621076703071594 - improvement 0\n",
      "Step 6910 - Loss: -0.4811016619205475 - prediction 0.4811016619205475 - improvement 2\n",
      "Step 6920 - Loss: -0.4808371067047119 - prediction 0.4808371067047119 - improvement 2\n",
      "Step 6930 - Loss: -0.32632210850715637 - prediction 0.32632210850715637 - improvement 0\n",
      "Step 6940 - Loss: -0.4805978238582611 - prediction 0.4805978238582611 - improvement 2\n",
      "Step 6950 - Loss: -0.480624258518219 - prediction 0.480624258518219 - improvement 2\n",
      "Step 6960 - Loss: -0.4807121157646179 - prediction 0.4807121157646179 - improvement 2\n",
      "Step 6970 - Loss: -0.48044657707214355 - prediction 0.48044657707214355 - improvement 2\n",
      "Step 6980 - Loss: -0.3263695240020752 - prediction 0.3263695240020752 - improvement 0\n",
      "Step 6990 - Loss: -0.32634106278419495 - prediction 0.32634106278419495 - improvement 0\n",
      "Step 7000 - Loss: -0.19308596849441528 - prediction 0.19308596849441528 - improvement 1\n",
      "Step 7010 - Loss: -0.48064959049224854 - prediction 0.48064959049224854 - improvement 2\n",
      "Step 7020 - Loss: -0.32634034752845764 - prediction 0.32634034752845764 - improvement 0\n",
      "Step 7030 - Loss: -0.32628458738327026 - prediction 0.32628458738327026 - improvement 0\n",
      "Step 7040 - Loss: -0.32629701495170593 - prediction 0.32629701495170593 - improvement 0\n",
      "Step 7050 - Loss: -0.32627102732658386 - prediction 0.32627102732658386 - improvement 0\n",
      "Step 7060 - Loss: -0.48078423738479614 - prediction 0.48078423738479614 - improvement 2\n",
      "Step 7070 - Loss: -0.48115673661231995 - prediction 0.48115673661231995 - improvement 2\n",
      "Step 7080 - Loss: -0.48114293813705444 - prediction 0.48114293813705444 - improvement 2\n",
      "Step 7090 - Loss: -0.1927216500043869 - prediction 0.1927216500043869 - improvement 1\n",
      "Step 7100 - Loss: -0.32624903321266174 - prediction 0.32624903321266174 - improvement 0\n",
      "Step 7110 - Loss: -0.19274897873401642 - prediction 0.19274897873401642 - improvement 1\n",
      "Step 7120 - Loss: -0.19304078817367554 - prediction 0.19304078817367554 - improvement 1\n",
      "Step 7130 - Loss: -0.19297848641872406 - prediction 0.19297848641872406 - improvement 1\n",
      "Step 7140 - Loss: -0.32631900906562805 - prediction 0.32631900906562805 - improvement 0\n",
      "Step 7150 - Loss: -0.19295445084571838 - prediction 0.19295445084571838 - improvement 1\n",
      "Step 7160 - Loss: -0.19309286773204803 - prediction 0.19309286773204803 - improvement 1\n",
      "Step 7170 - Loss: -0.1933693140745163 - prediction 0.1933693140745163 - improvement 1\n",
      "Step 7180 - Loss: -0.48005038499832153 - prediction 0.48005038499832153 - improvement 2\n",
      "Step 7190 - Loss: -0.3264866769313812 - prediction 0.3264866769313812 - improvement 0\n",
      "Step 7200 - Loss: -0.4802226424217224 - prediction 0.4802226424217224 - improvement 2\n",
      "Step 7210 - Loss: -0.3264172673225403 - prediction 0.3264172673225403 - improvement 0\n",
      "Step 7220 - Loss: -0.19309113919734955 - prediction 0.19309113919734955 - improvement 1\n",
      "Step 7230 - Loss: -0.32640647888183594 - prediction 0.32640647888183594 - improvement 0\n",
      "Step 7240 - Loss: -0.4802987575531006 - prediction 0.4802987575531006 - improvement 2\n",
      "Step 7250 - Loss: -0.32646629214286804 - prediction 0.32646629214286804 - improvement 0\n",
      "Step 7260 - Loss: -0.19323819875717163 - prediction 0.19323819875717163 - improvement 1\n",
      "Step 7270 - Loss: -0.48051467537879944 - prediction 0.48051467537879944 - improvement 2\n",
      "Step 7280 - Loss: -0.3264104127883911 - prediction 0.3264104127883911 - improvement 0\n",
      "Step 7290 - Loss: -0.1932036429643631 - prediction 0.1932036429643631 - improvement 1\n",
      "Step 7300 - Loss: -0.48052552342414856 - prediction 0.48052552342414856 - improvement 2\n",
      "Step 7310 - Loss: -0.48068687319755554 - prediction 0.48068687319755554 - improvement 2\n",
      "Step 7320 - Loss: -0.32634687423706055 - prediction 0.32634687423706055 - improvement 0\n",
      "Step 7330 - Loss: -0.3263293504714966 - prediction 0.3263293504714966 - improvement 0\n",
      "Step 7340 - Loss: -0.3263913691043854 - prediction 0.3263913691043854 - improvement 0\n",
      "Step 7350 - Loss: -0.32639482617378235 - prediction 0.32639482617378235 - improvement 0\n",
      "Step 7360 - Loss: -0.4802963435649872 - prediction 0.4802963435649872 - improvement 2\n",
      "Step 7370 - Loss: -0.48029035329818726 - prediction 0.48029035329818726 - improvement 2\n",
      "Step 7380 - Loss: -0.32647451758384705 - prediction 0.32647451758384705 - improvement 0\n",
      "Step 7390 - Loss: -0.4801930785179138 - prediction 0.4801930785179138 - improvement 2\n",
      "Step 7400 - Loss: -0.48019731044769287 - prediction 0.48019731044769287 - improvement 2\n",
      "Step 7410 - Loss: -0.19338274002075195 - prediction 0.19338274002075195 - improvement 1\n",
      "Step 7420 - Loss: -0.19324642419815063 - prediction 0.19324642419815063 - improvement 1\n",
      "Step 7430 - Loss: -0.3264050781726837 - prediction 0.3264050781726837 - improvement 0\n",
      "Step 7440 - Loss: -0.32641005516052246 - prediction 0.32641005516052246 - improvement 0\n",
      "Step 7450 - Loss: -0.32642826437950134 - prediction 0.32642826437950134 - improvement 0\n",
      "Step 7460 - Loss: -0.4802480638027191 - prediction 0.4802480638027191 - improvement 2\n",
      "Step 7470 - Loss: -0.32651594281196594 - prediction 0.32651594281196594 - improvement 0\n",
      "Step 7480 - Loss: -0.4800467789173126 - prediction 0.4800467789173126 - improvement 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7490 - Loss: -0.3264729976654053 - prediction 0.3264729976654053 - improvement 0\n",
      "Step 7500 - Loss: -0.3264326751232147 - prediction 0.3264326751232147 - improvement 0\n",
      "Step 7510 - Loss: -0.19296732544898987 - prediction 0.19296732544898987 - improvement 1\n",
      "Step 7520 - Loss: -0.3262978494167328 - prediction 0.3262978494167328 - improvement 0\n",
      "Step 7530 - Loss: -0.32636168599128723 - prediction 0.32636168599128723 - improvement 0\n",
      "Step 7540 - Loss: -0.48099496960639954 - prediction 0.48099496960639954 - improvement 2\n",
      "Step 7550 - Loss: -0.3261962831020355 - prediction 0.3261962831020355 - improvement 0\n",
      "Step 7560 - Loss: -0.3262315094470978 - prediction 0.3262315094470978 - improvement 0\n",
      "Step 7570 - Loss: -0.4811394214630127 - prediction 0.4811394214630127 - improvement 2\n",
      "Step 7580 - Loss: -0.3261750042438507 - prediction 0.3261750042438507 - improvement 0\n",
      "Step 7590 - Loss: -0.32624736428260803 - prediction 0.32624736428260803 - improvement 0\n",
      "Step 7600 - Loss: -0.19284217059612274 - prediction 0.19284217059612274 - improvement 1\n",
      "Step 7610 - Loss: -0.480621874332428 - prediction 0.480621874332428 - improvement 2\n",
      "Step 7620 - Loss: -0.32637348771095276 - prediction 0.32637348771095276 - improvement 0\n",
      "Step 7630 - Loss: -0.4806338846683502 - prediction 0.4806338846683502 - improvement 2\n",
      "Step 7640 - Loss: -0.4804357588291168 - prediction 0.4804357588291168 - improvement 2\n",
      "Step 7650 - Loss: -0.3263527452945709 - prediction 0.3263527452945709 - improvement 0\n",
      "Step 7660 - Loss: -0.1927720457315445 - prediction 0.1927720457315445 - improvement 1\n",
      "Step 7670 - Loss: -0.32618245482444763 - prediction 0.32618245482444763 - improvement 0\n",
      "Step 7680 - Loss: -0.19237485527992249 - prediction 0.19237485527992249 - improvement 1\n",
      "Step 7690 - Loss: -0.4816877841949463 - prediction 0.4816877841949463 - improvement 2\n",
      "Step 7700 - Loss: -0.48140615224838257 - prediction 0.48140615224838257 - improvement 2\n",
      "Step 7710 - Loss: -0.3261070251464844 - prediction 0.3261070251464844 - improvement 0\n",
      "Step 7720 - Loss: -0.3261108696460724 - prediction 0.3261108696460724 - improvement 0\n",
      "Step 7730 - Loss: -0.4815446436405182 - prediction 0.4815446436405182 - improvement 2\n",
      "Step 7740 - Loss: -0.32617244124412537 - prediction 0.32617244124412537 - improvement 0\n",
      "Step 7750 - Loss: -0.32624301314353943 - prediction 0.32624301314353943 - improvement 0\n",
      "Step 7760 - Loss: -0.48099321126937866 - prediction 0.48099321126937866 - improvement 2\n",
      "Step 7770 - Loss: -0.3262653052806854 - prediction 0.3262653052806854 - improvement 0\n",
      "Step 7780 - Loss: -0.3262580931186676 - prediction 0.3262580931186676 - improvement 0\n",
      "Step 7790 - Loss: -0.4806988835334778 - prediction 0.4806988835334778 - improvement 2\n",
      "Step 7800 - Loss: -0.4806212782859802 - prediction 0.4806212782859802 - improvement 2\n",
      "Step 7810 - Loss: -0.48053938150405884 - prediction 0.48053938150405884 - improvement 2\n",
      "Step 7820 - Loss: -0.32639122009277344 - prediction 0.32639122009277344 - improvement 0\n",
      "Step 7830 - Loss: -0.3263109028339386 - prediction 0.3263109028339386 - improvement 0\n",
      "Step 7840 - Loss: -0.19293473660945892 - prediction 0.19293473660945892 - improvement 1\n",
      "Step 7850 - Loss: -0.32631969451904297 - prediction 0.32631969451904297 - improvement 0\n",
      "Step 7860 - Loss: -0.48087194561958313 - prediction 0.48087194561958313 - improvement 2\n",
      "Step 7870 - Loss: -0.48079389333724976 - prediction 0.48079389333724976 - improvement 2\n",
      "Step 7880 - Loss: -0.3263777792453766 - prediction 0.3263777792453766 - improvement 0\n",
      "Step 7890 - Loss: -0.3263561725616455 - prediction 0.3263561725616455 - improvement 0\n",
      "Step 7900 - Loss: -0.3263561725616455 - prediction 0.3263561725616455 - improvement 0\n",
      "Step 7910 - Loss: -0.3263537585735321 - prediction 0.3263537585735321 - improvement 0\n",
      "Step 7920 - Loss: -0.19309458136558533 - prediction 0.19309458136558533 - improvement 1\n",
      "Step 7930 - Loss: -0.4803856611251831 - prediction 0.4803856611251831 - improvement 2\n",
      "Step 7940 - Loss: -0.4801211655139923 - prediction 0.4801211655139923 - improvement 2\n",
      "Step 7950 - Loss: -0.48006671667099 - prediction 0.48006671667099 - improvement 2\n",
      "Step 7960 - Loss: -0.32651132345199585 - prediction 0.32651132345199585 - improvement 0\n",
      "Step 7970 - Loss: -0.48002979159355164 - prediction 0.48002979159355164 - improvement 2\n",
      "Step 7980 - Loss: -0.19350166618824005 - prediction 0.19350166618824005 - improvement 1\n",
      "Step 7990 - Loss: -0.3265056610107422 - prediction 0.3265056610107422 - improvement 0\n",
      "Step 8000 - Loss: -0.19336970150470734 - prediction 0.19336970150470734 - improvement 1\n",
      "Step 8010 - Loss: -0.47968608140945435 - prediction 0.47968608140945435 - improvement 2\n",
      "Step 8020 - Loss: -0.32662275433540344 - prediction 0.32662275433540344 - improvement 0\n",
      "Step 8030 - Loss: -0.32659396529197693 - prediction 0.32659396529197693 - improvement 0\n",
      "Step 8040 - Loss: -0.3264889121055603 - prediction 0.3264889121055603 - improvement 0\n",
      "Step 8050 - Loss: -0.4804496169090271 - prediction 0.4804496169090271 - improvement 2\n",
      "Step 8060 - Loss: -0.3263821303844452 - prediction 0.3263821303844452 - improvement 0\n",
      "Step 8070 - Loss: -0.32639941573143005 - prediction 0.32639941573143005 - improvement 0\n",
      "Step 8080 - Loss: -0.48046281933784485 - prediction 0.48046281933784485 - improvement 2\n",
      "Step 8090 - Loss: -0.32639461755752563 - prediction 0.32639461755752563 - improvement 0\n",
      "Step 8100 - Loss: -0.32643887400627136 - prediction 0.32643887400627136 - improvement 0\n",
      "Step 8110 - Loss: -0.19321228563785553 - prediction 0.19321228563785553 - improvement 1\n",
      "Step 8120 - Loss: -0.4802999794483185 - prediction 0.4802999794483185 - improvement 2\n",
      "Step 8130 - Loss: -0.32650187611579895 - prediction 0.32650187611579895 - improvement 0\n",
      "Step 8140 - Loss: -0.3265693783760071 - prediction 0.3265693783760071 - improvement 0\n",
      "Step 8150 - Loss: -0.3266133964061737 - prediction 0.3266133964061737 - improvement 0\n",
      "Step 8160 - Loss: -0.47984203696250916 - prediction 0.47984203696250916 - improvement 2\n",
      "Step 8170 - Loss: -0.4797474145889282 - prediction 0.4797474145889282 - improvement 2\n",
      "Step 8180 - Loss: -0.19363617897033691 - prediction 0.19363617897033691 - improvement 1\n",
      "Step 8190 - Loss: -0.1936706304550171 - prediction 0.1936706304550171 - improvement 1\n",
      "Step 8200 - Loss: -0.4797188639640808 - prediction 0.4797188639640808 - improvement 2\n",
      "Step 8210 - Loss: -0.3266312777996063 - prediction 0.3266312777996063 - improvement 0\n",
      "Step 8220 - Loss: -0.47991713881492615 - prediction 0.47991713881492615 - improvement 2\n",
      "Step 8230 - Loss: -0.3265712559223175 - prediction 0.3265712559223175 - improvement 0\n",
      "Step 8240 - Loss: -0.1936536282300949 - prediction 0.1936536282300949 - improvement 1\n",
      "Step 8250 - Loss: -0.3266069293022156 - prediction 0.3266069293022156 - improvement 0\n",
      "Step 8260 - Loss: -0.326649010181427 - prediction 0.326649010181427 - improvement 0\n",
      "Step 8270 - Loss: -0.47945263981819153 - prediction 0.47945263981819153 - improvement 2\n",
      "Step 8280 - Loss: -0.47905397415161133 - prediction 0.47905397415161133 - improvement 2\n",
      "Step 8290 - Loss: -0.32683977484703064 - prediction 0.32683977484703064 - improvement 0\n",
      "Step 8300 - Loss: -0.194153293967247 - prediction 0.194153293967247 - improvement 1\n",
      "Step 8310 - Loss: -0.19388806819915771 - prediction 0.19388806819915771 - improvement 1\n",
      "Step 8320 - Loss: -0.4794721007347107 - prediction 0.4794721007347107 - improvement 2\n",
      "Step 8330 - Loss: -0.4794647991657257 - prediction 0.4794647991657257 - improvement 2\n",
      "Step 8340 - Loss: -0.3266814947128296 - prediction 0.3266814947128296 - improvement 0\n",
      "Step 8350 - Loss: -0.47948065400123596 - prediction 0.47948065400123596 - improvement 2\n",
      "Step 8360 - Loss: -0.3266592025756836 - prediction 0.3266592025756836 - improvement 0\n",
      "Step 8370 - Loss: -0.19399519264698029 - prediction 0.19399519264698029 - improvement 1\n",
      "Step 8380 - Loss: -0.4796326458454132 - prediction 0.4796326458454132 - improvement 2\n",
      "Step 8390 - Loss: -0.4797528386116028 - prediction 0.4797528386116028 - improvement 2\n",
      "Step 8400 - Loss: -0.4795815944671631 - prediction 0.4795815944671631 - improvement 2\n",
      "Step 8410 - Loss: -0.3267173767089844 - prediction 0.3267173767089844 - improvement 0\n",
      "Step 8420 - Loss: -0.4796448051929474 - prediction 0.4796448051929474 - improvement 2\n",
      "Step 8430 - Loss: -0.4794009029865265 - prediction 0.4794009029865265 - improvement 2\n",
      "Step 8440 - Loss: -0.4794977009296417 - prediction 0.4794977009296417 - improvement 2\n",
      "Step 8450 - Loss: -0.4793771803379059 - prediction 0.4793771803379059 - improvement 2\n",
      "Step 8460 - Loss: -0.32677674293518066 - prediction 0.32677674293518066 - improvement 0\n",
      "Step 8470 - Loss: -0.3267554044723511 - prediction 0.3267554044723511 - improvement 0\n",
      "Step 8480 - Loss: -0.479266881942749 - prediction 0.479266881942749 - improvement 2\n",
      "Step 8490 - Loss: -0.3268354833126068 - prediction 0.3268354833126068 - improvement 0\n",
      "Step 8500 - Loss: -0.4787653088569641 - prediction 0.4787653088569641 - improvement 2\n",
      "Step 8510 - Loss: -0.3268451690673828 - prediction 0.3268451690673828 - improvement 0\n",
      "Step 8520 - Loss: -0.47888097167015076 - prediction 0.47888097167015076 - improvement 2\n",
      "Step 8530 - Loss: -0.47852498292922974 - prediction 0.47852498292922974 - improvement 2\n",
      "Step 8540 - Loss: -0.32685545086860657 - prediction 0.32685545086860657 - improvement 0\n",
      "Step 8550 - Loss: -0.19422174990177155 - prediction 0.19422174990177155 - improvement 1\n",
      "Step 8560 - Loss: -0.4791211187839508 - prediction 0.4791211187839508 - improvement 2\n",
      "Step 8570 - Loss: -0.4791528880596161 - prediction 0.4791528880596161 - improvement 2\n",
      "Step 8580 - Loss: -0.1941312700510025 - prediction 0.1941312700510025 - improvement 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8590 - Loss: -0.32677963376045227 - prediction 0.32677963376045227 - improvement 0\n",
      "Step 8600 - Loss: -0.32680994272232056 - prediction 0.32680994272232056 - improvement 0\n",
      "Step 8610 - Loss: -0.32678335905075073 - prediction 0.32678335905075073 - improvement 0\n",
      "Step 8620 - Loss: -0.4792979657649994 - prediction 0.4792979657649994 - improvement 2\n",
      "Step 8630 - Loss: -0.19388629496097565 - prediction 0.19388629496097565 - improvement 1\n",
      "Step 8640 - Loss: -0.32670754194259644 - prediction 0.32670754194259644 - improvement 0\n",
      "Step 8650 - Loss: -0.3266935646533966 - prediction 0.3266935646533966 - improvement 0\n",
      "Step 8660 - Loss: -0.4794246554374695 - prediction 0.4794246554374695 - improvement 2\n",
      "Step 8670 - Loss: -0.19371996819972992 - prediction 0.19371996819972992 - improvement 1\n",
      "Step 8680 - Loss: -0.3265429139137268 - prediction 0.3265429139137268 - improvement 0\n",
      "Step 8690 - Loss: -0.32651403546333313 - prediction 0.32651403546333313 - improvement 0\n",
      "Step 8700 - Loss: -0.4802323877811432 - prediction 0.4802323877811432 - improvement 2\n",
      "Step 8710 - Loss: -0.4801477789878845 - prediction 0.4801477789878845 - improvement 2\n",
      "Step 8720 - Loss: -0.4799862205982208 - prediction 0.4799862205982208 - improvement 2\n",
      "Step 8730 - Loss: -0.19346864521503448 - prediction 0.19346864521503448 - improvement 1\n",
      "Step 8740 - Loss: -0.4798692762851715 - prediction 0.4798692762851715 - improvement 2\n",
      "Step 8750 - Loss: -0.4798274338245392 - prediction 0.4798274338245392 - improvement 2\n",
      "Step 8760 - Loss: -0.47977709770202637 - prediction 0.47977709770202637 - improvement 2\n",
      "Step 8770 - Loss: -0.47978439927101135 - prediction 0.47978439927101135 - improvement 2\n",
      "Step 8780 - Loss: -0.19356340169906616 - prediction 0.19356340169906616 - improvement 1\n",
      "Step 8790 - Loss: -0.3264829218387604 - prediction 0.3264829218387604 - improvement 0\n",
      "Step 8800 - Loss: -0.19338403642177582 - prediction 0.19338403642177582 - improvement 1\n",
      "Step 8810 - Loss: -0.3265156149864197 - prediction 0.3265156149864197 - improvement 0\n",
      "Step 8820 - Loss: -0.4802214801311493 - prediction 0.4802214801311493 - improvement 2\n",
      "Step 8830 - Loss: -0.1934812366962433 - prediction 0.1934812366962433 - improvement 1\n",
      "Step 8840 - Loss: -0.19371123611927032 - prediction 0.19371123611927032 - improvement 1\n",
      "Step 8850 - Loss: -0.19374968111515045 - prediction 0.19374968111515045 - improvement 1\n",
      "Step 8860 - Loss: -0.3266695737838745 - prediction 0.3266695737838745 - improvement 0\n",
      "Step 8870 - Loss: -0.4794051945209503 - prediction 0.4794051945209503 - improvement 2\n",
      "Step 8880 - Loss: -0.3266887962818146 - prediction 0.3266887962818146 - improvement 0\n",
      "Step 8890 - Loss: -0.19388717412948608 - prediction 0.19388717412948608 - improvement 1\n",
      "Step 8900 - Loss: -0.3267209231853485 - prediction 0.3267209231853485 - improvement 0\n",
      "Step 8910 - Loss: -0.47926145792007446 - prediction 0.47926145792007446 - improvement 2\n",
      "Step 8920 - Loss: -0.3267090320587158 - prediction 0.3267090320587158 - improvement 0\n",
      "Step 8930 - Loss: -0.3266673982143402 - prediction 0.3266673982143402 - improvement 0\n",
      "Step 8940 - Loss: -0.4795967638492584 - prediction 0.4795967638492584 - improvement 2\n",
      "Step 8950 - Loss: -0.47967877984046936 - prediction 0.47967877984046936 - improvement 2\n",
      "Step 8960 - Loss: -0.19372348487377167 - prediction 0.19372348487377167 - improvement 1\n",
      "Step 8970 - Loss: -0.32661592960357666 - prediction 0.32661592960357666 - improvement 0\n",
      "Step 8980 - Loss: -0.47960105538368225 - prediction 0.47960105538368225 - improvement 2\n",
      "Step 8990 - Loss: -0.4795730710029602 - prediction 0.4795730710029602 - improvement 2\n",
      "Step 0 - Loss: -0.4793016314506531 - prediction 0.4793016314506531 - improvement 2\n",
      "Step 10 - Loss: -0.47566935420036316 - prediction 0.47566935420036316 - improvement 2\n",
      "Step 20 - Loss: -0.32767489552497864 - prediction 0.32767489552497864 - improvement 0\n",
      "Step 30 - Loss: -0.3275674879550934 - prediction 0.3275674879550934 - improvement 0\n",
      "Step 40 - Loss: -0.327438622713089 - prediction 0.327438622713089 - improvement 0\n",
      "Step 50 - Loss: -0.47639063000679016 - prediction 0.47639063000679016 - improvement 2\n",
      "Step 60 - Loss: -0.327457457780838 - prediction 0.327457457780838 - improvement 0\n",
      "Step 70 - Loss: -0.47651129961013794 - prediction 0.47651129961013794 - improvement 2\n",
      "Step 80 - Loss: -0.3275533616542816 - prediction 0.3275533616542816 - improvement 0\n",
      "Step 90 - Loss: -0.32734161615371704 - prediction 0.32734161615371704 - improvement 0\n",
      "Step 100 - Loss: -0.32738178968429565 - prediction 0.32738178968429565 - improvement 0\n",
      "Step 110 - Loss: -0.1957419067621231 - prediction 0.1957419067621231 - improvement 1\n",
      "Step 120 - Loss: -0.32740893959999084 - prediction 0.32740893959999084 - improvement 0\n",
      "Step 130 - Loss: -0.3272450864315033 - prediction 0.3272450864315033 - improvement 0\n",
      "Step 140 - Loss: -0.3272962272167206 - prediction 0.3272962272167206 - improvement 0\n",
      "Step 150 - Loss: -0.47713154554367065 - prediction 0.47713154554367065 - improvement 2\n",
      "Step 160 - Loss: -0.47720423340797424 - prediction 0.47720423340797424 - improvement 2\n",
      "Step 170 - Loss: -0.32716965675354004 - prediction 0.32716965675354004 - improvement 0\n",
      "Step 180 - Loss: -0.477771133184433 - prediction 0.477771133184433 - improvement 2\n",
      "Step 190 - Loss: -0.47772476077079773 - prediction 0.47772476077079773 - improvement 2\n",
      "Step 200 - Loss: -0.4776524007320404 - prediction 0.4776524007320404 - improvement 2\n",
      "Step 210 - Loss: -0.4775063395500183 - prediction 0.4775063395500183 - improvement 2\n",
      "Step 220 - Loss: -0.3272911012172699 - prediction 0.3272911012172699 - improvement 0\n",
      "Step 230 - Loss: -0.47760289907455444 - prediction 0.47760289907455444 - improvement 2\n",
      "Step 240 - Loss: -0.4773476719856262 - prediction 0.4773476719856262 - improvement 2\n",
      "Step 250 - Loss: -0.3271040618419647 - prediction 0.3271040618419647 - improvement 0\n",
      "Step 260 - Loss: -0.47774946689605713 - prediction 0.47774946689605713 - improvement 2\n",
      "Step 270 - Loss: -0.19486138224601746 - prediction 0.19486138224601746 - improvement 1\n",
      "Step 280 - Loss: -0.4777328073978424 - prediction 0.4777328073978424 - improvement 2\n",
      "Step 290 - Loss: -0.4778674840927124 - prediction 0.4778674840927124 - improvement 2\n",
      "Step 300 - Loss: -0.4776870608329773 - prediction 0.4776870608329773 - improvement 2\n",
      "Step 310 - Loss: -0.47732529044151306 - prediction 0.47732529044151306 - improvement 2\n",
      "Step 320 - Loss: -0.32729342579841614 - prediction 0.32729342579841614 - improvement 0\n",
      "Step 330 - Loss: -0.4773891568183899 - prediction 0.4773891568183899 - improvement 2\n",
      "Step 340 - Loss: -0.47708237171173096 - prediction 0.47708237171173096 - improvement 2\n",
      "Step 350 - Loss: -0.3273753225803375 - prediction 0.3273753225803375 - improvement 0\n",
      "Step 360 - Loss: -0.19585265219211578 - prediction 0.19585265219211578 - improvement 1\n",
      "Step 370 - Loss: -0.32746773958206177 - prediction 0.32746773958206177 - improvement 0\n",
      "Step 380 - Loss: -0.3275187313556671 - prediction 0.3275187313556671 - improvement 0\n",
      "Step 390 - Loss: -0.4769996702671051 - prediction 0.4769996702671051 - improvement 2\n",
      "Step 400 - Loss: -0.4771973788738251 - prediction 0.4771973788738251 - improvement 2\n",
      "Step 410 - Loss: -0.32739439606666565 - prediction 0.32739439606666565 - improvement 0\n",
      "Step 420 - Loss: -0.47683900594711304 - prediction 0.47683900594711304 - improvement 2\n",
      "Step 430 - Loss: -0.4764031767845154 - prediction 0.4764031767845154 - improvement 2\n",
      "Step 440 - Loss: -0.32755136489868164 - prediction 0.32755136489868164 - improvement 0\n",
      "Step 450 - Loss: -0.3274982273578644 - prediction 0.3274982273578644 - improvement 0\n",
      "Step 460 - Loss: -0.3274417519569397 - prediction 0.3274417519569397 - improvement 0\n",
      "Step 470 - Loss: -0.4765763282775879 - prediction 0.4765763282775879 - improvement 2\n",
      "Step 480 - Loss: -0.47666874527931213 - prediction 0.47666874527931213 - improvement 2\n",
      "Step 490 - Loss: -0.4768003523349762 - prediction 0.4768003523349762 - improvement 2\n",
      "Step 500 - Loss: -0.3275332450866699 - prediction 0.3275332450866699 - improvement 0\n",
      "Step 510 - Loss: -0.4762917160987854 - prediction 0.4762917160987854 - improvement 2\n",
      "Step 520 - Loss: -0.1962141990661621 - prediction 0.1962141990661621 - improvement 1\n",
      "Step 530 - Loss: -0.4768003523349762 - prediction 0.4768003523349762 - improvement 2\n",
      "Step 540 - Loss: -0.19592051208019257 - prediction 0.19592051208019257 - improvement 1\n",
      "Step 550 - Loss: -0.4765969216823578 - prediction 0.4765969216823578 - improvement 2\n",
      "Step 560 - Loss: -0.4764644205570221 - prediction 0.4764644205570221 - improvement 2\n",
      "Step 570 - Loss: -0.32747843861579895 - prediction 0.32747843861579895 - improvement 0\n",
      "Step 580 - Loss: -0.32741329073905945 - prediction 0.32741329073905945 - improvement 0\n",
      "Step 590 - Loss: -0.32727327942848206 - prediction 0.32727327942848206 - improvement 0\n",
      "Step 600 - Loss: -0.3273220360279083 - prediction 0.3273220360279083 - improvement 0\n",
      "Step 610 - Loss: -0.47698286175727844 - prediction 0.47698286175727844 - improvement 2\n",
      "Step 620 - Loss: -0.32737645506858826 - prediction 0.32737645506858826 - improvement 0\n",
      "Step 630 - Loss: -0.3272280991077423 - prediction 0.3272280991077423 - improvement 0\n",
      "Step 640 - Loss: -0.19505774974822998 - prediction 0.19505774974822998 - improvement 1\n",
      "Step 650 - Loss: -0.3271598219871521 - prediction 0.3271598219871521 - improvement 0\n",
      "Step 660 - Loss: -0.327178031206131 - prediction 0.327178031206131 - improvement 0\n",
      "Step 670 - Loss: -0.47808709740638733 - prediction 0.47808709740638733 - improvement 2\n",
      "Step 680 - Loss: -0.3270716667175293 - prediction 0.3270716667175293 - improvement 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 690 - Loss: -0.3271191418170929 - prediction 0.3271191418170929 - improvement 0\n",
      "Step 700 - Loss: -0.3271365463733673 - prediction 0.3271365463733673 - improvement 0\n",
      "Step 710 - Loss: -0.47827550768852234 - prediction 0.47827550768852234 - improvement 2\n",
      "Step 720 - Loss: -0.478380024433136 - prediction 0.478380024433136 - improvement 2\n",
      "Step 730 - Loss: -0.4781986176967621 - prediction 0.4781986176967621 - improvement 2\n",
      "Step 740 - Loss: -0.4777686297893524 - prediction 0.4777686297893524 - improvement 2\n",
      "Step 750 - Loss: -0.47758060693740845 - prediction 0.47758060693740845 - improvement 2\n",
      "Step 760 - Loss: -0.3272852301597595 - prediction 0.3272852301597595 - improvement 0\n",
      "Step 770 - Loss: -0.3272891044616699 - prediction 0.3272891044616699 - improvement 0\n",
      "Step 780 - Loss: -0.19551846385002136 - prediction 0.19551846385002136 - improvement 1\n",
      "Step 790 - Loss: -0.3273092210292816 - prediction 0.3273092210292816 - improvement 0\n",
      "Step 800 - Loss: -0.4771650731563568 - prediction 0.4771650731563568 - improvement 2\n",
      "Step 810 - Loss: -0.32731354236602783 - prediction 0.32731354236602783 - improvement 0\n",
      "Step 820 - Loss: -0.47705069184303284 - prediction 0.47705069184303284 - improvement 2\n",
      "Step 830 - Loss: -0.3274478614330292 - prediction 0.3274478614330292 - improvement 0\n",
      "Step 840 - Loss: -0.4767255187034607 - prediction 0.4767255187034607 - improvement 2\n",
      "Step 850 - Loss: -0.47681400179862976 - prediction 0.47681400179862976 - improvement 2\n",
      "Step 860 - Loss: -0.32740944623947144 - prediction 0.32740944623947144 - improvement 0\n",
      "Step 870 - Loss: -0.4770171046257019 - prediction 0.4770171046257019 - improvement 2\n",
      "Step 880 - Loss: -0.3273841142654419 - prediction 0.3273841142654419 - improvement 0\n",
      "Step 890 - Loss: -0.19574281573295593 - prediction 0.19574281573295593 - improvement 1\n",
      "Step 900 - Loss: -0.1956387311220169 - prediction 0.1956387311220169 - improvement 1\n",
      "Step 910 - Loss: -0.3272307813167572 - prediction 0.3272307813167572 - improvement 0\n",
      "Step 920 - Loss: -0.47750014066696167 - prediction 0.47750014066696167 - improvement 2\n",
      "Step 930 - Loss: -0.4778069853782654 - prediction 0.4778069853782654 - improvement 2\n",
      "Step 940 - Loss: -0.32721641659736633 - prediction 0.32721641659736633 - improvement 0\n",
      "Step 950 - Loss: -0.3269919157028198 - prediction 0.3269919157028198 - improvement 0\n",
      "Step 960 - Loss: -0.4784082770347595 - prediction 0.4784082770347595 - improvement 2\n",
      "Step 970 - Loss: -0.4789097309112549 - prediction 0.4789097309112549 - improvement 2\n",
      "Step 980 - Loss: -0.4785286486148834 - prediction 0.4785286486148834 - improvement 2\n",
      "Step 990 - Loss: -0.47865501046180725 - prediction 0.47865501046180725 - improvement 2\n",
      "Step 1000 - Loss: -0.3269682824611664 - prediction 0.3269682824611664 - improvement 0\n",
      "Step 1010 - Loss: -0.4786059558391571 - prediction 0.4786059558391571 - improvement 2\n",
      "Step 1020 - Loss: -0.32690998911857605 - prediction 0.32690998911857605 - improvement 0\n",
      "Step 1030 - Loss: -0.3268977105617523 - prediction 0.3268977105617523 - improvement 0\n",
      "Step 1040 - Loss: -0.3268830180168152 - prediction 0.3268830180168152 - improvement 0\n",
      "Step 1050 - Loss: -0.3268769383430481 - prediction 0.3268769383430481 - improvement 0\n",
      "Step 1060 - Loss: -0.47911927103996277 - prediction 0.47911927103996277 - improvement 2\n",
      "Step 1070 - Loss: -0.3267950415611267 - prediction 0.3267950415611267 - improvement 0\n",
      "Step 1080 - Loss: -0.194377601146698 - prediction 0.194377601146698 - improvement 1\n",
      "Step 1090 - Loss: -0.4785832464694977 - prediction 0.4785832464694977 - improvement 2\n",
      "Step 1100 - Loss: -0.4784850776195526 - prediction 0.4784850776195526 - improvement 2\n",
      "Step 1110 - Loss: -0.47847646474838257 - prediction 0.47847646474838257 - improvement 2\n",
      "Step 1120 - Loss: -0.47843098640441895 - prediction 0.47843098640441895 - improvement 2\n",
      "Step 1130 - Loss: -0.3268667757511139 - prediction 0.3268667757511139 - improvement 0\n",
      "Step 1140 - Loss: -0.32678133249282837 - prediction 0.32678133249282837 - improvement 0\n",
      "Step 1150 - Loss: -0.3266105055809021 - prediction 0.3266105055809021 - improvement 0\n",
      "Step 1160 - Loss: -0.3265290856361389 - prediction 0.3265290856361389 - improvement 0\n",
      "Step 1170 - Loss: -0.4799922704696655 - prediction 0.4799922704696655 - improvement 2\n",
      "Step 1180 - Loss: -0.32654857635498047 - prediction 0.32654857635498047 - improvement 0\n",
      "Step 1190 - Loss: -0.32642054557800293 - prediction 0.32642054557800293 - improvement 0\n",
      "Step 1200 - Loss: -0.326333612203598 - prediction 0.326333612203598 - improvement 0\n",
      "Step 1210 - Loss: -0.4803856611251831 - prediction 0.4803856611251831 - improvement 2\n",
      "Step 1220 - Loss: -0.48041945695877075 - prediction 0.48041945695877075 - improvement 2\n",
      "Step 1230 - Loss: -0.48068687319755554 - prediction 0.48068687319755554 - improvement 2\n",
      "Step 1240 - Loss: -0.3263974189758301 - prediction 0.3263974189758301 - improvement 0\n",
      "Step 1250 - Loss: -0.4802655279636383 - prediction 0.4802655279636383 - improvement 2\n",
      "Step 1260 - Loss: -0.4802094101905823 - prediction 0.4802094101905823 - improvement 2\n",
      "Step 1270 - Loss: -0.3264416456222534 - prediction 0.3264416456222534 - improvement 0\n",
      "Step 1280 - Loss: -0.32635942101478577 - prediction 0.32635942101478577 - improvement 0\n",
      "Step 1290 - Loss: -0.4805905520915985 - prediction 0.4805905520915985 - improvement 2\n",
      "Step 1300 - Loss: -0.19308596849441528 - prediction 0.19308596849441528 - improvement 1\n",
      "Step 1310 - Loss: -0.4808701276779175 - prediction 0.4808701276779175 - improvement 2\n",
      "Step 1320 - Loss: -0.32630419731140137 - prediction 0.32630419731140137 - improvement 0\n",
      "Step 1330 - Loss: -0.32627731561660767 - prediction 0.32627731561660767 - improvement 0\n",
      "Step 1340 - Loss: -0.326345831155777 - prediction 0.326345831155777 - improvement 0\n",
      "Step 1350 - Loss: -0.4807289242744446 - prediction 0.4807289242744446 - improvement 2\n",
      "Step 1360 - Loss: -0.4804568290710449 - prediction 0.4804568290710449 - improvement 2\n",
      "Step 1370 - Loss: -0.19314798712730408 - prediction 0.19314798712730408 - improvement 1\n",
      "Step 1380 - Loss: -0.4804544448852539 - prediction 0.4804544448852539 - improvement 2\n",
      "Step 1390 - Loss: -0.32639461755752563 - prediction 0.32639461755752563 - improvement 0\n",
      "Step 1400 - Loss: -0.32628941535949707 - prediction 0.32628941535949707 - improvement 0\n",
      "Step 1410 - Loss: -0.48031085729599 - prediction 0.48031085729599 - improvement 2\n",
      "Step 1420 - Loss: -0.32649335265159607 - prediction 0.32649335265159607 - improvement 0\n",
      "Step 1430 - Loss: -0.32641369104385376 - prediction 0.32641369104385376 - improvement 0\n",
      "Step 1440 - Loss: -0.4801193177700043 - prediction 0.4801193177700043 - improvement 2\n",
      "Step 1450 - Loss: -0.32652682065963745 - prediction 0.32652682065963745 - improvement 0\n",
      "Step 1460 - Loss: -0.19339531660079956 - prediction 0.19339531660079956 - improvement 1\n",
      "Step 1470 - Loss: -0.48024019598960876 - prediction 0.48024019598960876 - improvement 2\n",
      "Step 1480 - Loss: -0.32646337151527405 - prediction 0.32646337151527405 - improvement 0\n",
      "Step 1490 - Loss: -0.48041218519210815 - prediction 0.48041218519210815 - improvement 2\n",
      "Step 1500 - Loss: -0.3264291286468506 - prediction 0.3264291286468506 - improvement 0\n",
      "Step 1510 - Loss: -0.3262052536010742 - prediction 0.3262052536010742 - improvement 0\n",
      "Step 1520 - Loss: -0.32617121934890747 - prediction 0.32617121934890747 - improvement 0\n",
      "Step 1530 - Loss: -0.48178422451019287 - prediction 0.48178422451019287 - improvement 2\n",
      "Step 1540 - Loss: -0.4818722903728485 - prediction 0.4818722903728485 - improvement 2\n",
      "Step 1550 - Loss: -0.4817348122596741 - prediction 0.4817348122596741 - improvement 2\n",
      "Step 1560 - Loss: -0.3261150121688843 - prediction 0.3261150121688843 - improvement 0\n",
      "Step 1570 - Loss: -0.4813404083251953 - prediction 0.4813404083251953 - improvement 2\n",
      "Step 1580 - Loss: -0.3261900544166565 - prediction 0.3261900544166565 - improvement 0\n",
      "Step 1590 - Loss: -0.32614096999168396 - prediction 0.32614096999168396 - improvement 0\n",
      "Step 1600 - Loss: -0.32602646946907043 - prediction 0.32602646946907043 - improvement 0\n",
      "Step 1610 - Loss: -0.32605162262916565 - prediction 0.32605162262916565 - improvement 0\n",
      "Step 1620 - Loss: -0.4816281497478485 - prediction 0.4816281497478485 - improvement 2\n",
      "Step 1630 - Loss: -0.3259839117527008 - prediction 0.3259839117527008 - improvement 0\n",
      "Step 1640 - Loss: -0.4820338785648346 - prediction 0.4820338785648346 - improvement 2\n",
      "Step 1650 - Loss: -0.3260462284088135 - prediction 0.3260462284088135 - improvement 0\n",
      "Step 1660 - Loss: -0.3260512351989746 - prediction 0.3260512351989746 - improvement 0\n",
      "Step 1670 - Loss: -0.4820178747177124 - prediction 0.4820178747177124 - improvement 2\n",
      "Step 1680 - Loss: -0.32591676712036133 - prediction 0.32591676712036133 - improvement 0\n",
      "Step 1690 - Loss: -0.48259881138801575 - prediction 0.48259881138801575 - improvement 2\n",
      "Step 1700 - Loss: -0.4823782444000244 - prediction 0.4823782444000244 - improvement 2\n",
      "Step 1710 - Loss: -0.32587021589279175 - prediction 0.32587021589279175 - improvement 0\n",
      "Step 1720 - Loss: -0.32565319538116455 - prediction 0.32565319538116455 - improvement 0\n",
      "Step 1730 - Loss: -0.19118602573871613 - prediction 0.19118602573871613 - improvement 1\n",
      "Step 1740 - Loss: -0.483064204454422 - prediction 0.483064204454422 - improvement 2\n",
      "Step 1750 - Loss: -0.48313236236572266 - prediction 0.48313236236572266 - improvement 2\n",
      "Step 1760 - Loss: -0.48305830359458923 - prediction 0.48305830359458923 - improvement 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1770 - Loss: -0.48357197642326355 - prediction 0.48357197642326355 - improvement 2\n",
      "Step 1780 - Loss: -0.32555481791496277 - prediction 0.32555481791496277 - improvement 0\n",
      "Step 1790 - Loss: -0.48365500569343567 - prediction 0.48365500569343567 - improvement 2\n",
      "Step 1800 - Loss: -0.48423531651496887 - prediction 0.48423531651496887 - improvement 2\n",
      "Step 1810 - Loss: -0.48401492834091187 - prediction 0.48401492834091187 - improvement 2\n",
      "Step 1820 - Loss: -0.32530826330184937 - prediction 0.32530826330184937 - improvement 0\n",
      "Step 1830 - Loss: -0.3252747654914856 - prediction 0.3252747654914856 - improvement 0\n",
      "Step 1840 - Loss: -0.3252108097076416 - prediction 0.3252108097076416 - improvement 0\n",
      "Step 1850 - Loss: -0.32537615299224854 - prediction 0.32537615299224854 - improvement 0\n",
      "Step 1860 - Loss: -0.3253609538078308 - prediction 0.3253609538078308 - improvement 0\n",
      "Step 1870 - Loss: -0.1908523589372635 - prediction 0.1908523589372635 - improvement 1\n",
      "Step 1880 - Loss: -0.19088295102119446 - prediction 0.19088295102119446 - improvement 1\n",
      "Step 1890 - Loss: -0.32548800110816956 - prediction 0.32548800110816956 - improvement 0\n",
      "Step 1900 - Loss: -0.48384061455726624 - prediction 0.48384061455726624 - improvement 2\n",
      "Step 1910 - Loss: -0.19058218598365784 - prediction 0.19058218598365784 - improvement 1\n",
      "Step 1920 - Loss: -0.19073878228664398 - prediction 0.19073878228664398 - improvement 1\n",
      "Step 1930 - Loss: -0.4836217164993286 - prediction 0.4836217164993286 - improvement 2\n",
      "Step 1940 - Loss: -0.19105423986911774 - prediction 0.19105423986911774 - improvement 1\n",
      "Step 1950 - Loss: -0.4835970997810364 - prediction 0.4835970997810364 - improvement 2\n",
      "Step 1960 - Loss: -0.32546260952949524 - prediction 0.32546260952949524 - improvement 0\n",
      "Step 1970 - Loss: -0.19085603952407837 - prediction 0.19085603952407837 - improvement 1\n",
      "Step 1980 - Loss: -0.19060000777244568 - prediction 0.19060000777244568 - improvement 1\n",
      "Step 1990 - Loss: -0.48377883434295654 - prediction 0.48377883434295654 - improvement 2\n",
      "Step 2000 - Loss: -0.32536131143569946 - prediction 0.32536131143569946 - improvement 0\n",
      "Step 2010 - Loss: -0.3253747224807739 - prediction 0.3253747224807739 - improvement 0\n",
      "Step 2020 - Loss: -0.19066691398620605 - prediction 0.19066691398620605 - improvement 1\n",
      "Step 2030 - Loss: -0.48474302887916565 - prediction 0.48474302887916565 - improvement 2\n",
      "Step 2040 - Loss: -0.48502352833747864 - prediction 0.48502352833747864 - improvement 2\n",
      "Step 2050 - Loss: -0.4852377772331238 - prediction 0.4852377772331238 - improvement 2\n",
      "Step 2060 - Loss: -0.4846748113632202 - prediction 0.4846748113632202 - improvement 2\n",
      "Step 2070 - Loss: -0.4845485985279083 - prediction 0.4845485985279083 - improvement 2\n",
      "Step 2080 - Loss: -0.4845532476902008 - prediction 0.4845532476902008 - improvement 2\n",
      "Step 2090 - Loss: -0.19033601880073547 - prediction 0.19033601880073547 - improvement 1\n",
      "Step 2100 - Loss: -0.32518187165260315 - prediction 0.32518187165260315 - improvement 0\n",
      "Step 2110 - Loss: -0.3251689076423645 - prediction 0.3251689076423645 - improvement 0\n",
      "Step 2120 - Loss: -0.4847545623779297 - prediction 0.4847545623779297 - improvement 2\n",
      "Step 2130 - Loss: -0.3250700533390045 - prediction 0.3250700533390045 - improvement 0\n",
      "Step 2140 - Loss: -0.4855465888977051 - prediction 0.4855465888977051 - improvement 2\n",
      "Step 2150 - Loss: -0.48521700501441956 - prediction 0.48521700501441956 - improvement 2\n",
      "Step 2160 - Loss: -0.4848804771900177 - prediction 0.4848804771900177 - improvement 2\n",
      "Step 2170 - Loss: -0.32506293058395386 - prediction 0.32506293058395386 - improvement 0\n",
      "Step 2180 - Loss: -0.48499587178230286 - prediction 0.48499587178230286 - improvement 2\n",
      "Step 2190 - Loss: -0.48459550738334656 - prediction 0.48459550738334656 - improvement 2\n",
      "Step 2200 - Loss: -0.32515254616737366 - prediction 0.32515254616737366 - improvement 0\n",
      "Step 2210 - Loss: -0.4848250448703766 - prediction 0.4848250448703766 - improvement 2\n",
      "Step 2220 - Loss: -0.3250778615474701 - prediction 0.3250778615474701 - improvement 0\n",
      "Step 2230 - Loss: -0.32504338026046753 - prediction 0.32504338026046753 - improvement 0\n",
      "Step 2240 - Loss: -0.32501089572906494 - prediction 0.32501089572906494 - improvement 0\n",
      "Step 2250 - Loss: -0.3251507878303528 - prediction 0.3251507878303528 - improvement 0\n",
      "Step 2260 - Loss: -0.3250679075717926 - prediction 0.3250679075717926 - improvement 0\n",
      "Step 2270 - Loss: -0.3251248300075531 - prediction 0.3251248300075531 - improvement 0\n",
      "Step 2280 - Loss: -0.32526713609695435 - prediction 0.32526713609695435 - improvement 0\n",
      "Step 2290 - Loss: -0.3253832161426544 - prediction 0.3253832161426544 - improvement 0\n",
      "Step 2300 - Loss: -0.3254932463169098 - prediction 0.3254932463169098 - improvement 0\n",
      "Step 2310 - Loss: -0.19079573452472687 - prediction 0.19079573452472687 - improvement 1\n",
      "Step 2320 - Loss: -0.3253888487815857 - prediction 0.3253888487815857 - improvement 0\n",
      "Step 2330 - Loss: -0.4839280843734741 - prediction 0.4839280843734741 - improvement 2\n",
      "Step 2340 - Loss: -0.483593612909317 - prediction 0.483593612909317 - improvement 2\n",
      "Step 2350 - Loss: -0.4837157428264618 - prediction 0.4837157428264618 - improvement 2\n",
      "Step 2360 - Loss: -0.4838255047798157 - prediction 0.4838255047798157 - improvement 2\n",
      "Step 2370 - Loss: -0.4838435649871826 - prediction 0.4838435649871826 - improvement 2\n",
      "Step 2380 - Loss: -0.483402818441391 - prediction 0.483402818441391 - improvement 2\n",
      "Step 2390 - Loss: -0.3254597783088684 - prediction 0.3254597783088684 - improvement 0\n",
      "Step 2400 - Loss: -0.4836947023868561 - prediction 0.4836947023868561 - improvement 2\n",
      "Step 2410 - Loss: -0.48409000039100647 - prediction 0.48409000039100647 - improvement 2\n",
      "Step 2420 - Loss: -0.3253117799758911 - prediction 0.3253117799758911 - improvement 0\n",
      "Step 2430 - Loss: -0.48437759280204773 - prediction 0.48437759280204773 - improvement 2\n",
      "Step 2440 - Loss: -0.4842725694179535 - prediction 0.4842725694179535 - improvement 2\n",
      "Step 2450 - Loss: -0.19063079357147217 - prediction 0.19063079357147217 - improvement 1\n",
      "Step 2460 - Loss: -0.19069164991378784 - prediction 0.19069164991378784 - improvement 1\n",
      "Step 2470 - Loss: -0.3254390060901642 - prediction 0.3254390060901642 - improvement 0\n",
      "Step 2480 - Loss: -0.325436532497406 - prediction 0.325436532497406 - improvement 0\n",
      "Step 2490 - Loss: -0.32531461119651794 - prediction 0.32531461119651794 - improvement 0\n",
      "Step 2500 - Loss: -0.32533547282218933 - prediction 0.32533547282218933 - improvement 0\n",
      "Step 2510 - Loss: -0.48387154936790466 - prediction 0.48387154936790466 - improvement 2\n",
      "Step 2520 - Loss: -0.3254500925540924 - prediction 0.3254500925540924 - improvement 0\n",
      "Step 2530 - Loss: -0.32542043924331665 - prediction 0.32542043924331665 - improvement 0\n",
      "Step 2540 - Loss: -0.3254091441631317 - prediction 0.3254091441631317 - improvement 0\n",
      "Step 2550 - Loss: -0.484140008687973 - prediction 0.484140008687973 - improvement 2\n",
      "Step 2560 - Loss: -0.3256503939628601 - prediction 0.3256503939628601 - improvement 0\n",
      "Step 2570 - Loss: -0.19144582748413086 - prediction 0.19144582748413086 - improvement 1\n",
      "Step 2580 - Loss: -0.32563531398773193 - prediction 0.32563531398773193 - improvement 0\n",
      "Step 2590 - Loss: -0.32564055919647217 - prediction 0.32564055919647217 - improvement 0\n",
      "Step 2600 - Loss: -0.48305362462997437 - prediction 0.48305362462997437 - improvement 2\n",
      "Step 2610 - Loss: -0.4831582009792328 - prediction 0.4831582009792328 - improvement 2\n",
      "Step 2620 - Loss: -0.3256029486656189 - prediction 0.3256029486656189 - improvement 0\n",
      "Step 2630 - Loss: -0.19143712520599365 - prediction 0.19143712520599365 - improvement 1\n",
      "Step 2640 - Loss: -0.4828871190547943 - prediction 0.4828871190547943 - improvement 2\n",
      "Step 2650 - Loss: -0.32576948404312134 - prediction 0.32576948404312134 - improvement 0\n",
      "Step 2660 - Loss: -0.19151124358177185 - prediction 0.19151124358177185 - improvement 1\n",
      "Step 2670 - Loss: -0.19146567583084106 - prediction 0.19146567583084106 - improvement 1\n",
      "Step 2680 - Loss: -0.19128891825675964 - prediction 0.19128891825675964 - improvement 1\n",
      "Step 2690 - Loss: -0.4831993281841278 - prediction 0.4831993281841278 - improvement 2\n",
      "Step 2700 - Loss: -0.3255939185619354 - prediction 0.3255939185619354 - improvement 0\n",
      "Step 2710 - Loss: -0.3254372477531433 - prediction 0.3254372477531433 - improvement 0\n",
      "Step 2720 - Loss: -0.3252820670604706 - prediction 0.3252820670604706 - improvement 0\n",
      "Step 2730 - Loss: -0.48386862874031067 - prediction 0.48386862874031067 - improvement 2\n",
      "Step 2740 - Loss: -0.4840649962425232 - prediction 0.4840649962425232 - improvement 2\n",
      "Step 2750 - Loss: -0.4839363098144531 - prediction 0.4839363098144531 - improvement 2\n",
      "Step 2760 - Loss: -0.48367953300476074 - prediction 0.48367953300476074 - improvement 2\n",
      "Step 2770 - Loss: -0.3254893720149994 - prediction 0.3254893720149994 - improvement 0\n",
      "Step 2780 - Loss: -0.4837367832660675 - prediction 0.4837367832660675 - improvement 2\n",
      "Step 2790 - Loss: -0.32530006766319275 - prediction 0.32530006766319275 - improvement 0\n",
      "Step 2800 - Loss: -0.3252660930156708 - prediction 0.3252660930156708 - improvement 0\n",
      "Step 2810 - Loss: -0.4847574830055237 - prediction 0.4847574830055237 - improvement 2\n",
      "Step 2820 - Loss: -0.48458218574523926 - prediction 0.48458218574523926 - improvement 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2830 - Loss: -0.48453354835510254 - prediction 0.48453354835510254 - improvement 2\n",
      "Step 2840 - Loss: -0.48437702655792236 - prediction 0.48437702655792236 - improvement 2\n",
      "Step 2850 - Loss: -0.4843636453151703 - prediction 0.4843636453151703 - improvement 2\n",
      "Step 2860 - Loss: -0.3251582384109497 - prediction 0.3251582384109497 - improvement 0\n",
      "Step 2870 - Loss: -0.4845544099807739 - prediction 0.4845544099807739 - improvement 2\n",
      "Step 2880 - Loss: -0.3252749443054199 - prediction 0.3252749443054199 - improvement 0\n",
      "Step 2890 - Loss: -0.4841923415660858 - prediction 0.4841923415660858 - improvement 2\n",
      "Step 2900 - Loss: -0.4841248691082001 - prediction 0.4841248691082001 - improvement 2\n",
      "Step 2910 - Loss: -0.48394155502319336 - prediction 0.48394155502319336 - improvement 2\n",
      "Step 2920 - Loss: -0.19080713391304016 - prediction 0.19080713391304016 - improvement 1\n",
      "Step 2930 - Loss: -0.3254013955593109 - prediction 0.3254013955593109 - improvement 0\n",
      "Step 2940 - Loss: -0.4839397668838501 - prediction 0.4839397668838501 - improvement 2\n",
      "Step 2950 - Loss: -0.4837000072002411 - prediction 0.4837000072002411 - improvement 2\n",
      "Step 2960 - Loss: -0.3254421055316925 - prediction 0.3254421055316925 - improvement 0\n",
      "Step 2970 - Loss: -0.4841446578502655 - prediction 0.4841446578502655 - improvement 2\n",
      "Step 2980 - Loss: -0.19072090089321136 - prediction 0.19072090089321136 - improvement 1\n",
      "Step 2990 - Loss: -0.32543984055519104 - prediction 0.32543984055519104 - improvement 0\n",
      "Step 3000 - Loss: -0.4839327335357666 - prediction 0.4839327335357666 - improvement 2\n",
      "Step 3010 - Loss: -0.32530468702316284 - prediction 0.32530468702316284 - improvement 0\n",
      "Step 3020 - Loss: -0.3252851963043213 - prediction 0.3252851963043213 - improvement 0\n",
      "Step 3030 - Loss: -0.3252313435077667 - prediction 0.3252313435077667 - improvement 0\n",
      "Step 3040 - Loss: -0.32527318596839905 - prediction 0.32527318596839905 - improvement 0\n",
      "Step 3050 - Loss: -0.3252428472042084 - prediction 0.3252428472042084 - improvement 0\n",
      "Step 3060 - Loss: -0.48439788818359375 - prediction 0.48439788818359375 - improvement 2\n",
      "Step 3070 - Loss: -0.48457637429237366 - prediction 0.48457637429237366 - improvement 2\n",
      "Step 3080 - Loss: -0.3251180648803711 - prediction 0.3251180648803711 - improvement 0\n",
      "Step 3090 - Loss: -0.325088232755661 - prediction 0.325088232755661 - improvement 0\n",
      "Step 3100 - Loss: -0.32496917247772217 - prediction 0.32496917247772217 - improvement 0\n",
      "Step 3110 - Loss: -0.4854801595211029 - prediction 0.4854801595211029 - improvement 2\n",
      "Step 3120 - Loss: -0.4853020906448364 - prediction 0.4853020906448364 - improvement 2\n",
      "Step 3130 - Loss: -0.32488635182380676 - prediction 0.32488635182380676 - improvement 0\n",
      "Step 3140 - Loss: -0.32491496205329895 - prediction 0.32491496205329895 - improvement 0\n",
      "Step 3150 - Loss: -0.4852607250213623 - prediction 0.4852607250213623 - improvement 2\n",
      "Step 3160 - Loss: -0.48495835065841675 - prediction 0.48495835065841675 - improvement 2\n",
      "Step 3170 - Loss: -0.3251093924045563 - prediction 0.3251093924045563 - improvement 0\n",
      "Step 3180 - Loss: -0.4849197268486023 - prediction 0.4849197268486023 - improvement 2\n",
      "Step 3190 - Loss: -0.324940949678421 - prediction 0.324940949678421 - improvement 0\n",
      "Step 3200 - Loss: -0.48586544394493103 - prediction 0.48586544394493103 - improvement 2\n",
      "Step 3210 - Loss: -0.3245979845523834 - prediction 0.3245979845523834 - improvement 0\n",
      "Step 3220 - Loss: -0.4866633415222168 - prediction 0.4866633415222168 - improvement 2\n",
      "Step 3230 - Loss: -0.3245297372341156 - prediction 0.3245297372341156 - improvement 0\n",
      "Step 3240 - Loss: -0.4865867793560028 - prediction 0.4865867793560028 - improvement 2\n",
      "Step 3250 - Loss: -0.48656922578811646 - prediction 0.48656922578811646 - improvement 2\n",
      "Step 3260 - Loss: -0.48675447702407837 - prediction 0.48675447702407837 - improvement 2\n",
      "Step 3270 - Loss: -0.3245815932750702 - prediction 0.3245815932750702 - improvement 0\n",
      "Step 3280 - Loss: -0.4866684079170227 - prediction 0.4866684079170227 - improvement 2\n",
      "Step 3290 - Loss: -0.3244607150554657 - prediction 0.3244607150554657 - improvement 0\n",
      "Step 3300 - Loss: -0.3242935240268707 - prediction 0.3242935240268707 - improvement 0\n",
      "Step 3310 - Loss: -0.48796749114990234 - prediction 0.48796749114990234 - improvement 2\n",
      "Step 3320 - Loss: -0.4877346158027649 - prediction 0.4877346158027649 - improvement 2\n",
      "Step 3330 - Loss: -0.4875023663043976 - prediction 0.4875023663043976 - improvement 2\n",
      "Step 3340 - Loss: -0.4873381555080414 - prediction 0.4873381555080414 - improvement 2\n",
      "Step 3350 - Loss: -0.3242737650871277 - prediction 0.3242737650871277 - improvement 0\n",
      "Step 3360 - Loss: -0.18824222683906555 - prediction 0.18824222683906555 - improvement 1\n",
      "Step 3370 - Loss: -0.324188232421875 - prediction 0.324188232421875 - improvement 0\n",
      "Step 3380 - Loss: -0.4879087209701538 - prediction 0.4879087209701538 - improvement 2\n",
      "Step 3390 - Loss: -0.18806946277618408 - prediction 0.18806946277618408 - improvement 1\n",
      "Step 3400 - Loss: -0.18839320540428162 - prediction 0.18839320540428162 - improvement 1\n",
      "Step 3410 - Loss: -0.3243197798728943 - prediction 0.3243197798728943 - improvement 0\n",
      "Step 3420 - Loss: -0.48668885231018066 - prediction 0.48668885231018066 - improvement 2\n",
      "Step 3430 - Loss: -0.48671939969062805 - prediction 0.48671939969062805 - improvement 2\n",
      "Step 3440 - Loss: -0.32452234625816345 - prediction 0.32452234625816345 - improvement 0\n",
      "Step 3450 - Loss: -0.48676133155822754 - prediction 0.48676133155822754 - improvement 2\n",
      "Step 3460 - Loss: -0.32447007298469543 - prediction 0.32447007298469543 - improvement 0\n",
      "Step 3470 - Loss: -0.3243648111820221 - prediction 0.3243648111820221 - improvement 0\n",
      "Step 3480 - Loss: -0.32424551248550415 - prediction 0.32424551248550415 - improvement 0\n",
      "Step 3490 - Loss: -0.18801987171173096 - prediction 0.18801987171173096 - improvement 1\n",
      "Step 3500 - Loss: -0.4881366491317749 - prediction 0.4881366491317749 - improvement 2\n",
      "Step 3510 - Loss: -0.18826352059841156 - prediction 0.18826352059841156 - improvement 1\n",
      "Step 3520 - Loss: -0.4873915910720825 - prediction 0.4873915910720825 - improvement 2\n",
      "Step 3530 - Loss: -0.18853949010372162 - prediction 0.18853949010372162 - improvement 1\n",
      "Step 3540 - Loss: -0.48701125383377075 - prediction 0.48701125383377075 - improvement 2\n",
      "Step 3550 - Loss: -0.3244350850582123 - prediction 0.3244350850582123 - improvement 0\n",
      "Step 3560 - Loss: -0.48690274357795715 - prediction 0.48690274357795715 - improvement 2\n",
      "Step 3570 - Loss: -0.32440778613090515 - prediction 0.32440778613090515 - improvement 0\n",
      "Step 3580 - Loss: -0.4870631694793701 - prediction 0.4870631694793701 - improvement 2\n",
      "Step 3590 - Loss: -0.4864806830883026 - prediction 0.4864806830883026 - improvement 2\n",
      "Step 3600 - Loss: -0.4863414466381073 - prediction 0.4863414466381073 - improvement 2\n",
      "Step 3610 - Loss: -0.32466158270835876 - prediction 0.32466158270835876 - improvement 0\n",
      "Step 3620 - Loss: -0.3245023488998413 - prediction 0.3245023488998413 - improvement 0\n",
      "Step 3630 - Loss: -0.3244244456291199 - prediction 0.3244244456291199 - improvement 0\n",
      "Step 3640 - Loss: -0.48717427253723145 - prediction 0.48717427253723145 - improvement 2\n",
      "Step 3650 - Loss: -0.32426905632019043 - prediction 0.32426905632019043 - improvement 0\n",
      "Step 3660 - Loss: -0.32427743077278137 - prediction 0.32427743077278137 - improvement 0\n",
      "Step 3670 - Loss: -0.4874359965324402 - prediction 0.4874359965324402 - improvement 2\n",
      "Step 3680 - Loss: -0.4868699610233307 - prediction 0.4868699610233307 - improvement 2\n",
      "Step 3690 - Loss: -0.32441338896751404 - prediction 0.32441338896751404 - improvement 0\n",
      "Step 3700 - Loss: -0.48708686232566833 - prediction 0.48708686232566833 - improvement 2\n",
      "Step 3710 - Loss: -0.3242955207824707 - prediction 0.3242955207824707 - improvement 0\n",
      "Step 3720 - Loss: -0.1884230375289917 - prediction 0.1884230375289917 - improvement 1\n",
      "Step 3730 - Loss: -0.48707497119903564 - prediction 0.48707497119903564 - improvement 2\n",
      "Step 3740 - Loss: -0.48696210980415344 - prediction 0.48696210980415344 - improvement 2\n",
      "Step 3750 - Loss: -0.32451874017715454 - prediction 0.32451874017715454 - improvement 0\n",
      "Step 3760 - Loss: -0.18852493166923523 - prediction 0.18852493166923523 - improvement 1\n",
      "Step 3770 - Loss: -0.3245648443698883 - prediction 0.3245648443698883 - improvement 0\n",
      "Step 3780 - Loss: -0.3245275914669037 - prediction 0.3245275914669037 - improvement 0\n",
      "Step 3790 - Loss: -0.4869344234466553 - prediction 0.4869344234466553 - improvement 2\n",
      "Step 3800 - Loss: -0.18854331970214844 - prediction 0.18854331970214844 - improvement 1\n",
      "Step 3810 - Loss: -0.3243264853954315 - prediction 0.3243264853954315 - improvement 0\n",
      "Step 3820 - Loss: -0.18811152875423431 - prediction 0.18811152875423431 - improvement 1\n",
      "Step 3830 - Loss: -0.32427307963371277 - prediction 0.32427307963371277 - improvement 0\n",
      "Step 3840 - Loss: -0.48711729049682617 - prediction 0.48711729049682617 - improvement 2\n",
      "Step 3850 - Loss: -0.4869649410247803 - prediction 0.4869649410247803 - improvement 2\n",
      "Step 3860 - Loss: -0.18857286870479584 - prediction 0.18857286870479584 - improvement 1\n",
      "Step 3870 - Loss: -0.32442858815193176 - prediction 0.32442858815193176 - improvement 0\n",
      "Step 3880 - Loss: -0.3244531452655792 - prediction 0.3244531452655792 - improvement 0\n",
      "Step 3890 - Loss: -0.32451188564300537 - prediction 0.32451188564300537 - improvement 0\n",
      "Step 3900 - Loss: -0.48682472109794617 - prediction 0.48682472109794617 - improvement 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3910 - Loss: -0.48657655715942383 - prediction 0.48657655715942383 - improvement 2\n",
      "Step 3920 - Loss: -0.3245750963687897 - prediction 0.3245750963687897 - improvement 0\n",
      "Step 3930 - Loss: -0.3244992792606354 - prediction 0.3244992792606354 - improvement 0\n",
      "Step 3940 - Loss: -0.48700326681137085 - prediction 0.48700326681137085 - improvement 2\n",
      "Step 3950 - Loss: -0.3243577480316162 - prediction 0.3243577480316162 - improvement 0\n",
      "Step 3960 - Loss: -0.48727959394454956 - prediction 0.48727959394454956 - improvement 2\n",
      "Step 3970 - Loss: -0.32443001866340637 - prediction 0.32443001866340637 - improvement 0\n",
      "Step 3980 - Loss: -0.487055242061615 - prediction 0.487055242061615 - improvement 2\n",
      "Step 3990 - Loss: -0.32440274953842163 - prediction 0.32440274953842163 - improvement 0\n",
      "Step 4000 - Loss: -0.18836189806461334 - prediction 0.18836189806461334 - improvement 1\n",
      "Step 4010 - Loss: -0.18841461837291718 - prediction 0.18841461837291718 - improvement 1\n",
      "Step 4020 - Loss: -0.4873652160167694 - prediction 0.4873652160167694 - improvement 2\n",
      "Step 4030 - Loss: -0.48730722069740295 - prediction 0.48730722069740295 - improvement 2\n",
      "Step 4040 - Loss: -0.4869411885738373 - prediction 0.4869411885738373 - improvement 2\n",
      "Step 4050 - Loss: -0.48669958114624023 - prediction 0.48669958114624023 - improvement 2\n",
      "Step 4060 - Loss: -0.32462602853775024 - prediction 0.32462602853775024 - improvement 0\n",
      "Step 4070 - Loss: -0.4864789843559265 - prediction 0.4864789843559265 - improvement 2\n",
      "Step 4080 - Loss: -0.4862060248851776 - prediction 0.4862060248851776 - improvement 2\n",
      "Step 4090 - Loss: -0.48629137873649597 - prediction 0.48629137873649597 - improvement 2\n",
      "Step 4100 - Loss: -0.3248694837093353 - prediction 0.3248694837093353 - improvement 0\n",
      "Step 4110 - Loss: -0.485887736082077 - prediction 0.485887736082077 - improvement 2\n",
      "Step 4120 - Loss: -0.485603928565979 - prediction 0.485603928565979 - improvement 2\n",
      "Step 4130 - Loss: -0.18988798558712006 - prediction 0.18988798558712006 - improvement 1\n",
      "Step 4140 - Loss: -0.48496127128601074 - prediction 0.48496127128601074 - improvement 2\n",
      "Step 4150 - Loss: -0.32503408193588257 - prediction 0.32503408193588257 - improvement 0\n",
      "Step 4160 - Loss: -0.3250630795955658 - prediction 0.3250630795955658 - improvement 0\n",
      "Step 4170 - Loss: -0.4849162697792053 - prediction 0.4849162697792053 - improvement 2\n",
      "Step 4180 - Loss: -0.4847777187824249 - prediction 0.4847777187824249 - improvement 2\n",
      "Step 4190 - Loss: -0.4843631088733673 - prediction 0.4843631088733673 - improvement 2\n",
      "Step 4200 - Loss: -0.4843085706233978 - prediction 0.4843085706233978 - improvement 2\n",
      "Step 4210 - Loss: -0.4847256541252136 - prediction 0.4847256541252136 - improvement 2\n",
      "Step 4220 - Loss: -0.3251369297504425 - prediction 0.3251369297504425 - improvement 0\n",
      "Step 4230 - Loss: -0.32507431507110596 - prediction 0.32507431507110596 - improvement 0\n",
      "Step 4240 - Loss: -0.48478230834007263 - prediction 0.48478230834007263 - improvement 2\n",
      "Step 4250 - Loss: -0.48420631885528564 - prediction 0.48420631885528564 - improvement 2\n",
      "Step 4260 - Loss: -0.4840620756149292 - prediction 0.4840620756149292 - improvement 2\n",
      "Step 4270 - Loss: -0.19063688814640045 - prediction 0.19063688814640045 - improvement 1\n",
      "Step 4280 - Loss: -0.48402541875839233 - prediction 0.48402541875839233 - improvement 2\n",
      "Step 4290 - Loss: -0.4836982190608978 - prediction 0.4836982190608978 - improvement 2\n",
      "Step 4300 - Loss: -0.4839368462562561 - prediction 0.4839368462562561 - improvement 2\n",
      "Step 4310 - Loss: -0.32539117336273193 - prediction 0.32539117336273193 - improvement 0\n",
      "Step 4320 - Loss: -0.32554006576538086 - prediction 0.32554006576538086 - improvement 0\n",
      "Step 4330 - Loss: -0.32548198103904724 - prediction 0.32548198103904724 - improvement 0\n",
      "Step 4340 - Loss: -0.4835643768310547 - prediction 0.4835643768310547 - improvement 2\n",
      "Step 4350 - Loss: -0.4835497736930847 - prediction 0.4835497736930847 - improvement 2\n",
      "Step 4360 - Loss: -0.19136685132980347 - prediction 0.19136685132980347 - improvement 1\n",
      "Step 4370 - Loss: -0.4824545979499817 - prediction 0.4824545979499817 - improvement 2\n",
      "Step 4380 - Loss: -0.1912156045436859 - prediction 0.1912156045436859 - improvement 1\n",
      "Step 4390 - Loss: -0.19118022918701172 - prediction 0.19118022918701172 - improvement 1\n",
      "Step 4400 - Loss: -0.4832978844642639 - prediction 0.4832978844642639 - improvement 2\n",
      "Step 4410 - Loss: -0.3255431056022644 - prediction 0.3255431056022644 - improvement 0\n",
      "Step 4420 - Loss: -0.48336705565452576 - prediction 0.48336705565452576 - improvement 2\n",
      "Step 4430 - Loss: -0.32549604773521423 - prediction 0.32549604773521423 - improvement 0\n",
      "Step 4440 - Loss: -0.48385581374168396 - prediction 0.48385581374168396 - improvement 2\n",
      "Step 4450 - Loss: -0.48418423533439636 - prediction 0.48418423533439636 - improvement 2\n",
      "Step 4460 - Loss: -0.48377999663352966 - prediction 0.48377999663352966 - improvement 2\n",
      "Step 4470 - Loss: -0.4839828610420227 - prediction 0.4839828610420227 - improvement 2\n",
      "Step 4480 - Loss: -0.3254352807998657 - prediction 0.3254352807998657 - improvement 0\n",
      "Step 4490 - Loss: -0.32545167207717896 - prediction 0.32545167207717896 - improvement 0\n",
      "Step 4500 - Loss: -0.325368732213974 - prediction 0.325368732213974 - improvement 0\n",
      "Step 4510 - Loss: -0.4833389222621918 - prediction 0.4833389222621918 - improvement 2\n",
      "Step 4520 - Loss: -0.32571539282798767 - prediction 0.32571539282798767 - improvement 0\n",
      "Step 4530 - Loss: -0.1915162205696106 - prediction 0.1915162205696106 - improvement 1\n",
      "Step 4540 - Loss: -0.4830189347267151 - prediction 0.4830189347267151 - improvement 2\n",
      "Step 4550 - Loss: -0.4827675223350525 - prediction 0.4827675223350525 - improvement 2\n",
      "Step 4560 - Loss: -0.4827539622783661 - prediction 0.4827539622783661 - improvement 2\n",
      "Step 4570 - Loss: -0.3257226049900055 - prediction 0.3257226049900055 - improvement 0\n",
      "Step 4580 - Loss: -0.4832228124141693 - prediction 0.4832228124141693 - improvement 2\n",
      "Step 4590 - Loss: -0.32561245560646057 - prediction 0.32561245560646057 - improvement 0\n",
      "Step 4600 - Loss: -0.3255210816860199 - prediction 0.3255210816860199 - improvement 0\n",
      "Step 4610 - Loss: -0.3256224989891052 - prediction 0.3256224989891052 - improvement 0\n",
      "Step 4620 - Loss: -0.4827156364917755 - prediction 0.4827156364917755 - improvement 2\n",
      "Step 4630 - Loss: -0.32576560974121094 - prediction 0.32576560974121094 - improvement 0\n",
      "Step 4640 - Loss: -0.48310592770576477 - prediction 0.48310592770576477 - improvement 2\n",
      "Step 4650 - Loss: -0.48311060667037964 - prediction 0.48311060667037964 - improvement 2\n",
      "Step 4660 - Loss: -0.48302069306373596 - prediction 0.48302069306373596 - improvement 2\n",
      "Step 4670 - Loss: -0.32560059428215027 - prediction 0.32560059428215027 - improvement 0\n",
      "Step 4680 - Loss: -0.48320984840393066 - prediction 0.48320984840393066 - improvement 2\n",
      "Step 4690 - Loss: -0.48348772525787354 - prediction 0.48348772525787354 - improvement 2\n",
      "Step 4700 - Loss: -0.3255045413970947 - prediction 0.3255045413970947 - improvement 0\n",
      "Step 4710 - Loss: -0.32541975378990173 - prediction 0.32541975378990173 - improvement 0\n",
      "Step 4720 - Loss: -0.48375311493873596 - prediction 0.48375311493873596 - improvement 2\n",
      "Step 4730 - Loss: -0.4836643636226654 - prediction 0.4836643636226654 - improvement 2\n",
      "Step 4740 - Loss: -0.48340165615081787 - prediction 0.48340165615081787 - improvement 2\n",
      "Step 4750 - Loss: -0.3255784213542938 - prediction 0.3255784213542938 - improvement 0\n",
      "Step 4760 - Loss: -0.32565706968307495 - prediction 0.32565706968307495 - improvement 0\n",
      "Step 4770 - Loss: -0.32555311918258667 - prediction 0.32555311918258667 - improvement 0\n",
      "Step 4780 - Loss: -0.32551583647727966 - prediction 0.32551583647727966 - improvement 0\n",
      "Step 4790 - Loss: -0.48318052291870117 - prediction 0.48318052291870117 - improvement 2\n",
      "Step 4800 - Loss: -0.4831288754940033 - prediction 0.4831288754940033 - improvement 2\n",
      "Step 4810 - Loss: -0.3257278800010681 - prediction 0.3257278800010681 - improvement 0\n",
      "Step 4820 - Loss: -0.32563215494155884 - prediction 0.32563215494155884 - improvement 0\n",
      "Step 4830 - Loss: -0.48360180854797363 - prediction 0.48360180854797363 - improvement 2\n",
      "Step 4840 - Loss: -0.48336297273635864 - prediction 0.48336297273635864 - improvement 2\n",
      "Step 4850 - Loss: -0.32555821537971497 - prediction 0.32555821537971497 - improvement 0\n",
      "Step 4860 - Loss: -0.32556629180908203 - prediction 0.32556629180908203 - improvement 0\n",
      "Step 4870 - Loss: -0.4833706021308899 - prediction 0.4833706021308899 - improvement 2\n",
      "Step 4880 - Loss: -0.32565808296203613 - prediction 0.32565808296203613 - improvement 0\n",
      "Step 4890 - Loss: -0.32560721039772034 - prediction 0.32560721039772034 - improvement 0\n",
      "Step 4900 - Loss: -0.3255119025707245 - prediction 0.3255119025707245 - improvement 0\n",
      "Step 4910 - Loss: -0.1911046952009201 - prediction 0.1911046952009201 - improvement 1\n",
      "Step 4920 - Loss: -0.4832134246826172 - prediction 0.4832134246826172 - improvement 2\n",
      "Step 4930 - Loss: -0.48297247290611267 - prediction 0.48297247290611267 - improvement 2\n",
      "Step 4940 - Loss: -0.3257404565811157 - prediction 0.3257404565811157 - improvement 0\n",
      "Step 4950 - Loss: -0.19165655970573425 - prediction 0.19165655970573425 - improvement 1\n",
      "Step 4960 - Loss: -0.4825863838195801 - prediction 0.4825863838195801 - improvement 2\n",
      "Step 4970 - Loss: -0.32570716738700867 - prediction 0.32570716738700867 - improvement 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4980 - Loss: -0.32571282982826233 - prediction 0.32571282982826233 - improvement 0\n",
      "Step 4990 - Loss: -0.48281586170196533 - prediction 0.48281586170196533 - improvement 2\n",
      "Step 5000 - Loss: -0.4819721281528473 - prediction 0.4819721281528473 - improvement 2\n",
      "Step 5010 - Loss: -0.32610946893692017 - prediction 0.32610946893692017 - improvement 0\n",
      "Step 5020 - Loss: -0.19230212271213531 - prediction 0.19230212271213531 - improvement 1\n",
      "Step 5030 - Loss: -0.48129498958587646 - prediction 0.48129498958587646 - improvement 2\n",
      "Step 5040 - Loss: -0.3261502981185913 - prediction 0.3261502981185913 - improvement 0\n",
      "Step 5050 - Loss: -0.192464679479599 - prediction 0.192464679479599 - improvement 1\n",
      "Step 5060 - Loss: -0.32605940103530884 - prediction 0.32605940103530884 - improvement 0\n",
      "Step 5070 - Loss: -0.4817889630794525 - prediction 0.4817889630794525 - improvement 2\n",
      "Step 5080 - Loss: -0.32584038376808167 - prediction 0.32584038376808167 - improvement 0\n",
      "Step 5090 - Loss: -0.3257553279399872 - prediction 0.3257553279399872 - improvement 0\n",
      "Step 5100 - Loss: -0.32572102546691895 - prediction 0.32572102546691895 - improvement 0\n",
      "Step 5110 - Loss: -0.4826660752296448 - prediction 0.4826660752296448 - improvement 2\n",
      "Step 5120 - Loss: -0.32576844096183777 - prediction 0.32576844096183777 - improvement 0\n",
      "Step 5130 - Loss: -0.4829253852367401 - prediction 0.4829253852367401 - improvement 2\n",
      "Step 5140 - Loss: -0.4833552837371826 - prediction 0.4833552837371826 - improvement 2\n",
      "Step 5150 - Loss: -0.48337411880493164 - prediction 0.48337411880493164 - improvement 2\n",
      "Step 5160 - Loss: -0.3255193531513214 - prediction 0.3255193531513214 - improvement 0\n",
      "Step 5170 - Loss: -0.48327264189720154 - prediction 0.48327264189720154 - improvement 2\n",
      "Step 5180 - Loss: -0.48322397470474243 - prediction 0.48322397470474243 - improvement 2\n",
      "Step 5190 - Loss: -0.3256935477256775 - prediction 0.3256935477256775 - improvement 0\n",
      "Step 5200 - Loss: -0.19146443903446198 - prediction 0.19146443903446198 - improvement 1\n",
      "Step 5210 - Loss: -0.32573381066322327 - prediction 0.32573381066322327 - improvement 0\n",
      "Step 5220 - Loss: -0.3256666958332062 - prediction 0.3256666958332062 - improvement 0\n",
      "Step 5230 - Loss: -0.48306187987327576 - prediction 0.48306187987327576 - improvement 2\n",
      "Step 5240 - Loss: -0.4830736219882965 - prediction 0.4830736219882965 - improvement 2\n",
      "Step 5250 - Loss: -0.32579606771469116 - prediction 0.32579606771469116 - improvement 0\n",
      "Step 5260 - Loss: -0.3257232904434204 - prediction 0.3257232904434204 - improvement 0\n",
      "Step 5270 - Loss: -0.48284292221069336 - prediction 0.48284292221069336 - improvement 2\n",
      "Step 5280 - Loss: -0.48298949003219604 - prediction 0.48298949003219604 - improvement 2\n",
      "Step 5290 - Loss: -0.19115352630615234 - prediction 0.19115352630615234 - improvement 1\n",
      "Step 5300 - Loss: -0.4834408760070801 - prediction 0.4834408760070801 - improvement 2\n",
      "Step 5310 - Loss: -0.32563984394073486 - prediction 0.32563984394073486 - improvement 0\n",
      "Step 5320 - Loss: -0.3255738615989685 - prediction 0.3255738615989685 - improvement 0\n",
      "Step 5330 - Loss: -0.19119997322559357 - prediction 0.19119997322559357 - improvement 1\n",
      "Step 5340 - Loss: -0.4833049476146698 - prediction 0.4833049476146698 - improvement 2\n",
      "Step 5350 - Loss: -0.4838138222694397 - prediction 0.4838138222694397 - improvement 2\n",
      "Step 5360 - Loss: -0.48394790291786194 - prediction 0.48394790291786194 - improvement 2\n",
      "Step 5370 - Loss: -0.3254519999027252 - prediction 0.3254519999027252 - improvement 0\n",
      "Step 5380 - Loss: -0.4837087094783783 - prediction 0.4837087094783783 - improvement 2\n",
      "Step 5390 - Loss: -0.4837939739227295 - prediction 0.4837939739227295 - improvement 2\n",
      "Step 5400 - Loss: -0.4837069809436798 - prediction 0.4837069809436798 - improvement 2\n",
      "Step 5410 - Loss: -0.19116955995559692 - prediction 0.19116955995559692 - improvement 1\n",
      "Step 5420 - Loss: -0.48290130496025085 - prediction 0.48290130496025085 - improvement 2\n",
      "Step 5430 - Loss: -0.32571175694465637 - prediction 0.32571175694465637 - improvement 0\n",
      "Step 5440 - Loss: -0.4826737344264984 - prediction 0.4826737344264984 - improvement 2\n",
      "Step 5450 - Loss: -0.48251786828041077 - prediction 0.48251786828041077 - improvement 2\n",
      "Step 5460 - Loss: -0.4823113679885864 - prediction 0.4823113679885864 - improvement 2\n",
      "Step 5470 - Loss: -0.4822770357131958 - prediction 0.4822770357131958 - improvement 2\n",
      "Step 5480 - Loss: -0.3258921802043915 - prediction 0.3258921802043915 - improvement 0\n",
      "Step 5490 - Loss: -0.48229360580444336 - prediction 0.48229360580444336 - improvement 2\n",
      "Step 5500 - Loss: -0.3258973956108093 - prediction 0.3258973956108093 - improvement 0\n",
      "Step 5510 - Loss: -0.32589271664619446 - prediction 0.32589271664619446 - improvement 0\n",
      "Step 5520 - Loss: -0.48245692253112793 - prediction 0.48245692253112793 - improvement 2\n",
      "Step 5530 - Loss: -0.19158633053302765 - prediction 0.19158633053302765 - improvement 1\n",
      "Step 5540 - Loss: -0.48273923993110657 - prediction 0.48273923993110657 - improvement 2\n",
      "Step 5550 - Loss: -0.3257925510406494 - prediction 0.3257925510406494 - improvement 0\n",
      "Step 5560 - Loss: -0.4827580749988556 - prediction 0.4827580749988556 - improvement 2\n",
      "Step 5570 - Loss: -0.3257483243942261 - prediction 0.3257483243942261 - improvement 0\n",
      "Step 5580 - Loss: -0.4823906719684601 - prediction 0.4823906719684601 - improvement 2\n",
      "Step 5590 - Loss: -0.4822331666946411 - prediction 0.4822331666946411 - improvement 2\n",
      "Step 5600 - Loss: -0.3259682357311249 - prediction 0.3259682357311249 - improvement 0\n",
      "Step 5610 - Loss: -0.32585957646369934 - prediction 0.32585957646369934 - improvement 0\n",
      "Step 5620 - Loss: -0.4819946587085724 - prediction 0.4819946587085724 - improvement 2\n",
      "Step 5630 - Loss: -0.48206058144569397 - prediction 0.48206058144569397 - improvement 2\n",
      "Step 5640 - Loss: -0.48159298300743103 - prediction 0.48159298300743103 - improvement 2\n",
      "Step 5650 - Loss: -0.48181039094924927 - prediction 0.48181039094924927 - improvement 2\n",
      "Step 5660 - Loss: -0.4815088212490082 - prediction 0.4815088212490082 - improvement 2\n",
      "Step 5670 - Loss: -0.48158523440361023 - prediction 0.48158523440361023 - improvement 2\n",
      "Step 5680 - Loss: -0.32609283924102783 - prediction 0.32609283924102783 - improvement 0\n",
      "Step 5690 - Loss: -0.4813045561313629 - prediction 0.4813045561313629 - improvement 2\n",
      "Step 5700 - Loss: -0.1926824152469635 - prediction 0.1926824152469635 - improvement 1\n",
      "Step 5710 - Loss: -0.326074481010437 - prediction 0.326074481010437 - improvement 0\n",
      "Step 5720 - Loss: -0.4818127155303955 - prediction 0.4818127155303955 - improvement 2\n",
      "Step 5730 - Loss: -0.48157450556755066 - prediction 0.48157450556755066 - improvement 2\n",
      "Step 5740 - Loss: -0.19228731095790863 - prediction 0.19228731095790863 - improvement 1\n",
      "Step 5750 - Loss: -0.48179134726524353 - prediction 0.48179134726524353 - improvement 2\n",
      "Step 5760 - Loss: -0.48201248049736023 - prediction 0.48201248049736023 - improvement 2\n",
      "Step 5770 - Loss: -0.48178958892822266 - prediction 0.48178958892822266 - improvement 2\n",
      "Step 5780 - Loss: -0.3261604905128479 - prediction 0.3261604905128479 - improvement 0\n",
      "Step 5790 - Loss: -0.4810938239097595 - prediction 0.4810938239097595 - improvement 2\n",
      "Step 5800 - Loss: -0.3263359069824219 - prediction 0.3263359069824219 - improvement 0\n",
      "Step 5810 - Loss: -0.326241672039032 - prediction 0.326241672039032 - improvement 0\n",
      "Step 5820 - Loss: -0.32623669505119324 - prediction 0.32623669505119324 - improvement 0\n",
      "Step 5830 - Loss: -0.32624271512031555 - prediction 0.32624271512031555 - improvement 0\n",
      "Step 5840 - Loss: -0.326156884431839 - prediction 0.326156884431839 - improvement 0\n",
      "Step 5850 - Loss: -0.32605764269828796 - prediction 0.32605764269828796 - improvement 0\n",
      "Step 5860 - Loss: -0.19211748242378235 - prediction 0.19211748242378235 - improvement 1\n",
      "Step 5870 - Loss: -0.19244049489498138 - prediction 0.19244049489498138 - improvement 1\n",
      "Step 5880 - Loss: -0.48160967230796814 - prediction 0.48160967230796814 - improvement 2\n",
      "Step 5890 - Loss: -0.32604676485061646 - prediction 0.32604676485061646 - improvement 0\n",
      "Step 5900 - Loss: -0.4814796447753906 - prediction 0.4814796447753906 - improvement 2\n",
      "Step 5910 - Loss: -0.32614272832870483 - prediction 0.32614272832870483 - improvement 0\n",
      "Step 5920 - Loss: -0.3261744976043701 - prediction 0.3261744976043701 - improvement 0\n",
      "Step 5930 - Loss: -0.48099374771118164 - prediction 0.48099374771118164 - improvement 2\n",
      "Step 5940 - Loss: -0.3261491060256958 - prediction 0.3261491060256958 - improvement 0\n",
      "Step 5950 - Loss: -0.19243501126766205 - prediction 0.19243501126766205 - improvement 1\n",
      "Step 5960 - Loss: -0.4814593195915222 - prediction 0.4814593195915222 - improvement 2\n",
      "Step 5970 - Loss: -0.19245705008506775 - prediction 0.19245705008506775 - improvement 1\n",
      "Step 5980 - Loss: -0.32611122727394104 - prediction 0.32611122727394104 - improvement 0\n",
      "Step 5990 - Loss: -0.4812932312488556 - prediction 0.4812932312488556 - improvement 2\n",
      "Step 6000 - Loss: -0.4813344478607178 - prediction 0.4813344478607178 - improvement 2\n",
      "Step 6010 - Loss: -0.3261219263076782 - prediction 0.3261219263076782 - improvement 0\n",
      "Step 6020 - Loss: -0.3261108696460724 - prediction 0.3261108696460724 - improvement 0\n",
      "Step 6030 - Loss: -0.481845498085022 - prediction 0.481845498085022 - improvement 2\n",
      "Step 6040 - Loss: -0.3261178135871887 - prediction 0.3261178135871887 - improvement 0\n",
      "Step 6050 - Loss: -0.32619452476501465 - prediction 0.32619452476501465 - improvement 0\n",
      "Step 6060 - Loss: -0.48149213194847107 - prediction 0.48149213194847107 - improvement 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6070 - Loss: -0.3261478841304779 - prediction 0.3261478841304779 - improvement 0\n",
      "Step 6080 - Loss: -0.48099076747894287 - prediction 0.48099076747894287 - improvement 2\n",
      "Step 6090 - Loss: -0.32621148228645325 - prediction 0.32621148228645325 - improvement 0\n",
      "Step 6100 - Loss: -0.48138701915740967 - prediction 0.48138701915740967 - improvement 2\n",
      "Step 6110 - Loss: -0.32611677050590515 - prediction 0.32611677050590515 - improvement 0\n",
      "Step 6120 - Loss: -0.48178184032440186 - prediction 0.48178184032440186 - improvement 2\n",
      "Step 6130 - Loss: -0.32597315311431885 - prediction 0.32597315311431885 - improvement 0\n",
      "Step 6140 - Loss: -0.4817996621131897 - prediction 0.4817996621131897 - improvement 2\n",
      "Step 6150 - Loss: -0.32606858015060425 - prediction 0.32606858015060425 - improvement 0\n",
      "Step 6160 - Loss: -0.48169252276420593 - prediction 0.48169252276420593 - improvement 2\n",
      "Step 6170 - Loss: -0.32613545656204224 - prediction 0.32613545656204224 - improvement 0\n",
      "Step 6180 - Loss: -0.4818466901779175 - prediction 0.4818466901779175 - improvement 2\n",
      "Step 6190 - Loss: -0.4819946587085724 - prediction 0.4819946587085724 - improvement 2\n",
      "Step 6200 - Loss: -0.3259618282318115 - prediction 0.3259618282318115 - improvement 0\n",
      "Step 6210 - Loss: -0.3261484205722809 - prediction 0.3261484205722809 - improvement 0\n",
      "Step 6220 - Loss: -0.4816245436668396 - prediction 0.4816245436668396 - improvement 2\n",
      "Step 6230 - Loss: -0.3261018991470337 - prediction 0.3261018991470337 - improvement 0\n",
      "Step 6240 - Loss: -0.32598111033439636 - prediction 0.32598111033439636 - improvement 0\n",
      "Step 6250 - Loss: -0.32597658038139343 - prediction 0.32597658038139343 - improvement 0\n",
      "Step 6260 - Loss: -0.3259526193141937 - prediction 0.3259526193141937 - improvement 0\n",
      "Step 6270 - Loss: -0.3258044421672821 - prediction 0.3258044421672821 - improvement 0\n",
      "Step 6280 - Loss: -0.48226165771484375 - prediction 0.48226165771484375 - improvement 2\n",
      "Step 6290 - Loss: -0.4823835790157318 - prediction 0.4823835790157318 - improvement 2\n",
      "Step 6300 - Loss: -0.48200303316116333 - prediction 0.48200303316116333 - improvement 2\n",
      "Step 6310 - Loss: -0.4821276366710663 - prediction 0.4821276366710663 - improvement 2\n",
      "Step 6320 - Loss: -0.32605576515197754 - prediction 0.32605576515197754 - improvement 0\n",
      "Step 6330 - Loss: -0.3260675370693207 - prediction 0.3260675370693207 - improvement 0\n",
      "Step 6340 - Loss: -0.3260762095451355 - prediction 0.3260762095451355 - improvement 0\n",
      "Step 6350 - Loss: -0.4816180169582367 - prediction 0.4816180169582367 - improvement 2\n",
      "Step 6360 - Loss: -0.4814610779285431 - prediction 0.4814610779285431 - improvement 2\n",
      "Step 6370 - Loss: -0.3261258900165558 - prediction 0.3261258900165558 - improvement 0\n",
      "Step 6380 - Loss: -0.3261122405529022 - prediction 0.3261122405529022 - improvement 0\n",
      "Step 6390 - Loss: -0.3260860741138458 - prediction 0.3260860741138458 - improvement 0\n",
      "Step 6400 - Loss: -0.3262268602848053 - prediction 0.3262268602848053 - improvement 0\n",
      "Step 6410 - Loss: -0.32612645626068115 - prediction 0.32612645626068115 - improvement 0\n",
      "Step 6420 - Loss: -0.32603806257247925 - prediction 0.32603806257247925 - improvement 0\n",
      "Step 6430 - Loss: -0.32599517703056335 - prediction 0.32599517703056335 - improvement 0\n",
      "Step 6440 - Loss: -0.3259839117527008 - prediction 0.3259839117527008 - improvement 0\n",
      "Step 6450 - Loss: -0.32591360807418823 - prediction 0.32591360807418823 - improvement 0\n",
      "Step 6460 - Loss: -0.4819287657737732 - prediction 0.4819287657737732 - improvement 2\n",
      "Step 6470 - Loss: -0.4820707142353058 - prediction 0.4820707142353058 - improvement 2\n",
      "Step 6480 - Loss: -0.325973778963089 - prediction 0.325973778963089 - improvement 0\n",
      "Step 6490 - Loss: -0.4818359613418579 - prediction 0.4818359613418579 - improvement 2\n",
      "Step 6500 - Loss: -0.48163947463035583 - prediction 0.48163947463035583 - improvement 2\n",
      "Step 6510 - Loss: -0.19247739017009735 - prediction 0.19247739017009735 - improvement 1\n",
      "Step 6520 - Loss: -0.48132845759391785 - prediction 0.48132845759391785 - improvement 2\n",
      "Step 6530 - Loss: -0.4813153147697449 - prediction 0.4813153147697449 - improvement 2\n",
      "Step 6540 - Loss: -0.3262212872505188 - prediction 0.3262212872505188 - improvement 0\n",
      "Step 6550 - Loss: -0.480910986661911 - prediction 0.480910986661911 - improvement 2\n",
      "Step 6560 - Loss: -0.3261648118495941 - prediction 0.3261648118495941 - improvement 0\n",
      "Step 6570 - Loss: -0.32629334926605225 - prediction 0.32629334926605225 - improvement 0\n",
      "Step 6580 - Loss: -0.1927541047334671 - prediction 0.1927541047334671 - improvement 1\n",
      "Step 6590 - Loss: -0.3263069689273834 - prediction 0.3263069689273834 - improvement 0\n",
      "Step 6600 - Loss: -0.32619887590408325 - prediction 0.32619887590408325 - improvement 0\n",
      "Step 6610 - Loss: -0.3262263238430023 - prediction 0.3262263238430023 - improvement 0\n",
      "Step 6620 - Loss: -0.4813607335090637 - prediction 0.4813607335090637 - improvement 2\n",
      "Step 6630 - Loss: -0.48125550150871277 - prediction 0.48125550150871277 - improvement 2\n",
      "Step 6640 - Loss: -0.3261728286743164 - prediction 0.3261728286743164 - improvement 0\n",
      "Step 6650 - Loss: -0.3261629641056061 - prediction 0.3261629641056061 - improvement 0\n",
      "Step 6660 - Loss: -0.32614028453826904 - prediction 0.32614028453826904 - improvement 0\n",
      "Step 6670 - Loss: -0.32612335681915283 - prediction 0.32612335681915283 - improvement 0\n",
      "Step 6680 - Loss: -0.4815458655357361 - prediction 0.4815458655357361 - improvement 2\n",
      "Step 6690 - Loss: -0.3261055052280426 - prediction 0.3261055052280426 - improvement 0\n",
      "Step 6700 - Loss: -0.48152321577072144 - prediction 0.48152321577072144 - improvement 2\n",
      "Step 6710 - Loss: -0.48152440786361694 - prediction 0.48152440786361694 - improvement 2\n",
      "Step 6720 - Loss: -0.1925576627254486 - prediction 0.1925576627254486 - improvement 1\n",
      "Step 6730 - Loss: -0.192616805434227 - prediction 0.192616805434227 - improvement 1\n",
      "Step 6740 - Loss: -0.4810507297515869 - prediction 0.4810507297515869 - improvement 2\n",
      "Step 6750 - Loss: -0.19271229207515717 - prediction 0.19271229207515717 - improvement 1\n",
      "Step 6760 - Loss: -0.4805857539176941 - prediction 0.4805857539176941 - improvement 2\n",
      "Step 6770 - Loss: -0.32635390758514404 - prediction 0.32635390758514404 - improvement 0\n",
      "Step 6780 - Loss: -0.4807121157646179 - prediction 0.4807121157646179 - improvement 2\n",
      "Step 6790 - Loss: -0.3263090252876282 - prediction 0.3263090252876282 - improvement 0\n",
      "Step 6800 - Loss: -0.4809625446796417 - prediction 0.4809625446796417 - improvement 2\n",
      "Step 6810 - Loss: -0.19262191653251648 - prediction 0.19262191653251648 - improvement 1\n",
      "Step 6820 - Loss: -0.326226145029068 - prediction 0.326226145029068 - improvement 0\n",
      "Step 6830 - Loss: -0.32621923089027405 - prediction 0.32621923089027405 - improvement 0\n",
      "Step 6840 - Loss: -0.19254957139492035 - prediction 0.19254957139492035 - improvement 1\n",
      "Step 6850 - Loss: -0.3261461853981018 - prediction 0.3261461853981018 - improvement 0\n",
      "Step 6860 - Loss: -0.19253897666931152 - prediction 0.19253897666931152 - improvement 1\n",
      "Step 6870 - Loss: -0.326030433177948 - prediction 0.326030433177948 - improvement 0\n",
      "Step 6880 - Loss: -0.3261352777481079 - prediction 0.3261352777481079 - improvement 0\n",
      "Step 6890 - Loss: -0.4810686707496643 - prediction 0.4810686707496643 - improvement 2\n",
      "Step 6900 - Loss: -0.32621076703071594 - prediction 0.32621076703071594 - improvement 0\n",
      "Step 6910 - Loss: -0.4811016619205475 - prediction 0.4811016619205475 - improvement 2\n",
      "Step 6920 - Loss: -0.4808371067047119 - prediction 0.4808371067047119 - improvement 2\n",
      "Step 6930 - Loss: -0.32632210850715637 - prediction 0.32632210850715637 - improvement 0\n",
      "Step 6940 - Loss: -0.4805978238582611 - prediction 0.4805978238582611 - improvement 2\n",
      "Step 6950 - Loss: -0.480624258518219 - prediction 0.480624258518219 - improvement 2\n",
      "Step 6960 - Loss: -0.4807121157646179 - prediction 0.4807121157646179 - improvement 2\n",
      "Step 6970 - Loss: -0.48044657707214355 - prediction 0.48044657707214355 - improvement 2\n",
      "Step 6980 - Loss: -0.3263695240020752 - prediction 0.3263695240020752 - improvement 0\n",
      "Step 6990 - Loss: -0.32634106278419495 - prediction 0.32634106278419495 - improvement 0\n",
      "Step 7000 - Loss: -0.19308596849441528 - prediction 0.19308596849441528 - improvement 1\n",
      "Step 7010 - Loss: -0.48064959049224854 - prediction 0.48064959049224854 - improvement 2\n",
      "Step 7020 - Loss: -0.32634034752845764 - prediction 0.32634034752845764 - improvement 0\n",
      "Step 7030 - Loss: -0.32628458738327026 - prediction 0.32628458738327026 - improvement 0\n",
      "Step 7040 - Loss: -0.32629701495170593 - prediction 0.32629701495170593 - improvement 0\n",
      "Step 7050 - Loss: -0.32627102732658386 - prediction 0.32627102732658386 - improvement 0\n",
      "Step 7060 - Loss: -0.48078423738479614 - prediction 0.48078423738479614 - improvement 2\n",
      "Step 7070 - Loss: -0.48115673661231995 - prediction 0.48115673661231995 - improvement 2\n",
      "Step 7080 - Loss: -0.48114293813705444 - prediction 0.48114293813705444 - improvement 2\n",
      "Step 7090 - Loss: -0.1927216500043869 - prediction 0.1927216500043869 - improvement 1\n",
      "Step 7100 - Loss: -0.32624903321266174 - prediction 0.32624903321266174 - improvement 0\n",
      "Step 7110 - Loss: -0.19274897873401642 - prediction 0.19274897873401642 - improvement 1\n",
      "Step 7120 - Loss: -0.19304078817367554 - prediction 0.19304078817367554 - improvement 1\n",
      "Step 7130 - Loss: -0.19297848641872406 - prediction 0.19297848641872406 - improvement 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7140 - Loss: -0.32631900906562805 - prediction 0.32631900906562805 - improvement 0\n",
      "Step 7150 - Loss: -0.19295445084571838 - prediction 0.19295445084571838 - improvement 1\n",
      "Step 7160 - Loss: -0.19309286773204803 - prediction 0.19309286773204803 - improvement 1\n",
      "Step 7170 - Loss: -0.1933693140745163 - prediction 0.1933693140745163 - improvement 1\n",
      "Step 7180 - Loss: -0.48005038499832153 - prediction 0.48005038499832153 - improvement 2\n",
      "Step 7190 - Loss: -0.3264866769313812 - prediction 0.3264866769313812 - improvement 0\n",
      "Step 7200 - Loss: -0.4802226424217224 - prediction 0.4802226424217224 - improvement 2\n",
      "Step 7210 - Loss: -0.3264172673225403 - prediction 0.3264172673225403 - improvement 0\n",
      "Step 7220 - Loss: -0.19309113919734955 - prediction 0.19309113919734955 - improvement 1\n",
      "Step 7230 - Loss: -0.32640647888183594 - prediction 0.32640647888183594 - improvement 0\n",
      "Step 7240 - Loss: -0.4802987575531006 - prediction 0.4802987575531006 - improvement 2\n",
      "Step 7250 - Loss: -0.32646629214286804 - prediction 0.32646629214286804 - improvement 0\n",
      "Step 7260 - Loss: -0.19323819875717163 - prediction 0.19323819875717163 - improvement 1\n",
      "Step 7270 - Loss: -0.48051467537879944 - prediction 0.48051467537879944 - improvement 2\n",
      "Step 7280 - Loss: -0.3264104127883911 - prediction 0.3264104127883911 - improvement 0\n",
      "Step 7290 - Loss: -0.1932036429643631 - prediction 0.1932036429643631 - improvement 1\n",
      "Step 7300 - Loss: -0.48052552342414856 - prediction 0.48052552342414856 - improvement 2\n",
      "Step 7310 - Loss: -0.48068687319755554 - prediction 0.48068687319755554 - improvement 2\n",
      "Step 7320 - Loss: -0.32634687423706055 - prediction 0.32634687423706055 - improvement 0\n",
      "Step 7330 - Loss: -0.3263293504714966 - prediction 0.3263293504714966 - improvement 0\n",
      "Step 7340 - Loss: -0.3263913691043854 - prediction 0.3263913691043854 - improvement 0\n",
      "Step 7350 - Loss: -0.32639482617378235 - prediction 0.32639482617378235 - improvement 0\n",
      "Step 7360 - Loss: -0.4802963435649872 - prediction 0.4802963435649872 - improvement 2\n",
      "Step 7370 - Loss: -0.48029035329818726 - prediction 0.48029035329818726 - improvement 2\n",
      "Step 7380 - Loss: -0.32647451758384705 - prediction 0.32647451758384705 - improvement 0\n",
      "Step 7390 - Loss: -0.4801930785179138 - prediction 0.4801930785179138 - improvement 2\n",
      "Step 7400 - Loss: -0.48019731044769287 - prediction 0.48019731044769287 - improvement 2\n",
      "Step 7410 - Loss: -0.19338274002075195 - prediction 0.19338274002075195 - improvement 1\n",
      "Step 7420 - Loss: -0.19324642419815063 - prediction 0.19324642419815063 - improvement 1\n",
      "Step 7430 - Loss: -0.3264050781726837 - prediction 0.3264050781726837 - improvement 0\n",
      "Step 7440 - Loss: -0.32641005516052246 - prediction 0.32641005516052246 - improvement 0\n",
      "Step 7450 - Loss: -0.32642826437950134 - prediction 0.32642826437950134 - improvement 0\n",
      "Step 7460 - Loss: -0.4802480638027191 - prediction 0.4802480638027191 - improvement 2\n",
      "Step 7470 - Loss: -0.32651594281196594 - prediction 0.32651594281196594 - improvement 0\n",
      "Step 7480 - Loss: -0.4800467789173126 - prediction 0.4800467789173126 - improvement 2\n",
      "Step 7490 - Loss: -0.3264729976654053 - prediction 0.3264729976654053 - improvement 0\n",
      "Step 7500 - Loss: -0.3264326751232147 - prediction 0.3264326751232147 - improvement 0\n",
      "Step 7510 - Loss: -0.19296732544898987 - prediction 0.19296732544898987 - improvement 1\n",
      "Step 7520 - Loss: -0.3262978494167328 - prediction 0.3262978494167328 - improvement 0\n",
      "Step 7530 - Loss: -0.32636168599128723 - prediction 0.32636168599128723 - improvement 0\n",
      "Step 7540 - Loss: -0.48099496960639954 - prediction 0.48099496960639954 - improvement 2\n",
      "Step 7550 - Loss: -0.3261962831020355 - prediction 0.3261962831020355 - improvement 0\n",
      "Step 7560 - Loss: -0.3262315094470978 - prediction 0.3262315094470978 - improvement 0\n",
      "Step 7570 - Loss: -0.4811394214630127 - prediction 0.4811394214630127 - improvement 2\n",
      "Step 7580 - Loss: -0.3261750042438507 - prediction 0.3261750042438507 - improvement 0\n",
      "Step 7590 - Loss: -0.32624736428260803 - prediction 0.32624736428260803 - improvement 0\n",
      "Step 7600 - Loss: -0.19284217059612274 - prediction 0.19284217059612274 - improvement 1\n",
      "Step 7610 - Loss: -0.480621874332428 - prediction 0.480621874332428 - improvement 2\n",
      "Step 7620 - Loss: -0.32637348771095276 - prediction 0.32637348771095276 - improvement 0\n",
      "Step 7630 - Loss: -0.4806338846683502 - prediction 0.4806338846683502 - improvement 2\n",
      "Step 7640 - Loss: -0.4804357588291168 - prediction 0.4804357588291168 - improvement 2\n",
      "Step 7650 - Loss: -0.3263527452945709 - prediction 0.3263527452945709 - improvement 0\n",
      "Step 7660 - Loss: -0.1927720457315445 - prediction 0.1927720457315445 - improvement 1\n",
      "Step 7670 - Loss: -0.32618245482444763 - prediction 0.32618245482444763 - improvement 0\n",
      "Step 7680 - Loss: -0.19237485527992249 - prediction 0.19237485527992249 - improvement 1\n",
      "Step 7690 - Loss: -0.4816877841949463 - prediction 0.4816877841949463 - improvement 2\n",
      "Step 7700 - Loss: -0.48140615224838257 - prediction 0.48140615224838257 - improvement 2\n",
      "Step 7710 - Loss: -0.3261070251464844 - prediction 0.3261070251464844 - improvement 0\n",
      "Step 7720 - Loss: -0.3261108696460724 - prediction 0.3261108696460724 - improvement 0\n",
      "Step 7730 - Loss: -0.4815446436405182 - prediction 0.4815446436405182 - improvement 2\n",
      "Step 7740 - Loss: -0.32617244124412537 - prediction 0.32617244124412537 - improvement 0\n",
      "Step 7750 - Loss: -0.32624301314353943 - prediction 0.32624301314353943 - improvement 0\n",
      "Step 7760 - Loss: -0.48099321126937866 - prediction 0.48099321126937866 - improvement 2\n",
      "Step 7770 - Loss: -0.3262653052806854 - prediction 0.3262653052806854 - improvement 0\n",
      "Step 7780 - Loss: -0.3262580931186676 - prediction 0.3262580931186676 - improvement 0\n",
      "Step 7790 - Loss: -0.4806988835334778 - prediction 0.4806988835334778 - improvement 2\n",
      "Step 7800 - Loss: -0.4806212782859802 - prediction 0.4806212782859802 - improvement 2\n",
      "Step 7810 - Loss: -0.48053938150405884 - prediction 0.48053938150405884 - improvement 2\n",
      "Step 7820 - Loss: -0.32639122009277344 - prediction 0.32639122009277344 - improvement 0\n",
      "Step 7830 - Loss: -0.3263109028339386 - prediction 0.3263109028339386 - improvement 0\n",
      "Step 7840 - Loss: -0.19293473660945892 - prediction 0.19293473660945892 - improvement 1\n",
      "Step 7850 - Loss: -0.32631969451904297 - prediction 0.32631969451904297 - improvement 0\n",
      "Step 7860 - Loss: -0.48087194561958313 - prediction 0.48087194561958313 - improvement 2\n",
      "Step 7870 - Loss: -0.48079389333724976 - prediction 0.48079389333724976 - improvement 2\n",
      "Step 7880 - Loss: -0.3263777792453766 - prediction 0.3263777792453766 - improvement 0\n",
      "Step 7890 - Loss: -0.3263561725616455 - prediction 0.3263561725616455 - improvement 0\n",
      "Step 7900 - Loss: -0.3263561725616455 - prediction 0.3263561725616455 - improvement 0\n",
      "Step 7910 - Loss: -0.3263537585735321 - prediction 0.3263537585735321 - improvement 0\n",
      "Step 7920 - Loss: -0.19309458136558533 - prediction 0.19309458136558533 - improvement 1\n",
      "Step 7930 - Loss: -0.4803856611251831 - prediction 0.4803856611251831 - improvement 2\n",
      "Step 7940 - Loss: -0.4801211655139923 - prediction 0.4801211655139923 - improvement 2\n",
      "Step 7950 - Loss: -0.48006671667099 - prediction 0.48006671667099 - improvement 2\n",
      "Step 7960 - Loss: -0.32651132345199585 - prediction 0.32651132345199585 - improvement 0\n",
      "Step 7970 - Loss: -0.48002979159355164 - prediction 0.48002979159355164 - improvement 2\n",
      "Step 7980 - Loss: -0.19350166618824005 - prediction 0.19350166618824005 - improvement 1\n",
      "Step 7990 - Loss: -0.3265056610107422 - prediction 0.3265056610107422 - improvement 0\n",
      "Step 8000 - Loss: -0.19336970150470734 - prediction 0.19336970150470734 - improvement 1\n",
      "Step 8010 - Loss: -0.47968608140945435 - prediction 0.47968608140945435 - improvement 2\n",
      "Step 8020 - Loss: -0.32662275433540344 - prediction 0.32662275433540344 - improvement 0\n",
      "Step 8030 - Loss: -0.32659396529197693 - prediction 0.32659396529197693 - improvement 0\n",
      "Step 8040 - Loss: -0.3264889121055603 - prediction 0.3264889121055603 - improvement 0\n",
      "Step 8050 - Loss: -0.4804496169090271 - prediction 0.4804496169090271 - improvement 2\n",
      "Step 8060 - Loss: -0.3263821303844452 - prediction 0.3263821303844452 - improvement 0\n",
      "Step 8070 - Loss: -0.32639941573143005 - prediction 0.32639941573143005 - improvement 0\n",
      "Step 8080 - Loss: -0.48046281933784485 - prediction 0.48046281933784485 - improvement 2\n",
      "Step 8090 - Loss: -0.32639461755752563 - prediction 0.32639461755752563 - improvement 0\n",
      "Step 8100 - Loss: -0.32643887400627136 - prediction 0.32643887400627136 - improvement 0\n",
      "Step 8110 - Loss: -0.19321228563785553 - prediction 0.19321228563785553 - improvement 1\n",
      "Step 8120 - Loss: -0.4802999794483185 - prediction 0.4802999794483185 - improvement 2\n",
      "Step 8130 - Loss: -0.32650187611579895 - prediction 0.32650187611579895 - improvement 0\n",
      "Step 8140 - Loss: -0.3265693783760071 - prediction 0.3265693783760071 - improvement 0\n",
      "Step 8150 - Loss: -0.3266133964061737 - prediction 0.3266133964061737 - improvement 0\n",
      "Step 8160 - Loss: -0.47984203696250916 - prediction 0.47984203696250916 - improvement 2\n",
      "Step 8170 - Loss: -0.4797474145889282 - prediction 0.4797474145889282 - improvement 2\n",
      "Step 8180 - Loss: -0.19363617897033691 - prediction 0.19363617897033691 - improvement 1\n",
      "Step 8190 - Loss: -0.1936706304550171 - prediction 0.1936706304550171 - improvement 1\n",
      "Step 8200 - Loss: -0.4797188639640808 - prediction 0.4797188639640808 - improvement 2\n",
      "Step 8210 - Loss: -0.3266312777996063 - prediction 0.3266312777996063 - improvement 0\n",
      "Step 8220 - Loss: -0.47991713881492615 - prediction 0.47991713881492615 - improvement 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8230 - Loss: -0.3265712559223175 - prediction 0.3265712559223175 - improvement 0\n",
      "Step 8240 - Loss: -0.1936536282300949 - prediction 0.1936536282300949 - improvement 1\n",
      "Step 8250 - Loss: -0.3266069293022156 - prediction 0.3266069293022156 - improvement 0\n",
      "Step 8260 - Loss: -0.326649010181427 - prediction 0.326649010181427 - improvement 0\n",
      "Step 8270 - Loss: -0.47945263981819153 - prediction 0.47945263981819153 - improvement 2\n",
      "Step 8280 - Loss: -0.47905397415161133 - prediction 0.47905397415161133 - improvement 2\n",
      "Step 8290 - Loss: -0.32683977484703064 - prediction 0.32683977484703064 - improvement 0\n",
      "Step 8300 - Loss: -0.194153293967247 - prediction 0.194153293967247 - improvement 1\n",
      "Step 8310 - Loss: -0.19388806819915771 - prediction 0.19388806819915771 - improvement 1\n",
      "Step 8320 - Loss: -0.4794721007347107 - prediction 0.4794721007347107 - improvement 2\n",
      "Step 8330 - Loss: -0.4794647991657257 - prediction 0.4794647991657257 - improvement 2\n",
      "Step 8340 - Loss: -0.3266814947128296 - prediction 0.3266814947128296 - improvement 0\n",
      "Step 8350 - Loss: -0.47948065400123596 - prediction 0.47948065400123596 - improvement 2\n",
      "Step 8360 - Loss: -0.3266592025756836 - prediction 0.3266592025756836 - improvement 0\n",
      "Step 8370 - Loss: -0.19399519264698029 - prediction 0.19399519264698029 - improvement 1\n",
      "Step 8380 - Loss: -0.4796326458454132 - prediction 0.4796326458454132 - improvement 2\n",
      "Step 8390 - Loss: -0.4797528386116028 - prediction 0.4797528386116028 - improvement 2\n",
      "Step 8400 - Loss: -0.4795815944671631 - prediction 0.4795815944671631 - improvement 2\n",
      "Step 8410 - Loss: -0.3267173767089844 - prediction 0.3267173767089844 - improvement 0\n",
      "Step 8420 - Loss: -0.4796448051929474 - prediction 0.4796448051929474 - improvement 2\n",
      "Step 8430 - Loss: -0.4794009029865265 - prediction 0.4794009029865265 - improvement 2\n",
      "Step 8440 - Loss: -0.4794977009296417 - prediction 0.4794977009296417 - improvement 2\n",
      "Step 8450 - Loss: -0.4793771803379059 - prediction 0.4793771803379059 - improvement 2\n",
      "Step 8460 - Loss: -0.32677674293518066 - prediction 0.32677674293518066 - improvement 0\n",
      "Step 8470 - Loss: -0.3267554044723511 - prediction 0.3267554044723511 - improvement 0\n",
      "Step 8480 - Loss: -0.479266881942749 - prediction 0.479266881942749 - improvement 2\n",
      "Step 8490 - Loss: -0.3268354833126068 - prediction 0.3268354833126068 - improvement 0\n",
      "Step 8500 - Loss: -0.4787653088569641 - prediction 0.4787653088569641 - improvement 2\n",
      "Step 8510 - Loss: -0.3268451690673828 - prediction 0.3268451690673828 - improvement 0\n",
      "Step 8520 - Loss: -0.47888097167015076 - prediction 0.47888097167015076 - improvement 2\n",
      "Step 8530 - Loss: -0.47852498292922974 - prediction 0.47852498292922974 - improvement 2\n",
      "Step 8540 - Loss: -0.32685545086860657 - prediction 0.32685545086860657 - improvement 0\n",
      "Step 8550 - Loss: -0.19422174990177155 - prediction 0.19422174990177155 - improvement 1\n",
      "Step 8560 - Loss: -0.4791211187839508 - prediction 0.4791211187839508 - improvement 2\n",
      "Step 8570 - Loss: -0.4791528880596161 - prediction 0.4791528880596161 - improvement 2\n",
      "Step 8580 - Loss: -0.1941312700510025 - prediction 0.1941312700510025 - improvement 1\n",
      "Step 8590 - Loss: -0.32677963376045227 - prediction 0.32677963376045227 - improvement 0\n",
      "Step 8600 - Loss: -0.32680994272232056 - prediction 0.32680994272232056 - improvement 0\n",
      "Step 8610 - Loss: -0.32678335905075073 - prediction 0.32678335905075073 - improvement 0\n",
      "Step 8620 - Loss: -0.4792979657649994 - prediction 0.4792979657649994 - improvement 2\n",
      "Step 8630 - Loss: -0.19388629496097565 - prediction 0.19388629496097565 - improvement 1\n",
      "Step 8640 - Loss: -0.32670754194259644 - prediction 0.32670754194259644 - improvement 0\n",
      "Step 8650 - Loss: -0.3266935646533966 - prediction 0.3266935646533966 - improvement 0\n",
      "Step 8660 - Loss: -0.4794246554374695 - prediction 0.4794246554374695 - improvement 2\n",
      "Step 8670 - Loss: -0.19371996819972992 - prediction 0.19371996819972992 - improvement 1\n",
      "Step 8680 - Loss: -0.3265429139137268 - prediction 0.3265429139137268 - improvement 0\n",
      "Step 8690 - Loss: -0.32651403546333313 - prediction 0.32651403546333313 - improvement 0\n",
      "Step 8700 - Loss: -0.4802323877811432 - prediction 0.4802323877811432 - improvement 2\n",
      "Step 8710 - Loss: -0.4801477789878845 - prediction 0.4801477789878845 - improvement 2\n",
      "Step 8720 - Loss: -0.4799862205982208 - prediction 0.4799862205982208 - improvement 2\n",
      "Step 8730 - Loss: -0.19346864521503448 - prediction 0.19346864521503448 - improvement 1\n",
      "Step 8740 - Loss: -0.4798692762851715 - prediction 0.4798692762851715 - improvement 2\n",
      "Step 8750 - Loss: -0.4798274338245392 - prediction 0.4798274338245392 - improvement 2\n",
      "Step 8760 - Loss: -0.47977709770202637 - prediction 0.47977709770202637 - improvement 2\n",
      "Step 8770 - Loss: -0.47978439927101135 - prediction 0.47978439927101135 - improvement 2\n",
      "Step 8780 - Loss: -0.19356340169906616 - prediction 0.19356340169906616 - improvement 1\n",
      "Step 8790 - Loss: -0.3264829218387604 - prediction 0.3264829218387604 - improvement 0\n",
      "Step 8800 - Loss: -0.19338403642177582 - prediction 0.19338403642177582 - improvement 1\n",
      "Step 8810 - Loss: -0.3265156149864197 - prediction 0.3265156149864197 - improvement 0\n",
      "Step 8820 - Loss: -0.4802214801311493 - prediction 0.4802214801311493 - improvement 2\n",
      "Step 8830 - Loss: -0.1934812366962433 - prediction 0.1934812366962433 - improvement 1\n",
      "Step 8840 - Loss: -0.19371123611927032 - prediction 0.19371123611927032 - improvement 1\n",
      "Step 8850 - Loss: -0.19374968111515045 - prediction 0.19374968111515045 - improvement 1\n",
      "Step 8860 - Loss: -0.3266695737838745 - prediction 0.3266695737838745 - improvement 0\n",
      "Step 8870 - Loss: -0.4794051945209503 - prediction 0.4794051945209503 - improvement 2\n",
      "Step 8880 - Loss: -0.3266887962818146 - prediction 0.3266887962818146 - improvement 0\n",
      "Step 8890 - Loss: -0.19388717412948608 - prediction 0.19388717412948608 - improvement 1\n",
      "Step 8900 - Loss: -0.3267209231853485 - prediction 0.3267209231853485 - improvement 0\n",
      "Step 8910 - Loss: -0.47926145792007446 - prediction 0.47926145792007446 - improvement 2\n",
      "Step 8920 - Loss: -0.3267090320587158 - prediction 0.3267090320587158 - improvement 0\n",
      "Step 8930 - Loss: -0.3266673982143402 - prediction 0.3266673982143402 - improvement 0\n",
      "Step 8940 - Loss: -0.4795967638492584 - prediction 0.4795967638492584 - improvement 2\n",
      "Step 8950 - Loss: -0.47967877984046936 - prediction 0.47967877984046936 - improvement 2\n",
      "Step 8960 - Loss: -0.19372348487377167 - prediction 0.19372348487377167 - improvement 1\n",
      "Step 8970 - Loss: -0.32661592960357666 - prediction 0.32661592960357666 - improvement 0\n",
      "Step 8980 - Loss: -0.47960105538368225 - prediction 0.47960105538368225 - improvement 2\n",
      "Step 8990 - Loss: -0.4795730710029602 - prediction 0.4795730710029602 - improvement 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save() missing 2 required positional arguments: 'obj' and 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-1154a9fb3149>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {index} - Loss: {loss} - prediction {classification[0,improvement]} - improvement {improvement}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m#     print(value, previous_value, delta)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: save() missing 2 required positional arguments: 'obj' and 'f'"
     ]
    }
   ],
   "source": [
    "imp_one_hot = torch.FloatTensor(3)\n",
    "\n",
    "# # In your for loop\n",
    "# y_onehot.zero_()\n",
    "# y_onehot.scatter_(1, y, 1)\n",
    "for epoch in range(no_epochs):\n",
    "    for index, target_value in enumerate(history.values[1:]):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        previous_value = history.values[index-1]\n",
    "        delta = target_value - previous_value\n",
    "\n",
    "        if delta>-min_neg and delta<min_pos:\n",
    "            improvement = 1 # Class is 1 (no difference)\n",
    "        elif delta>=-min_pos:\n",
    "            improvement = 0 # Class is 0 (positive difference)\n",
    "        else:\n",
    "            improvement = 2 # Class is 2 (negative difference)\n",
    "\n",
    "\n",
    "        imp_one_hot.zero_()\n",
    "\n",
    "        imp_one_hot.scatter_(0, torch.tensor([improvement]), 1)\n",
    "\n",
    "        classification = model(torch.Tensor([[previous_value]]))\n",
    "\n",
    "        loss = loss_fn(classification, torch.tensor([improvement]))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if index % 10 == 0:\n",
    "            print(f\"Step {index} - Loss: {loss} - prediction {classification[0,improvement]} - improvement {improvement}\")\n",
    "#     print(value, previous_value, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73e801cb20>]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(timestamps,history_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store state dict\n",
    "if not os.path.isdir('Pre-trained Models'):\n",
    "    os.mkdir('Pre-trained Models')\n",
    "\n",
    "model_path = 'models/markov_kernel_n1.pt'\n",
    "    \n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
